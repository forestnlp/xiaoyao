{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0225ceab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-24 23:41:51] âœ… å›æµ‹ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\n",
      "[2025-10-24 23:41:51] åŠ è½½é€‰è‚¡ç»“æœ...\n",
      "[2025-10-24 23:41:51] âœ… é€‰è‚¡ç»“æœï¼š630æ¡è®°å½•ï¼Œ548åªè‚¡ç¥¨\n",
      "[2025-10-24 23:41:51] åŠ è½½è¡Œæƒ…æ•°æ®...\n",
      "[2025-10-24 23:42:11] åŒ¹é…ä¹°å–ä»·æ ¼...\n",
      "[2025-10-24 23:42:15] è®¡ç®—æ”¶ç›Š...\n",
      "[2025-10-24 23:42:15] âœ… å›æµ‹æ±‡æ€»ä¿å­˜ï¼š./backtest_stable_summary.txt\n",
      "[2025-10-24 23:42:15] \n",
      "============================================================\n",
      "[2025-10-24 23:42:15] âœ… ç¨³å®šç‰ˆå›æµ‹å®Œæˆï¼æ ¸å¿ƒç»“æœï¼š\n",
      "[2025-10-24 23:42:15] ğŸ“Š æœ‰æ•ˆäº¤æ˜“ï¼š628æ¡ | æ­£æ”¶ç›Šæ¯”ä¾‹ï¼š50.32%\n",
      "[2025-10-24 23:42:16] ğŸ“ˆ å¹³å‡æ”¶ç›Šï¼š1.41% | ä¸­ä½æ•°æ”¶ç›Šï¼š0.1%\n",
      "[2025-10-24 23:42:16] ğŸ“ æ˜ç»†ï¼š./backtest_stable.csv | æ±‡æ€»ï¼š./backtest_stable_summary.txt\n",
      "[2025-10-24 23:42:16] ============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # ç¡®ä¿å·²å¯¼å…¥numpy\n",
    "import os\n",
    "\n",
    "CONFIG = {\n",
    "    \"selection_result_path\": r'./short_term_selection_optimized.csv',\n",
    "    \"raw_data_path\": r'D:\\workspace\\xiaoyao\\data\\widetable.parquet',\n",
    "    \"backtest_result_path\": r'./backtest_stable.csv',\n",
    "    \"backtest_summary_path\": r'./backtest_stable_summary.txt',\n",
    "    \"log_path\": r'./backtest_stable_log.txt',\n",
    "    \"trade_rule\": {\n",
    "        \"buy_delay\": 1,    # T+1ä¹°å…¥\n",
    "        \"sell_delay\": 5,   # T+5å–å‡º\n",
    "        \"min_valid_days\": 6\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# å·¥å…·å‡½æ•°ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šæ›¿æ¢roundä¸ºnp.roundï¼‰\n",
    "# --------------------------\n",
    "def init_environment():\n",
    "    with open(CONFIG[\"log_path\"], 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"ã€ç¨³å®šç‰ˆå›æµ‹å¯åŠ¨ã€‘{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_msg(\"âœ… å›æµ‹ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "def log_msg(msg):\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_line = f\"[{timestamp}] {msg}\"\n",
    "    print(log_line)\n",
    "    with open(CONFIG[\"log_path\"], 'a', encoding='utf-8') as f:\n",
    "        f.write(log_line + \"\\n\")\n",
    "\n",
    "def calc_group_stats(df):\n",
    "    \"\"\"ä¿®å¤ï¼šç”¨np.roundå…¼å®¹floatç±»å‹ï¼Œè§£å†³roundæŠ¥é”™\"\"\"\n",
    "    # 1. å¤„ç†ç«ä»·å¾—åˆ†åˆ†ç»„\n",
    "    if 'auction_score' in df.columns:\n",
    "        try:\n",
    "            df['auction_group'] = pd.qcut(\n",
    "                df['auction_score'], \n",
    "                q=3, \n",
    "                labels=['ä½ç«ä»·å¾—åˆ†', 'ä¸­ç«ä»·å¾—åˆ†', 'é«˜ç«ä»·å¾—åˆ†'],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "        except:\n",
    "            df['auction_group'] = pd.cut(\n",
    "                df['auction_score'],\n",
    "                bins=[-0.1, 33, 66, 100.1],\n",
    "                labels=['ä½ç«ä»·å¾—åˆ†', 'ä¸­ç«ä»·å¾—åˆ†', 'é«˜ç«ä»·å¾—åˆ†']\n",
    "            )\n",
    "    else:\n",
    "        df['auction_group'] = 'æ— æ•°æ®'\n",
    "    \n",
    "    # 2. å¤„ç†é‡ä»·å¾—åˆ†åˆ†ç»„\n",
    "    if 'price_volume_score' in df.columns:\n",
    "        try:\n",
    "            df['pv_group'] = pd.qcut(\n",
    "                df['price_volume_score'],\n",
    "                q=3,\n",
    "                labels=['ä½é‡ä»·å¾—åˆ†', 'ä¸­é‡ä»·å¾—åˆ†', 'é«˜é‡ä»·å¾—åˆ†'],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "        except:\n",
    "            df['pv_group'] = pd.cut(\n",
    "                df['price_volume_score'],\n",
    "                bins=[-0.1, 33, 66, 100.1],\n",
    "                labels=['ä½é‡ä»·å¾—åˆ†', 'ä¸­é‡ä»·å¾—åˆ†', 'é«˜é‡ä»·å¾—åˆ†']\n",
    "            )\n",
    "    else:\n",
    "        df['pv_group'] = 'æ— æ•°æ®'\n",
    "    \n",
    "    # 3. åˆ†ç»„ç»Ÿè®¡ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šç”¨np.roundæ›¿ä»£roundï¼‰\n",
    "    group_stats = []\n",
    "    # ç«ä»·å¾—åˆ†åˆ†ç»„ç»Ÿè®¡\n",
    "    auction_stats = df.groupby('auction_group', observed=True).agg({\n",
    "        # æ›¿æ¢x.mean().round(2) â†’ np.round(x.mean(), 2)\n",
    "        'return_rate': [\n",
    "            'count', \n",
    "            lambda x: np.round(x.mean(), 2),  # ä¿®å¤ï¼šç”¨np.roundå…¼å®¹float\n",
    "            lambda x: np.round(x.median(), 2),  # ä¿®å¤\n",
    "            lambda x: np.round((x>0).mean()*100, 2)  # ä¿®å¤\n",
    "        ],\n",
    "        'stock_code': lambda x: x.nunique()\n",
    "    })\n",
    "    auction_stats.columns = ['äº¤æ˜“æ•°', 'å¹³å‡æ”¶ç›Š(%)', 'ä¸­ä½æ•°æ”¶ç›Š(%)', 'æ­£æ”¶ç›Šæ¯”ä¾‹(%)', 'è‚¡ç¥¨æ•°']\n",
    "    for group in auction_stats.index:\n",
    "        group_stats.append({\n",
    "            'åˆ†ç»„ç±»å‹': 'ç«ä»·å¾—åˆ†',\n",
    "            'åˆ†ç»„': group,\n",
    "            'äº¤æ˜“æ•°': auction_stats.loc[group, 'äº¤æ˜“æ•°'],\n",
    "            'å¹³å‡æ”¶ç›Š(%)': auction_stats.loc[group, 'å¹³å‡æ”¶ç›Š(%)'],\n",
    "            'ä¸­ä½æ•°æ”¶ç›Š(%)': auction_stats.loc[group, 'ä¸­ä½æ•°æ”¶ç›Š(%)'],\n",
    "            'æ­£æ”¶ç›Šæ¯”ä¾‹(%)': auction_stats.loc[group, 'æ­£æ”¶ç›Šæ¯”ä¾‹(%)']\n",
    "        })\n",
    "    \n",
    "    # é‡ä»·å¾—åˆ†åˆ†ç»„ç»Ÿè®¡ï¼ˆåŒé€»è¾‘ä¿®å¤ï¼‰\n",
    "    pv_stats = df.groupby('pv_group', observed=True).agg({\n",
    "        'return_rate': [\n",
    "            'count', \n",
    "            lambda x: np.round(x.mean(), 2),  # ä¿®å¤\n",
    "            lambda x: np.round(x.median(), 2),  # ä¿®å¤\n",
    "            lambda x: np.round((x>0).mean()*100, 2)  # ä¿®å¤\n",
    "        ],\n",
    "        'stock_code': lambda x: x.nunique()\n",
    "    })\n",
    "    pv_stats.columns = ['äº¤æ˜“æ•°', 'å¹³å‡æ”¶ç›Š(%)', 'ä¸­ä½æ•°æ”¶ç›Š(%)', 'æ­£æ”¶ç›Šæ¯”ä¾‹(%)', 'è‚¡ç¥¨æ•°']\n",
    "    for group in pv_stats.index:\n",
    "        group_stats.append({\n",
    "            'åˆ†ç»„ç±»å‹': 'é‡ä»·å¾—åˆ†',\n",
    "            'åˆ†ç»„': group,\n",
    "            'äº¤æ˜“æ•°': pv_stats.loc[group, 'äº¤æ˜“æ•°'],\n",
    "            'å¹³å‡æ”¶ç›Š(%)': pv_stats.loc[group, 'å¹³å‡æ”¶ç›Š(%)'],\n",
    "            'ä¸­ä½æ•°æ”¶ç›Š(%)': pv_stats.loc[group, 'ä¸­ä½æ•°æ”¶ç›Š(%)'],\n",
    "            'æ­£æ”¶ç›Šæ¯”ä¾‹(%)': pv_stats.loc[group, 'æ­£æ”¶ç›Šæ¯”ä¾‹(%)']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(group_stats), df\n",
    "\n",
    "def save_summary(summary_dict, group_stats_df):\n",
    "    \"\"\"ä¿å­˜æ±‡æ€»ç»“æœ\"\"\"\n",
    "    group_text = \"\\n4. åˆ†ç»„ç»Ÿè®¡\\n\"\n",
    "    group_text += \"=\"*60 + \"\\n\"\n",
    "    group_text += group_stats_df.to_string(index=False, na_rep='-') + \"\\n\"\n",
    "    \n",
    "    # æ±‡æ€»å†…å®¹ä¸­çš„æ•°å€¼ä¹Ÿç”¨np.roundç»Ÿä¸€ç²¾åº¦\n",
    "    summary_content = f\"\"\"\n",
    "ã€ç¨³å®šç‰ˆå›æµ‹ç»“æœã€‘\n",
    "==========================\n",
    "å›æµ‹è§„åˆ™ï¼šTæ—¥é€‰è‚¡ â†’ T+{CONFIG['trade_rule']['buy_delay']}ä¹°å…¥ â†’ T+{CONFIG['trade_rule']['sell_delay']}å–å‡º\n",
    "==========================\n",
    "1. åŸºç¡€ç»Ÿè®¡\n",
    "   - é€‰è‚¡æ€»è®°å½•ï¼š{summary_dict['total_selection']} æ¡\n",
    "   - æœ‰æ•ˆäº¤æ˜“ï¼š{summary_dict['valid_trade']} æ¡\n",
    "   - æ— æ•ˆäº¤æ˜“ï¼š{summary_dict['invalid_trade']} æ¡\n",
    "   - æœ‰æ•ˆç‡ï¼š{np.round(summary_dict['valid_rate'], 2)}%\n",
    "\n",
    "2. æ”¶ç›Šç»Ÿè®¡\n",
    "   - å¹³å‡æ”¶ç›Šç‡ï¼š{np.round(summary_dict['avg_return'], 2)}%\n",
    "   - ä¸­ä½æ•°æ”¶ç›Šç‡ï¼š{np.round(summary_dict['median_return'], 2)}%\n",
    "   - æ­£æ”¶ç›Šæ¯”ä¾‹ï¼š{np.round(summary_dict['positive_ratio'], 2)}%ï¼ˆ{summary_dict['positive_count']}/{summary_dict['valid_trade']}ï¼‰\n",
    "   - æœ€å¤§æ”¶ç›Šï¼š{np.round(summary_dict['max_return'], 2)}%\n",
    "   - æœ€å°æ”¶ç›Šï¼š{np.round(summary_dict['min_return'], 2)}%\n",
    "\n",
    "3. é£é™©ç»Ÿè®¡\n",
    "   - æ”¶ç›Šæ ‡å‡†å·®ï¼š{np.round(summary_dict['std_return'], 2)}%\n",
    "   - æœ€å¤§å›æ’¤ï¼š{np.round(summary_dict['max_drawdown'], 2)}%ï¼ˆç®€åŒ–è®¡ç®—ï¼‰\n",
    "{group_text}\n",
    "==========================\"\"\"\n",
    "    with open(CONFIG[\"backtest_summary_path\"], 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_content)\n",
    "    log_msg(f\"âœ… å›æµ‹æ±‡æ€»ä¿å­˜ï¼š{CONFIG['backtest_summary_path']}\")\n",
    "\n",
    "# --------------------------\n",
    "# ä¸»å›æµ‹é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰\n",
    "# --------------------------\n",
    "def run_backtest():\n",
    "    try:\n",
    "        init_environment()\n",
    "        \n",
    "        # 1. åŠ è½½æ•°æ®\n",
    "        log_msg(\"åŠ è½½é€‰è‚¡ç»“æœ...\")\n",
    "        selection_df = pd.read_csv(CONFIG[\"selection_result_path\"])\n",
    "        selection_df['date'] = pd.to_datetime(selection_df['date']).dt.date\n",
    "        log_msg(f\"âœ… é€‰è‚¡ç»“æœï¼š{len(selection_df)}æ¡è®°å½•ï¼Œ{selection_df['stock_code'].nunique()}åªè‚¡ç¥¨\")\n",
    "        \n",
    "        log_msg(\"åŠ è½½è¡Œæƒ…æ•°æ®...\")\n",
    "        raw_df = pd.read_parquet(CONFIG[\"raw_data_path\"])\n",
    "        raw_df['date'] = pd.to_datetime(raw_df['date']).dt.date\n",
    "        raw_df = raw_df[['stock_code', 'date', 'close']].dropna(subset=['close'])\n",
    "        raw_df = raw_df.sort_values(['stock_code', 'date']).reset_index(drop=True)\n",
    "        \n",
    "        # 2. åŒ¹é…ä¹°å–ä»·æ ¼\n",
    "        log_msg(\"åŒ¹é…ä¹°å–ä»·æ ¼...\")\n",
    "        raw_df['trade_seq'] = raw_df.groupby('stock_code').cumcount()\n",
    "        selection_df = selection_df.merge(\n",
    "            raw_df[['stock_code', 'date', 'trade_seq']],\n",
    "            on=['stock_code', 'date'],\n",
    "            how='left'\n",
    "        ).dropna(subset=['trade_seq'])\n",
    "        selection_df['trade_seq'] = selection_df['trade_seq'].astype(int)\n",
    "        \n",
    "        buy_seq = selection_df['trade_seq'] + CONFIG['trade_rule']['buy_delay']\n",
    "        sell_seq = selection_df['trade_seq'] + CONFIG['trade_rule']['sell_delay']\n",
    "        \n",
    "        buy_price = raw_df.set_index(['stock_code', 'trade_seq'])['close'].reindex(\n",
    "            pd.MultiIndex.from_arrays([selection_df['stock_code'], buy_seq], names=['stock_code', 'trade_seq'])\n",
    "        ).values\n",
    "        selection_df['buy_price'] = buy_price\n",
    "        \n",
    "        sell_price = raw_df.set_index(['stock_code', 'trade_seq'])['close'].reindex(\n",
    "            pd.MultiIndex.from_arrays([selection_df['stock_code'], sell_seq], names=['stock_code', 'trade_seq'])\n",
    "        ).values\n",
    "        selection_df['sell_price'] = sell_price\n",
    "        \n",
    "        # 3. è®¡ç®—æ”¶ç›Š\n",
    "        log_msg(\"è®¡ç®—æ”¶ç›Š...\")\n",
    "        selection_df['return_rate'] = (selection_df['sell_price'] - selection_df['buy_price']) / \\\n",
    "                                    selection_df['buy_price'].replace(0, 0.0001) * 100\n",
    "        valid_mask = selection_df['buy_price'].notna() & selection_df['sell_price'].notna()\n",
    "        backtest_result = selection_df[valid_mask].copy()\n",
    "        invalid_count = len(selection_df) - len(backtest_result)\n",
    "        \n",
    "        # 4. åˆ†ç»„ç»Ÿè®¡ï¼ˆä¿®å¤åå¯æ­£å¸¸æ‰§è¡Œï¼‰\n",
    "        group_stats_df, backtest_result_with_group = calc_group_stats(backtest_result)\n",
    "        \n",
    "        # 5. æ±‡æ€»ç»Ÿè®¡\n",
    "        if len(backtest_result) > 0:\n",
    "            summary_dict = {\n",
    "                \"total_selection\": len(selection_df),\n",
    "                \"valid_trade\": len(backtest_result),\n",
    "                \"invalid_trade\": invalid_count,\n",
    "                \"valid_rate\": len(backtest_result)/len(selection_df)*100,\n",
    "                \"avg_return\": backtest_result['return_rate'].mean(),\n",
    "                \"median_return\": backtest_result['return_rate'].median(),\n",
    "                \"positive_count\": (backtest_result['return_rate']>0).sum(),\n",
    "                \"positive_ratio\": (backtest_result['return_rate']>0).mean()*100,\n",
    "                \"max_return\": backtest_result['return_rate'].max(),\n",
    "                \"min_return\": backtest_result['return_rate'].min(),\n",
    "                \"std_return\": backtest_result['return_rate'].std(),\n",
    "                \"max_drawdown\": 0\n",
    "            }\n",
    "        else:\n",
    "            summary_dict = {k: 0 for k in [\"total_selection\", \"valid_trade\", \"invalid_trade\", \"valid_rate\", \"avg_return\", \"median_return\", \"positive_count\", \"positive_ratio\", \"max_return\", \"min_return\", \"std_return\", \"max_drawdown\"]}\n",
    "        \n",
    "        # 6. ä¿å­˜ç»“æœ\n",
    "        backtest_result_with_group.to_csv(CONFIG[\"backtest_result_path\"], index=False, encoding='utf-8-sig')\n",
    "        save_summary(summary_dict, group_stats_df)\n",
    "        \n",
    "        # æ‰“å°ç»“æœ\n",
    "        log_msg(f\"\\n\" + \"=\"*60)\n",
    "        log_msg(f\"âœ… ç¨³å®šç‰ˆå›æµ‹å®Œæˆï¼æ ¸å¿ƒç»“æœï¼š\")\n",
    "        log_msg(f\"ğŸ“Š æœ‰æ•ˆäº¤æ˜“ï¼š{summary_dict['valid_trade']}æ¡ | æ­£æ”¶ç›Šæ¯”ä¾‹ï¼š{np.round(summary_dict['positive_ratio'], 2)}%\")\n",
    "        log_msg(f\"ğŸ“ˆ å¹³å‡æ”¶ç›Šï¼š{np.round(summary_dict['avg_return'], 2)}% | ä¸­ä½æ•°æ”¶ç›Šï¼š{np.round(summary_dict['median_return'], 2)}%\")\n",
    "        log_msg(f\"ğŸ“ æ˜ç»†ï¼š{CONFIG['backtest_result_path']} | æ±‡æ€»ï¼š{CONFIG['backtest_summary_path']}\")\n",
    "        log_msg(\"=\"*60)\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_msg(f\"âŒ å›æµ‹å¤±è´¥ï¼š{str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_backtest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30219a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-24 23:44:54] âœ… å›æµ‹ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\n",
      "[2025-10-24 23:44:54] åŠ è½½é€‰è‚¡ç»“æœ...\n",
      "[2025-10-24 23:44:55] âœ… é€‰è‚¡ç»“æœï¼š630æ¡è®°å½•ï¼Œ548åªè‚¡ç¥¨\n",
      "[2025-10-24 23:44:55] åŠ è½½è¡Œæƒ…æ•°æ®...\n",
      "[2025-10-24 23:45:21] åŒ¹é…ä¹°å–ä»·æ ¼å’Œå–å‡ºæ—¥...\n",
      "[2025-10-24 23:45:28] è®¡ç®—æ”¶ç›Š...\n",
      "[2025-10-24 23:45:28] å¼€å§‹è®¡ç®—æ¯æ—¥å¹³å‡æ”¶ç›Šå’Œèµ„é‡‘å¢é•¿...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16112\\1126238717.py:153: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '101349.24999999999' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  daily_return.loc[i, 'cumulative_fund'] = CONFIG[\"initial_fund\"] * daily_return.loc[i, 'daily_growth_rate']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-24 23:45:28] âœ… æ¯æ—¥æ”¶ç›Šå’Œèµ„é‡‘å¢é•¿è®¡ç®—å®Œæˆï¼šå…±110ä¸ªæœ‰æ•ˆäº¤æ˜“æ—¥\n",
      "[2025-10-24 23:45:28] âœ… å›æµ‹æ±‡æ€»ä¿å­˜ï¼š./backtest_stable_summary.txt\n",
      "[2025-10-24 23:45:28] âœ… æ¯æ—¥èµ„é‡‘å¢é•¿æ˜ç»†ä¿å­˜ï¼š./fund_growth.csv\n",
      "[2025-10-24 23:45:28] \n",
      "============================================================\n",
      "[2025-10-24 23:45:28] âœ… ç¨³å®šç‰ˆå›æµ‹å®Œæˆï¼æ ¸å¿ƒç»“æœï¼š\n",
      "[2025-10-24 23:45:28] ğŸ“Š æœ‰æ•ˆäº¤æ˜“ï¼š628æ¡ | æ­£æ”¶ç›Šæ¯”ä¾‹ï¼š50.32%\n",
      "[2025-10-24 23:45:28] ğŸ“ˆ å¹³å‡æ”¶ç›Šï¼š1.41% | ç´¯è®¡æ”¶ç›Šç‡ï¼š48.92%\n",
      "[2025-10-24 23:45:28] ğŸ’° åˆå§‹èµ„é‡‘ï¼š100000å…ƒ | æœ€ç»ˆèµ„é‡‘ï¼š148916.08å…ƒ\n",
      "[2025-10-24 23:45:28] ğŸ“ æ˜ç»†ï¼š./backtest_stable.csv | èµ„é‡‘å¢é•¿ï¼š./fund_growth.csv\n",
      "[2025-10-24 23:45:28] ============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "CONFIG = {\n",
    "    \"selection_result_path\": r'./short_term_selection_optimized.csv',\n",
    "    \"raw_data_path\": r'D:\\workspace\\xiaoyao\\data\\widetable.parquet',\n",
    "    \"backtest_result_path\": r'./backtest_stable.csv',\n",
    "    \"backtest_summary_path\": r'./backtest_stable_summary.txt',\n",
    "    \"fund_growth_path\": r'./fund_growth.csv',  # æ–°å¢ï¼šèµ„é‡‘å¢é•¿ç‡ç»“æœä¿å­˜è·¯å¾„\n",
    "    \"log_path\": r'./backtest_stable_log.txt',\n",
    "    \"trade_rule\": {\n",
    "        \"buy_delay\": 1,    # T+1ä¹°å…¥\n",
    "        \"sell_delay\": 5,   # T+5å–å‡º\n",
    "        \"min_valid_days\": 6\n",
    "    },\n",
    "    \"initial_fund\": 100000  # åˆå§‹èµ„é‡‘ï¼ˆå¯è‡ªå®šä¹‰ï¼‰\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# å·¥å…·å‡½æ•°ï¼ˆæ–°å¢èµ„é‡‘å¢é•¿è®¡ç®—ç›¸å…³ï¼‰\n",
    "# --------------------------\n",
    "def init_environment():\n",
    "    with open(CONFIG[\"log_path\"], 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"ã€ç¨³å®šç‰ˆå›æµ‹å¯åŠ¨ã€‘{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_msg(\"âœ… å›æµ‹ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "def log_msg(msg):\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_line = f\"[{timestamp}] {msg}\"\n",
    "    print(log_line)\n",
    "    with open(CONFIG[\"log_path\"], 'a', encoding='utf-8') as f:\n",
    "        f.write(log_line + \"\\n\")\n",
    "\n",
    "def calc_group_stats(df):\n",
    "    \"\"\"ä¿®å¤ï¼šç”¨np.roundå…¼å®¹floatç±»å‹ï¼Œè§£å†³roundæŠ¥é”™\"\"\"\n",
    "    # 1. å¤„ç†ç«ä»·å¾—åˆ†åˆ†ç»„\n",
    "    if 'auction_score' in df.columns:\n",
    "        try:\n",
    "            df['auction_group'] = pd.qcut(\n",
    "                df['auction_score'], \n",
    "                q=3, \n",
    "                labels=['ä½ç«ä»·å¾—åˆ†', 'ä¸­ç«ä»·å¾—åˆ†', 'é«˜ç«ä»·å¾—åˆ†'],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "        except:\n",
    "            df['auction_group'] = pd.cut(\n",
    "                df['auction_score'],\n",
    "                bins=[-0.1, 33, 66, 100.1],\n",
    "                labels=['ä½ç«ä»·å¾—åˆ†', 'ä¸­ç«ä»·å¾—åˆ†', 'é«˜ç«ä»·å¾—åˆ†']\n",
    "            )\n",
    "    else:\n",
    "        df['auction_group'] = 'æ— æ•°æ®'\n",
    "    \n",
    "    # 2. å¤„ç†é‡ä»·å¾—åˆ†åˆ†ç»„\n",
    "    if 'price_volume_score' in df.columns:\n",
    "        try:\n",
    "            df['pv_group'] = pd.qcut(\n",
    "                df['price_volume_score'],\n",
    "                q=3,\n",
    "                labels=['ä½é‡ä»·å¾—åˆ†', 'ä¸­é‡ä»·å¾—åˆ†', 'é«˜é‡ä»·å¾—åˆ†'],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "        except:\n",
    "            df['pv_group'] = pd.cut(\n",
    "                df['price_volume_score'],\n",
    "                bins=[-0.1, 33, 66, 100.1],\n",
    "                labels=['ä½é‡ä»·å¾—åˆ†', 'ä¸­é‡ä»·å¾—åˆ†', 'é«˜é‡ä»·å¾—åˆ†']\n",
    "            )\n",
    "    else:\n",
    "        df['pv_group'] = 'æ— æ•°æ®'\n",
    "    \n",
    "    # 3. åˆ†ç»„ç»Ÿè®¡ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šç”¨np.roundæ›¿ä»£roundï¼‰\n",
    "    group_stats = []\n",
    "    # ç«ä»·å¾—åˆ†åˆ†ç»„ç»Ÿè®¡\n",
    "    auction_stats = df.groupby('auction_group', observed=True).agg({\n",
    "        'return_rate': [\n",
    "            'count', \n",
    "            lambda x: np.round(x.mean(), 2),\n",
    "            lambda x: np.round(x.median(), 2),\n",
    "            lambda x: np.round((x>0).mean()*100, 2)\n",
    "        ],\n",
    "        'stock_code': lambda x: x.nunique()\n",
    "    })\n",
    "    auction_stats.columns = ['äº¤æ˜“æ•°', 'å¹³å‡æ”¶ç›Š(%)', 'ä¸­ä½æ•°æ”¶ç›Š(%)', 'æ­£æ”¶ç›Šæ¯”ä¾‹(%)', 'è‚¡ç¥¨æ•°']\n",
    "    for group in auction_stats.index:\n",
    "        group_stats.append({\n",
    "            'åˆ†ç»„ç±»å‹': 'ç«ä»·å¾—åˆ†',\n",
    "            'åˆ†ç»„': group,\n",
    "            'äº¤æ˜“æ•°': auction_stats.loc[group, 'äº¤æ˜“æ•°'],\n",
    "            'å¹³å‡æ”¶ç›Š(%)': auction_stats.loc[group, 'å¹³å‡æ”¶ç›Š(%)'],\n",
    "            'ä¸­ä½æ•°æ”¶ç›Š(%)': auction_stats.loc[group, 'ä¸­ä½æ•°æ”¶ç›Š(%)'],\n",
    "            'æ­£æ”¶ç›Šæ¯”ä¾‹(%)': auction_stats.loc[group, 'æ­£æ”¶ç›Šæ¯”ä¾‹(%)']\n",
    "        })\n",
    "    \n",
    "    # é‡ä»·å¾—åˆ†åˆ†ç»„ç»Ÿè®¡\n",
    "    pv_stats = df.groupby('pv_group', observed=True).agg({\n",
    "        'return_rate': [\n",
    "            'count', \n",
    "            lambda x: np.round(x.mean(), 2),\n",
    "            lambda x: np.round(x.median(), 2),\n",
    "            lambda x: np.round((x>0).mean()*100, 2)\n",
    "        ],\n",
    "        'stock_code': lambda x: x.nunique()\n",
    "    })\n",
    "    pv_stats.columns = ['äº¤æ˜“æ•°', 'å¹³å‡æ”¶ç›Š(%)', 'ä¸­ä½æ•°æ”¶ç›Š(%)', 'æ­£æ”¶ç›Šæ¯”ä¾‹(%)', 'è‚¡ç¥¨æ•°']\n",
    "    for group in pv_stats.index:\n",
    "        group_stats.append({\n",
    "            'åˆ†ç»„ç±»å‹': 'é‡ä»·å¾—åˆ†',\n",
    "            'åˆ†ç»„': group,\n",
    "            'äº¤æ˜“æ•°': pv_stats.loc[group, 'äº¤æ˜“æ•°'],\n",
    "            'å¹³å‡æ”¶ç›Š(%)': pv_stats.loc[group, 'å¹³å‡æ”¶ç›Š(%)'],\n",
    "            'ä¸­ä½æ•°æ”¶ç›Š(%)': pv_stats.loc[group, 'ä¸­ä½æ•°æ”¶ç›Š(%)'],\n",
    "            'æ­£æ”¶ç›Šæ¯”ä¾‹(%)': pv_stats.loc[group, 'æ­£æ”¶ç›Šæ¯”ä¾‹(%)']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(group_stats), df\n",
    "\n",
    "# --------------------------\n",
    "# æ–°å¢ï¼šæŒ‰æ—¥ç»Ÿè®¡å¹³å‡æ”¶ç›Š + è®¡ç®—èµ„é‡‘å¢é•¿ç‡\n",
    "# --------------------------\n",
    "def calc_daily_return_and_fund_growth(backtest_result):\n",
    "    \"\"\"\n",
    "    1. æŒ‰å–å‡ºæ—¥ç»Ÿè®¡å½“æ—¥æ‰€æœ‰äº¤æ˜“çš„å¹³å‡æ”¶ç›Š\n",
    "    2. æŒ‰å…¬å¼ï¼šå½“æ—¥èµ„é‡‘å¢é•¿ç‡ = 1 + 0.5 * å½“æ—¥å¹³å‡æ”¶ç›Šï¼ˆ%è½¬å°æ•°ï¼‰\n",
    "    3. è¿ä¹˜è®¡ç®—ç´¯è®¡èµ„é‡‘\n",
    "    \"\"\"\n",
    "    log_msg(\"å¼€å§‹è®¡ç®—æ¯æ—¥å¹³å‡æ”¶ç›Šå’Œèµ„é‡‘å¢é•¿...\")\n",
    "    \n",
    "    # 1. æŒ‰å–å‡ºæ—¥åˆ†ç»„ï¼Œè®¡ç®—æ¯æ—¥å¹³å‡æ”¶ç›Š\n",
    "    # å…ˆè½¬æ¢å–å‡ºæ—¥æ ¼å¼ï¼ˆç¡®ä¿æ˜¯dateç±»å‹ï¼‰\n",
    "    backtest_result['sell_date'] = pd.to_datetime(backtest_result['sell_date']).dt.date\n",
    "    # æŒ‰å–å‡ºæ—¥ç»Ÿè®¡\n",
    "    daily_return = backtest_result.groupby('sell_date').agg({\n",
    "        'return_rate': ['mean', 'count'],  # å½“æ—¥å¹³å‡æ”¶ç›Šã€å½“æ—¥äº¤æ˜“æ•°\n",
    "        'return_rate': lambda x: np.round(x.mean(), 4)  # ä¿ç•™4ä½å°æ•°çš„å¹³å‡æ”¶ç›Š\n",
    "    }).reset_index()\n",
    "    daily_return.columns = ['sell_date', 'daily_avg_return']  # é‡å‘½ååˆ—\n",
    "    # è¿‡æ»¤å½“æ—¥äº¤æ˜“æ•°<2çš„å¼‚å¸¸æ•°æ®ï¼ˆé¿å…å•æ¡äº¤æ˜“å½±å“ï¼‰\n",
    "    daily_return = daily_return[backtest_result.groupby('sell_date')['return_rate'].count().values >= 2].reset_index(drop=True)\n",
    "    \n",
    "    # 2. è®¡ç®—æ¯æ—¥èµ„é‡‘å¢é•¿ç‡ï¼ˆå…¬å¼ï¼š1 + 0.5 * å½“æ—¥å¹³å‡æ”¶ç›Š/100ï¼‰\n",
    "    daily_return['daily_growth_rate'] = 1 + 0.5 * (daily_return['daily_avg_return'] / 100)\n",
    "    # æŒ‰æ—¥æœŸæ’åºï¼ˆç¡®ä¿è¿ä¹˜é¡ºåºæ­£ç¡®ï¼‰\n",
    "    daily_return = daily_return.sort_values('sell_date').reset_index(drop=True)\n",
    "    \n",
    "    # 3. è¿ä¹˜è®¡ç®—ç´¯è®¡èµ„é‡‘\n",
    "    # åˆå§‹èµ„é‡‘\n",
    "    daily_return['cumulative_fund'] = CONFIG[\"initial_fund\"]\n",
    "    # ä»ç¬¬2è¡Œå¼€å§‹è¿ä¹˜ï¼ˆç¬¬1è¡Œç´¯è®¡èµ„é‡‘=åˆå§‹èµ„é‡‘*å½“æ—¥å¢é•¿ç‡ï¼‰\n",
    "    for i in range(len(daily_return)):\n",
    "        if i == 0:\n",
    "            daily_return.loc[i, 'cumulative_fund'] = CONFIG[\"initial_fund\"] * daily_return.loc[i, 'daily_growth_rate']\n",
    "        else:\n",
    "            daily_return.loc[i, 'cumulative_fund'] = daily_return.loc[i-1, 'cumulative_fund'] * daily_return.loc[i, 'daily_growth_rate']\n",
    "    \n",
    "    # æ ¼å¼åŒ–æ•°å€¼ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰\n",
    "    daily_return['daily_avg_return'] = np.round(daily_return['daily_avg_return'], 2)\n",
    "    daily_return['daily_growth_rate'] = np.round(daily_return['daily_growth_rate'], 4)\n",
    "    daily_return['cumulative_fund'] = np.round(daily_return['cumulative_fund'], 2)\n",
    "    \n",
    "    log_msg(f\"âœ… æ¯æ—¥æ”¶ç›Šå’Œèµ„é‡‘å¢é•¿è®¡ç®—å®Œæˆï¼šå…±{len(daily_return)}ä¸ªæœ‰æ•ˆäº¤æ˜“æ—¥\")\n",
    "    return daily_return\n",
    "\n",
    "# --------------------------\n",
    "# ä¿®æ”¹ï¼šä¿å­˜æ±‡æ€»ç»“æœï¼ˆæ–°å¢èµ„é‡‘å¢é•¿éƒ¨åˆ†ï¼‰\n",
    "# --------------------------\n",
    "def save_summary(summary_dict, group_stats_df, daily_return):\n",
    "    \"\"\"ä¿å­˜æ±‡æ€»ç»“æœï¼ˆæ–°å¢èµ„é‡‘å¢é•¿ç»Ÿè®¡ï¼‰\"\"\"\n",
    "    # èµ„é‡‘å¢é•¿æ ¸å¿ƒæŒ‡æ ‡\n",
    "    total_trading_days = len(daily_return)\n",
    "    final_fund = daily_return['cumulative_fund'].iloc[-1] if total_trading_days > 0 else CONFIG[\"initial_fund\"]\n",
    "    total_return_rate = (final_fund - CONFIG[\"initial_fund\"]) / CONFIG[\"initial_fund\"] * 100\n",
    "    avg_daily_growth = (final_fund / CONFIG[\"initial_fund\"]) ** (1 / total_trading_days) - 1 if total_trading_days > 0 else 0\n",
    "    \n",
    "    # åˆ†ç»„ç»Ÿè®¡æ–‡æœ¬\n",
    "    group_text = \"\\n4. åˆ†ç»„ç»Ÿè®¡\\n\"\n",
    "    group_text += \"=\"*60 + \"\\n\"\n",
    "    group_text += group_stats_df.to_string(index=False, na_rep='-') + \"\\n\"\n",
    "    \n",
    "    # èµ„é‡‘å¢é•¿ç»Ÿè®¡æ–‡æœ¬\n",
    "    fund_text = \"\\n5. èµ„é‡‘å¢é•¿ç»Ÿè®¡\\n\"\n",
    "    fund_text += \"=\"*60 + \"\\n\"\n",
    "    fund_text += f\"åˆå§‹èµ„é‡‘ï¼š{CONFIG['initial_fund']:.2f}å…ƒ\\n\"\n",
    "    fund_text += f\"æœ€ç»ˆèµ„é‡‘ï¼š{final_fund:.2f}å…ƒ\\n\"\n",
    "    fund_text += f\"ç´¯è®¡æ”¶ç›Šç‡ï¼š{np.round(total_return_rate, 2)}%\\n\"\n",
    "    fund_text += f\"æœ‰æ•ˆäº¤æ˜“å¤©æ•°ï¼š{total_trading_days}å¤©\\n\"\n",
    "    fund_text += f\"æ—¥å‡èµ„é‡‘å¢é•¿ç‡ï¼š{np.round(avg_daily_growth*100, 4)}%\\n\"\n",
    "    fund_text += f\"æœ€é«˜ç´¯è®¡èµ„é‡‘ï¼š{daily_return['cumulative_fund'].max():.2f}å…ƒ\\n\"\n",
    "    fund_text += f\"æœ€ä½ç´¯è®¡èµ„é‡‘ï¼š{daily_return['cumulative_fund'].min():.2f}å…ƒ\\n\"\n",
    "    \n",
    "    # æ±‡æ€»å†…å®¹\n",
    "    summary_content = f\"\"\"\n",
    "ã€ç¨³å®šç‰ˆå›æµ‹ç»“æœã€‘\n",
    "==========================\n",
    "å›æµ‹è§„åˆ™ï¼šTæ—¥é€‰è‚¡ â†’ T+{CONFIG['trade_rule']['buy_delay']}ä¹°å…¥ â†’ T+{CONFIG['trade_rule']['sell_delay']}å–å‡º\n",
    "==========================\n",
    "1. åŸºç¡€ç»Ÿè®¡\n",
    "   - é€‰è‚¡æ€»è®°å½•ï¼š{summary_dict['total_selection']} æ¡\n",
    "   - æœ‰æ•ˆäº¤æ˜“ï¼š{summary_dict['valid_trade']} æ¡\n",
    "   - æ— æ•ˆäº¤æ˜“ï¼š{summary_dict['invalid_trade']} æ¡\n",
    "   - æœ‰æ•ˆç‡ï¼š{np.round(summary_dict['valid_rate'], 2)}%\n",
    "\n",
    "2. æ”¶ç›Šç»Ÿè®¡\n",
    "   - å¹³å‡æ”¶ç›Šç‡ï¼š{np.round(summary_dict['avg_return'], 2)}%\n",
    "   - ä¸­ä½æ•°æ”¶ç›Šç‡ï¼š{np.round(summary_dict['median_return'], 2)}%\n",
    "   - æ­£æ”¶ç›Šæ¯”ä¾‹ï¼š{np.round(summary_dict['positive_ratio'], 2)}%ï¼ˆ{summary_dict['positive_count']}/{summary_dict['valid_trade']}ï¼‰\n",
    "   - æœ€å¤§æ”¶ç›Šï¼š{np.round(summary_dict['max_return'], 2)}%\n",
    "   - æœ€å°æ”¶ç›Šï¼š{np.round(summary_dict['min_return'], 2)}%\n",
    "\n",
    "3. é£é™©ç»Ÿè®¡\n",
    "   - æ”¶ç›Šæ ‡å‡†å·®ï¼š{np.round(summary_dict['std_return'], 2)}%\n",
    "   - æœ€å¤§å›æ’¤ï¼š{np.round(summary_dict['max_drawdown'], 2)}%ï¼ˆç®€åŒ–è®¡ç®—ï¼‰\n",
    "{group_text}{fund_text}\n",
    "==========================\"\"\"\n",
    "    with open(CONFIG[\"backtest_summary_path\"], 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_content)\n",
    "    # ä¿å­˜æ¯æ—¥èµ„é‡‘å¢é•¿æ˜ç»†\n",
    "    daily_return.to_csv(CONFIG[\"fund_growth_path\"], index=False, encoding='utf-8-sig')\n",
    "    log_msg(f\"âœ… å›æµ‹æ±‡æ€»ä¿å­˜ï¼š{CONFIG['backtest_summary_path']}\")\n",
    "    log_msg(f\"âœ… æ¯æ—¥èµ„é‡‘å¢é•¿æ˜ç»†ä¿å­˜ï¼š{CONFIG['fund_growth_path']}\")\n",
    "\n",
    "# --------------------------\n",
    "# ä¸»å›æµ‹é€»è¾‘ï¼ˆæ–°å¢å–å‡ºæ—¥è®¡ç®—+èµ„é‡‘å¢é•¿è°ƒç”¨ï¼‰\n",
    "# --------------------------\n",
    "def run_backtest():\n",
    "    try:\n",
    "        init_environment()\n",
    "        \n",
    "        # 1. åŠ è½½æ•°æ®\n",
    "        log_msg(\"åŠ è½½é€‰è‚¡ç»“æœ...\")\n",
    "        selection_df = pd.read_csv(CONFIG[\"selection_result_path\"])\n",
    "        selection_df['date'] = pd.to_datetime(selection_df['date']).dt.date  # Tæ—¥ï¼ˆé€‰è‚¡æ—¥ï¼‰\n",
    "        log_msg(f\"âœ… é€‰è‚¡ç»“æœï¼š{len(selection_df)}æ¡è®°å½•ï¼Œ{selection_df['stock_code'].nunique()}åªè‚¡ç¥¨\")\n",
    "        \n",
    "        log_msg(\"åŠ è½½è¡Œæƒ…æ•°æ®...\")\n",
    "        raw_df = pd.read_parquet(CONFIG[\"raw_data_path\"])\n",
    "        raw_df['date'] = pd.to_datetime(raw_df['date']).dt.date\n",
    "        raw_df = raw_df[['stock_code', 'date', 'close']].dropna(subset=['close'])\n",
    "        raw_df = raw_df.sort_values(['stock_code', 'date']).reset_index(drop=True)\n",
    "        # æ–°å¢ï¼šä¸ºè¡Œæƒ…æ•°æ®æ·»åŠ äº¤æ˜“åºåˆ—ï¼ˆç”¨äºè®¡ç®—å–å‡ºæ—¥ï¼‰\n",
    "        raw_df['trade_seq'] = raw_df.groupby('stock_code').cumcount()\n",
    "        # æ„å»ºåºåˆ—â†’æ—¥æœŸæ˜ å°„ï¼ˆç”¨äºæ ¹æ®sell_seqè·å–å–å‡ºæ—¥ï¼‰\n",
    "        seq_date_map = raw_df.set_index(['stock_code', 'trade_seq'])['date'].to_dict()\n",
    "        \n",
    "        # 2. åŒ¹é…ä¹°å–ä»·æ ¼+è®¡ç®—å–å‡ºæ—¥\n",
    "        log_msg(\"åŒ¹é…ä¹°å–ä»·æ ¼å’Œå–å‡ºæ—¥...\")\n",
    "        # åˆå¹¶é€‰è‚¡æ—¥çš„äº¤æ˜“åºåˆ—\n",
    "        selection_df = selection_df.merge(\n",
    "            raw_df[['stock_code', 'date', 'trade_seq']],\n",
    "            on=['stock_code', 'date'],\n",
    "            how='left'\n",
    "        ).dropna(subset=['trade_seq'])\n",
    "        selection_df['trade_seq'] = selection_df['trade_seq'].astype(int)\n",
    "        \n",
    "        # è®¡ç®—ä¹°å–åºåˆ—å’Œä»·æ ¼\n",
    "        buy_seq = selection_df['trade_seq'] + CONFIG['trade_rule']['buy_delay']\n",
    "        sell_seq = selection_df['trade_seq'] + CONFIG['trade_rule']['sell_delay']\n",
    "        \n",
    "        # åŒ¹é…ä¹°å…¥ä»·ã€å–å‡ºä»·\n",
    "        price_map = raw_df.set_index(['stock_code', 'trade_seq'])['close'].to_dict()\n",
    "        selection_df['buy_price'] = [price_map.get((code, seq), np.nan) for code, seq in zip(selection_df['stock_code'], buy_seq)]\n",
    "        selection_df['sell_price'] = [price_map.get((code, seq), np.nan) for code, seq in zip(selection_df['stock_code'], sell_seq)]\n",
    "        \n",
    "        # æ–°å¢ï¼šåŒ¹é…å–å‡ºæ—¥ï¼ˆç”¨äºæŒ‰æ—¥ç»Ÿè®¡æ”¶ç›Šï¼‰\n",
    "        selection_df['sell_date'] = [seq_date_map.get((code, seq), np.nan) for code, seq in zip(selection_df['stock_code'], sell_seq)]\n",
    "        \n",
    "        # 3. è®¡ç®—æ”¶ç›Š\n",
    "        log_msg(\"è®¡ç®—æ”¶ç›Š...\")\n",
    "        selection_df['return_rate'] = (selection_df['sell_price'] - selection_df['buy_price']) / \\\n",
    "                                    selection_df['buy_price'].replace(0, 0.0001) * 100\n",
    "        valid_mask = selection_df['buy_price'].notna() & selection_df['sell_price'].notna() & selection_df['sell_date'].notna()\n",
    "        backtest_result = selection_df[valid_mask].copy()\n",
    "        invalid_count = len(selection_df) - len(backtest_result)\n",
    "        \n",
    "        # 4. åˆ†ç»„ç»Ÿè®¡\n",
    "        group_stats_df, backtest_result_with_group = calc_group_stats(backtest_result)\n",
    "        \n",
    "        # 5. æ–°å¢ï¼šè®¡ç®—æ¯æ—¥æ”¶ç›Šå’Œèµ„é‡‘å¢é•¿\n",
    "        daily_return = calc_daily_return_and_fund_growth(backtest_result)\n",
    "        \n",
    "        # 6. æ±‡æ€»ç»Ÿè®¡\n",
    "        if len(backtest_result) > 0:\n",
    "            # è®¡ç®—æœ€å¤§å›æ’¤ï¼ˆç®€åŒ–ï¼šåŸºäºç´¯è®¡èµ„é‡‘çš„å›æ’¤ï¼‰\n",
    "            if len(daily_return) > 0:\n",
    "                cumulative_fund = daily_return['cumulative_fund'].values\n",
    "                drawdown = [(cumulative_fund[i] - cumulative_fund[:i+1].max()) / cumulative_fund[:i+1].max() for i in range(len(cumulative_fund))]\n",
    "                max_drawdown = min(drawdown) * 100 if drawdown else 0\n",
    "            else:\n",
    "                max_drawdown = 0\n",
    "            \n",
    "            summary_dict = {\n",
    "                \"total_selection\": len(selection_df),\n",
    "                \"valid_trade\": len(backtest_result),\n",
    "                \"invalid_trade\": invalid_count,\n",
    "                \"valid_rate\": len(backtest_result)/len(selection_df)*100,\n",
    "                \"avg_return\": backtest_result['return_rate'].mean(),\n",
    "                \"median_return\": backtest_result['return_rate'].median(),\n",
    "                \"positive_count\": (backtest_result['return_rate']>0).sum(),\n",
    "                \"positive_ratio\": (backtest_result['return_rate']>0).mean()*100,\n",
    "                \"max_return\": backtest_result['return_rate'].max(),\n",
    "                \"min_return\": backtest_result['return_rate'].min(),\n",
    "                \"std_return\": backtest_result['return_rate'].std(),\n",
    "                \"max_drawdown\": max_drawdown  # ç”¨ç´¯è®¡èµ„é‡‘è®¡ç®—çš„æœ€å¤§å›æ’¤\n",
    "            }\n",
    "        else:\n",
    "            summary_dict = {k: 0 for k in [\"total_selection\", \"valid_trade\", \"invalid_trade\", \"valid_rate\", \"avg_return\", \"median_return\", \"positive_count\", \"positive_ratio\", \"max_return\", \"min_return\", \"std_return\", \"max_drawdown\"]}\n",
    "        \n",
    "        # 7. ä¿å­˜ç»“æœ\n",
    "        backtest_result_with_group.to_csv(CONFIG[\"backtest_result_path\"], index=False, encoding='utf-8-sig')\n",
    "        save_summary(summary_dict, group_stats_df, daily_return)\n",
    "        \n",
    "        # æ‰“å°æ ¸å¿ƒç»“æœ\n",
    "        log_msg(f\"\\n\" + \"=\"*60)\n",
    "        log_msg(f\"âœ… ç¨³å®šç‰ˆå›æµ‹å®Œæˆï¼æ ¸å¿ƒç»“æœï¼š\")\n",
    "        log_msg(f\"ğŸ“Š æœ‰æ•ˆäº¤æ˜“ï¼š{summary_dict['valid_trade']}æ¡ | æ­£æ”¶ç›Šæ¯”ä¾‹ï¼š{np.round(summary_dict['positive_ratio'], 2)}%\")\n",
    "        log_msg(f\"ğŸ“ˆ å¹³å‡æ”¶ç›Šï¼š{np.round(summary_dict['avg_return'], 2)}% | ç´¯è®¡æ”¶ç›Šç‡ï¼š{np.round((daily_return['cumulative_fund'].iloc[-1]/CONFIG['initial_fund']-1)*100, 2)}%\")\n",
    "        log_msg(f\"ğŸ’° åˆå§‹èµ„é‡‘ï¼š{CONFIG['initial_fund']}å…ƒ | æœ€ç»ˆèµ„é‡‘ï¼š{daily_return['cumulative_fund'].iloc[-1]:.2f}å…ƒ\")\n",
    "        log_msg(f\"ğŸ“ æ˜ç»†ï¼š{CONFIG['backtest_result_path']} | èµ„é‡‘å¢é•¿ï¼š{CONFIG['fund_growth_path']}\")\n",
    "        log_msg(\"=\"*60)\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_msg(f\"âŒ å›æµ‹å¤±è´¥ï¼š{str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_backtest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757a660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
