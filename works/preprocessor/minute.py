# ä»Jupyter Notebookè½¬æ¢è€Œæ¥çš„Pythonä»£ç 
# åŸå§‹æ–‡ä»¶ï¼šD:\workspace\xiaoyao\works\preprocessor\minute.ipynb



# ----------------------------------------------------------------------import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import os
import numpy as np
from tqdm import tqdm
from joblib import Parallel, delayed
import gc

# -------------------------- é…ç½®å‚æ•° --------------------------
MINUTE_ROOT_PATH = "D:/workspace/xiaoyao/data/stock_minutely_price"
OUTPUT_PATH = "D:/workspace/xiaoyao/data/minutely_processed"
START_DATE = pd.to_datetime("2025-01-01")
END_DATE = pd.to_datetime("2025-10-27")
# å…³é”®ï¼šæ–°å¢stock_codeåˆ°CORE_COLSï¼Œç¡®ä¿è¯»å–æ—¶ä¸€å¹¶å¤„ç†å…¶ç±»å‹
CORE_COLS = ["stock_code", "time", "close", "high", "low", "volume"]  
N_JOBS = 4

# -------------------------- 1. å•è‚¡ç¥¨å¤„ç†ï¼ˆå½»åº•ä¿®å¤ç±»å‹å†²çªï¼‰ --------------------------
def process_stock(stock_dir):
    stock_code = stock_dir.split("=")[1]
    stock_path = os.path.join(MINUTE_ROOT_PATH, stock_dir, "data.parquet")
    
    # 1. è¯»å–å¹¶è½¬æ¢æ‰€æœ‰å­—æ®µç±»å‹ï¼ˆå«stock_codeï¼‰
    try:
        # æ­¥éª¤1ï¼šå…ˆè¯»å–æ•´ä¸ªæ–‡ä»¶çš„schemaï¼ŒæŸ¥çœ‹æ‰€æœ‰å­—æ®µç±»å‹
        parquet_file = pq.ParquetFile(stock_path)
        full_schema = parquet_file.schema.to_arrow_schema()
        
        # æ­¥éª¤2ï¼šé‡å»ºschemaï¼Œå°†æ‰€æœ‰dictionaryç±»å‹è½¬ä¸ºstringï¼ˆé‡ç‚¹å¤„ç†stock_codeï¼‰
        new_fields = []
        for field in full_schema:
            field_name = field.name
            # æ— è®ºå“ªä¸ªå­—æ®µï¼Œåªè¦æ˜¯dictionaryç±»å‹ï¼Œéƒ½è½¬ä¸ºstring
            if str(field.type).startswith("dictionary"):
                new_field = pa.field(field_name, pa.string())
                new_fields.append(new_field)
            else:
                new_fields.append(field)
        new_full_schema = pa.schema(new_fields)
        
        # æ­¥éª¤3ï¼šç”¨æ–°schemaè¯»å–æ•°æ®ï¼ˆç¡®ä¿stock_codeå·²è½¬ä¸ºstringï¼‰
        table = pq.read_table(stock_path, schema=new_full_schema, columns=CORE_COLS)
    
    except Exception as e:
        print(f"âŒ è‚¡ç¥¨ {stock_code} è¯»å–å¤±è´¥ï¼š{str(e)}")
        return
    
    # 2. æ•°æ®ç­›é€‰ï¼ˆæ— éœ€å†è¡¥å……stock_codeï¼Œæ–‡ä»¶ä¸­å·²è¯»å–å¹¶è½¬æ¢ï¼‰
    try:
        min_df = table.to_pandas()
        # æ—¶é—´æ ¼å¼å¤„ç†
        min_df["time"] = pd.to_datetime(min_df["time"])
        min_df["trade_date"] = min_df["time"].dt.date.astype("datetime64[ns]")
        # ç­›é€‰æ—¶é—´èŒƒå›´
        mask = (min_df["trade_date"] >= START_DATE) & (min_df["trade_date"] <= END_DATE)
        min_df = min_df[mask].copy()
    except Exception as e:
        print(f"âŒ è‚¡ç¥¨ {stock_code} æ•°æ®å¤„ç†å¤±è´¥ï¼š{str(e)}")
        return
    
    # 3. æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if len(min_df) == 0:
        print(f"âš ï¸  è‚¡ç¥¨ {stock_code} æ— æŒ‡å®šæ—¶é—´èŒƒå›´æ•°æ®ï¼Œè·³è¿‡")
        return
    else:
        print(f"â„¹ï¸  è‚¡ç¥¨ {stock_code} æœ‰æ•ˆæ•°æ®ï¼š{len(min_df)} æ¡ï¼Œè¦†ç›– {min_df['trade_date'].nunique()} å¤©")
    
    # 4. è®¡ç®—æ‰©å±•æŒ‡æ ‡ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
    try:
        min_df = calc_enhanced_indicators(min_df)
    except Exception as e:
        print(f"âŒ è‚¡ç¥¨ {stock_code} æŒ‡æ ‡è®¡ç®—å¤±è´¥ï¼š{str(e)}")
        return
    
    # 5. å†™å…¥äºŒçº§åˆ†åŒºï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
    try:
        save_with_double_partition(min_df, OUTPUT_PATH)
        print(f"âœ… è‚¡ç¥¨ {stock_code} å¤„ç†å®Œæˆ")
    except Exception as e:
        print(f"âŒ è‚¡ç¥¨ {stock_code} å†™å…¥å¤±è´¥ï¼š{str(e)}")
        return
    
    del min_df, table
    gc.collect()
    return

# -------------------------- 2. æ‰©å±•æŒ‡æ ‡è®¡ç®—ï¼ˆä¸å˜ï¼‰ --------------------------
def calc_enhanced_indicators(min_df):
    min_df = min_df.sort_values(["trade_date", "time"]).reset_index(drop=True)
    
    # æŒ‡æ ‡1ï¼šæ—©ç›˜æˆäº¤é‡å æ¯”
    morning_mask = (min_df["time"].dt.hour == 9) & (min_df["time"].dt.minute >= 30) | \
                   (min_df["time"].dt.hour == 10) & (min_df["time"].dt.minute < 30)
    min_df["daily_total_vol"] = min_df.groupby("trade_date")["volume"].transform("sum")
    min_df["morning_vol"] = min_df[morning_mask].groupby("trade_date")["volume"].transform("sum").fillna(0)
    min_df["morning_vol_ratio"] = (min_df["morning_vol"] / min_df["daily_total_vol"].replace(0, np.nan) * 100).fillna(0)
    
    # æŒ‡æ ‡2ï¼šå°¾ç›˜ä¼ç¨³ä¿¡å·
    afternoon_mask = (min_df["time"].dt.hour == 14) & (min_df["time"].dt.minute >= 30) | \
                     (min_df["time"].dt.hour == 15) & (min_df["time"].dt.minute == 0)
    min_df["is_afternoon"] = afternoon_mask.astype(int)
    min_df["close_diff"] = min_df.groupby("trade_date")["close"].diff().fillna(0)
    min_df["up_streak"] = 0
    afternoon_groups = min_df[min_df["is_afternoon"] == 1].groupby("trade_date")
    for name, group in afternoon_groups:
        streak = 0
        streaks = []
        for diff in group["close_diff"]:
            streak = streak + 1 if diff > 0 else 0
            streaks.append(streak)
        min_df.loc[group.index, "up_streak"] = streaks
    min_df["afternoon_stable"] = (min_df.groupby("trade_date")["up_streak"].transform("max") >= 5).astype(int)
    
    # æŒ‡æ ‡3ï¼šæ—¥å†…æŒ¯å¹…
    min_df["intraday_amplitude"] = (
        (min_df.groupby("trade_date")["high"].transform("max") - 
         min_df.groupby("trade_date")["low"].transform("min")) / 
        min_df.groupby("trade_date")["low"].transform("min") * 100
    ).fillna(0)
    
    # æŒ‡æ ‡4ï¼šé‡ä»·åŒæ­¥æ€§
    min_df["vol_diff"] = min_df.groupby("trade_date")["volume"].diff().fillna(0)
    min_df["vol_price_sync"] = (min_df["close_diff"] * min_df["vol_diff"] > 0).astype(int)
    
    # æŒ‡æ ‡5ï¼šæ”¶ç›˜ä»·é è¿‘æœ€é«˜ä»·æ¯”ä¾‹
    min_df["close_to_high_ratio"] = (
        min_df["close"] / min_df.groupby("trade_date")["high"].transform("max") * 100
    ).fillna(0)
    
    # ä¿ç•™å¿…è¦å­—æ®µï¼ˆæ³¨æ„ï¼šstock_codeå·²ä»æ–‡ä»¶è¯»å–ï¼Œæ— éœ€é¢å¤–æ·»åŠ ï¼‰
    keep_cols = [
        "stock_code", "trade_date", "time", "close", "volume",
        "morning_vol_ratio", "afternoon_stable", "intraday_amplitude",
        "vol_price_sync", "close_to_high_ratio"
    ]
    return min_df[keep_cols]

# -------------------------- 3. äºŒçº§åˆ†åŒºå†™å…¥ï¼ˆä¸å˜ï¼‰ --------------------------
def save_with_double_partition(df, output_root):
    stock_code = df["stock_code"].iloc[0]
    # ä¸€çº§ç›®å½•ï¼šstock_code=XXX
    stock_dir = os.path.join(output_root, f"stock_code={stock_code}")
    os.makedirs(stock_dir, exist_ok=True)
    
    # äºŒçº§ç›®å½•ï¼štrade_date=XXX
    for trade_date, day_data in df.groupby("trade_date"):
        date_str = trade_date.strftime("%Y-%m-%d")
        date_dir = os.path.join(stock_dir, f"trade_date={date_str}")
        os.makedirs(date_dir, exist_ok=True)
        
        # å†™å…¥æ–‡ä»¶
        output_file = os.path.join(date_dir, "min_data.parquet")
        day_data.to_parquet(output_file, engine="pyarrow", index=False, compression=None)
    
    return

# -------------------------- 4. ä¸»å‡½æ•°ï¼ˆå•è¿›ç¨‹æµ‹è¯•+å¤šè¿›ç¨‹ï¼‰ --------------------------
def main():
    print("="*60)
    print("ç¬¬ä¸€æ­¥ï¼šå•è¿›ç¨‹æµ‹è¯•å‰10åªè‚¡ç¥¨ï¼ˆä¿®å¤ç±»å‹å†²çªï¼‰")
    print("="*60)
    
    if not os.path.exists(OUTPUT_PATH):
        os.makedirs(OUTPUT_PATH)
        print(f"âœ… å·²åˆ›å»ºè¾“å‡ºæ ¹ç›®å½•ï¼š{OUTPUT_PATH}")
    else:
        print(f"âœ… è¾“å‡ºæ ¹ç›®å½•å·²å­˜åœ¨ï¼š{OUTPUT_PATH}")
    
    try:
        all_stock_dirs = [d for d in os.listdir(MINUTE_ROOT_PATH) if d.startswith("stock_code=")]
        total_stocks = len(all_stock_dirs)
        print(f"âœ… å‘ç° {total_stocks} åªè‚¡ç¥¨ç›®å½•ï¼Œæµ‹è¯•å‰10åª...")
        
        test_success = 0
        for stock_dir in all_stock_dirs[:10]:
            stock_code = stock_dir.split("=")[1]
            try:
                process_stock(stock_dir)
                # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆæ–‡ä»¶
                stock_output_dir = os.path.join(OUTPUT_PATH, f"stock_code={stock_code}")
                if os.path.exists(stock_output_dir) and len(os.listdir(stock_output_dir)) > 0:
                    test_success += 1
            except Exception as e:
                print(f"âš ï¸  æµ‹è¯• {stock_code} å¼‚å¸¸ï¼š{str(e)}")
        
        print(f"\nå•è¿›ç¨‹æµ‹è¯•ç»“æœï¼š{test_success}/10 åªè‚¡ç¥¨å¤„ç†æˆåŠŸ")
        if test_success == 0:
            print("âŒ å•è¿›ç¨‹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æˆ–å­—æ®µç±»å‹ï¼")
            return
        else:
            print("âœ… å•è¿›ç¨‹æµ‹è¯•é€šè¿‡ï¼Œå‡†å¤‡å¤šè¿›ç¨‹å¤„ç†...")
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•é˜¶æ®µå¤±è´¥ï¼š{str(e)}")
        return
    
    # å¤šè¿›ç¨‹å¤„ç†å‰©ä½™è‚¡ç¥¨
    print("\n" + "="*60)
    print("ç¬¬äºŒæ­¥ï¼šå¤šè¿›ç¨‹å¤„ç†å‰©ä½™è‚¡ç¥¨")
    print("="*60)
    
    Parallel(
        n_jobs=N_JOBS,
        verbose=10,
        batch_size=2,
        backend="threading"
    )(
        delayed(process_stock)(stock_dir) for stock_dir in all_stock_dirs[10:]
    )
    
    # ç»Ÿè®¡ç»“æœ
    processed_stocks = 0
    for stock_dir in all_stock_dirs:
        stock_code = stock_dir.split("=")[1]
        stock_output_dir = os.path.join(OUTPUT_PATH, f"stock_code={stock_code}")
        if os.path.exists(stock_output_dir) and len(os.listdir(stock_output_dir)) > 0:
            processed_stocks += 1
    
    print("\n" + "="*60)
    print("æ‰€æœ‰å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼šæ€»è‚¡ç¥¨æ•°={total_stocks}ï¼ŒæˆåŠŸå¤„ç†={processed_stocks}ï¼ŒæˆåŠŸç‡={processed_stocks/total_stocks:.2%}")
    print(f"ç»“æœè·¯å¾„ï¼š{OUTPUT_PATH}")
    print("="*60)

if __name__ == "__main__":
    main()

