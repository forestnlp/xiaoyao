# ä»Jupyter Notebookè½¬æ¢è€Œæ¥çš„Pythonä»£ç 
# åŸå§‹æ–‡ä»¶ï¼šD:\workspace\xiaoyao\works\preprocessor\factors.ipynb



# ----------------------------------------------------------------------import pandas as pd
import numpy as np
import talib as ta
import os

# --------------------------
# é…ç½®å‚æ•°ï¼ˆæœ¬åœ°å­˜å‚¨è·¯å¾„ï¼‰
# --------------------------
CONFIG = {
    "raw_data_path": r'D:\workspace\xiaoyao\data\widetable.parquet',  # åŸå§‹widetableè·¯å¾„
    "factortable_output_path": r'./factortable.parquet',  # æœ¬åœ°è¾“å‡ºè·¯å¾„
    "sample_output_path": r'./factortable_sample.csv',  # æ ·æœ¬æ–‡ä»¶
    "log_path": r'./calc_factortable_log.txt'  # æ—¥å¿—è·¯å¾„
}

# --------------------------
# å·¥å…·å‡½æ•°
# --------------------------
def init_environment():
    """åˆå§‹åŒ–æœ¬åœ°ç›®å½•å’Œæ—¥å¿—"""
    os.makedirs(os.path.dirname(CONFIG["factortable_output_path"]), exist_ok=True)
    with open(CONFIG["log_path"], 'w', encoding='utf-8') as f:
        f.write(f"ã€Factortableè®¡ç®—å¯åŠ¨ã€‘{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    log_msg("âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºè·¯å¾„ï¼š./factortable.parquet")

def log_msg(msg):
    """æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    log_line = f"[{timestamp}] {msg}"
    print(log_line)
    with open(CONFIG["log_path"], 'a', encoding='utf-8') as f:
        f.write(log_line + "\n")

# --------------------------
# æ ¸å¿ƒé€»è¾‘ï¼šè®¡ç®—å®éªŒæ‰€éœ€æŒ‡æ ‡
# --------------------------
def calculate_factortable():
    try:
        init_environment()
        
        # 1. åŠ è½½widetableå¹¶é¢„å¤„ç†ï¼ˆå‚è€ƒæ—§ä»£ç é€»è¾‘ï¼‰
        log_msg("åŠ è½½widetableæ•°æ®...")
        df = pd.read_parquet(CONFIG["raw_data_path"])
        
        # éªŒè¯å¿…éœ€å­—æ®µï¼ˆä¸æ—§ä»£ç ä¸€è‡´ï¼‰
        must_have_cols = ['stock_code', 'date', 'close', 'open', 'volume', 'high', 'low']
        missing_cols = [col for col in must_have_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"widetableç¼ºå°‘å¿…éœ€å­—æ®µï¼š{missing_cols}")
        log_msg(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼š{len(df)}æ¡è®°å½•ï¼Œ{df['stock_code'].nunique()}åªè‚¡ç¥¨")
        
        # é¢„å¤„ç†ï¼ˆæ’åº+æ—¥æœŸè½¬æ¢+å»é‡ï¼‰
        df = df.sort_values(by=['stock_code', 'date']).reset_index(drop=True)
        df['date'] = pd.to_datetime(df['date'], errors='coerce')  # å­—ç¬¦ä¸²è½¬datetime
        df = df.dropna(subset=['date', 'close', 'open'])  # è¿‡æ»¤æ— æ•ˆæ—¥æœŸå’Œä»·æ ¼
        df = df.drop_duplicates(subset=['stock_code', 'date'], keep='first')  # å»é‡
        log_msg(f"âœ… é¢„å¤„ç†å®Œæˆï¼š{len(df)}æ¡æœ‰æ•ˆè®°å½•")

        # 2. è®¡ç®—å®éªŒæ‰€éœ€æ ¸å¿ƒæŒ‡æ ‡ï¼ˆå‚è€ƒæ—§ä»£ç ï¼Œä»…ä¿ç•™å¿…è¦æŒ‡æ ‡ï¼‰
        log_msg("è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡...")
        
        # ï¼ˆ1ï¼‰è¿ç»­ä¸Šæ¶¨å¤©æ•°ï¼ˆä¸æ—§ä»£ç ä¸€è‡´ï¼‰
        def calc_consecutive_up_days(close_series):
            up = close_series > close_series.shift(1)
            consecutive_up = up.groupby(up.ne(up.shift()).cumsum()).cumsum()
            return consecutive_up.astype(int)
        df['consecutive_up_days'] = df.groupby('stock_code', group_keys=False)['close'].apply(calc_consecutive_up_days)
        log_msg("âœ… è¿ç»­ä¸Šæ¶¨å¤©æ•°è®¡ç®—å®Œæˆ")
        
        # ï¼ˆ2ï¼‰ç«ä»·ç›¸å…³æŒ‡æ ‡ï¼ˆä¸æ—§ä»£ç ä¸€è‡´ï¼Œé€‚é…widetableçš„auc_volumeï¼‰
        if 'auc_volume' in df.columns:
            df['auction_volume_ratio'] = df['auc_volume'] / \
                                       df.groupby('stock_code')['volume'].shift(1).replace(0, 0.0001)
        else:
            df['auction_volume_ratio'] = np.nan
            log_msg("âš ï¸ æ— auc_volumeå­—æ®µï¼Œauction_volume_ratioå¡«å……ä¸ºNaN")
        df['auction_rise_ratio'] = (df['open'] - df.groupby('stock_code')['close'].shift(1)) / \
                                  df.groupby('stock_code')['close'].shift(1).replace(0, 0.0001)
        df['is_high_open'] = df['open'] > df.groupby('stock_code')['close'].shift(1)
        log_msg("âœ… ç«ä»·æŒ‡æ ‡è®¡ç®—å®Œæˆ")
        
        # ï¼ˆ3ï¼‰å‡çº¿æŒ‡æ ‡ï¼ˆä¸æ—§ä»£ç ä¸€è‡´ï¼‰
        df['ma5'] = df.groupby('stock_code')['close'].transform(lambda x: ta.SMA(x, 5))
        df['ma20'] = df.groupby('stock_code')['close'].transform(lambda x: ta.SMA(x, 20))
        log_msg("âœ… å‡çº¿æŒ‡æ ‡è®¡ç®—å®Œæˆ")
        
        # ï¼ˆ4ï¼‰MACDæŒ‡æ ‡ï¼ˆä¸æ—§ä»£ç ä¸€è‡´ï¼‰
        def calc_macd(close_series):
            macd, signal, hist = ta.MACD(close_series, 12, 26, 9)
            return pd.DataFrame({
                'macd_line': macd,
                'signal_line': signal,
                'macd_hist': hist
            }, index=close_series.index)
        df = df.join(df.groupby('stock_code', group_keys=False)['close'].apply(calc_macd))
        log_msg("âœ… MACDæŒ‡æ ‡è®¡ç®—å®Œæˆ")
        
        # ï¼ˆ5ï¼‰é‡èƒ½æŒ‡æ ‡ï¼ˆä¸æ—§ä»£ç ä¸€è‡´ï¼‰
        df['volume_ratio_5d'] = df.groupby('stock_code')['volume'].transform(
            lambda x: x / x.rolling(5, min_periods=1).mean().shift(1).replace(0, 0.0001)
        )
        log_msg("âœ… é‡èƒ½æŒ‡æ ‡è®¡ç®—å®Œæˆ")
        
        # ï¼ˆ6ï¼‰å›è°ƒæŒ‡æ ‡ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šå‚è€ƒæ—§ä»£ç ï¼Œé¿å…æ•´æ•°è½¬æ¢é”™è¯¯ï¼‰
        def calc_30d_high(group):
            # 30æ—¥æœ€é«˜ä»·ï¼ˆç”¨closeè¿˜æ˜¯highï¼Ÿæ—§ä»£ç ç”¨closeï¼Œè¿™é‡Œä¿æŒä¸€è‡´ï¼‰
            group['high_30d'] = group['close'].rolling(30, min_periods=5).max()  # é™ä½min_periodsé¿å…è¿‡å¤šNaN
            
            # è®¡ç®—æœ€é«˜ä»·å¯¹åº”çš„ç´¢å¼•ï¼ˆä¸ç«‹å³è½¬intï¼Œé¿å…NaNé”™è¯¯ï¼‰
            def get_high_idx(window):
                return window.idxmax()  # å¯èƒ½è¿”å›NaNï¼Œä½†å…ˆä¿ç•™float
            group['high_idx_30d'] = group['close'].rolling(30, min_periods=5).apply(get_high_idx, raw=False)
            
            # åŒ¹é…æœ€é«˜ä»·æ—¥æœŸï¼ˆæ—§ä»£ç é€»è¾‘ï¼šç”¨ç´¢å¼•å®šä½ï¼‰
            # å¤„ç†NaNç´¢å¼•ï¼šå¡«å……ä¸ºå½“å‰è¡Œç´¢å¼•ï¼ˆé¿å…åç»­locæŠ¥é”™ï¼‰
            group['high_idx_30d'] = group['high_idx_30d'].fillna(group.index.to_series())
            # è½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•ï¼ˆæ­¤æ—¶å·²æ— NaNï¼Œå¯å®‰å…¨è½¬æ¢ï¼‰
            group['high_idx_30d'] = group['high_idx_30d'].astype(int)
            # ç”¨ç´¢å¼•åŒ¹é…æ—¥æœŸ
            group['high_date_30d'] = group.loc[group['high_idx_30d'], 'date'].values
            return group.drop(columns=['high_idx_30d'])
        
        # åº”ç”¨è®¡ç®—å¹¶åˆå¹¶ç»“æœï¼ˆå‚è€ƒæ—§ä»£ç çš„joinæ–¹å¼ï¼‰
        high_30d_df = df.groupby('stock_code', group_keys=False)[['close', 'date']].apply(calc_30d_high)
        df = df.join(high_30d_df[['high_30d', 'high_date_30d']], how='left')
        
        # è½¬æ¢æ—¥æœŸå¹¶è¿‡æ»¤æ— æ•ˆå€¼ï¼ˆå…³é”®ï¼šåˆ é™¤æ—¥æœŸä¸ºç©ºçš„è®°å½•ï¼‰
        df['high_date_30d'] = pd.to_datetime(df['high_date_30d'], errors='coerce')
        df = df.dropna(subset=['high_date_30d'])
        
        # è®¡ç®—å›æ’¤æ¯”ä¾‹å’Œå¤©æ•°ï¼ˆä¸æ—§ä»£ç ä¸€è‡´ï¼‰
        df['pullback_ratio'] = (df['high_30d'] - df['close']) / df['high_30d'].replace(0, 0.0001)
        df['pullback_days'] = (df['date'] - df['high_date_30d']).dt.days
        log_msg("âœ… å›è°ƒæŒ‡æ ‡è®¡ç®—å®Œæˆï¼ˆä¿®å¤æ•´æ•°è½¬æ¢é”™è¯¯ï¼‰")
        
        # ï¼ˆ7ï¼‰æ”¯æ’‘ä½ä¸RSIï¼ˆä¸æ—§ä»£ç ä¸€è‡´ï¼‰
        df['bollinger_lower'] = df.groupby('stock_code', group_keys=False)['close'].apply(
            lambda x: ta.BBANDS(x, 20, 2, 2)[2]
        )
        df['rsi14'] = df.groupby('stock_code')['close'].transform(lambda x: ta.RSI(x, 14))
        log_msg("âœ… æ”¯æ’‘ä½ä¸RSIè®¡ç®—å®Œæˆ")
        
        # ï¼ˆ8ï¼‰æŒ¯å¹…ï¼ˆä¸æ—§ä»£ç ä¸€è‡´ï¼Œç”¨closeåšåˆ†æ¯ï¼‰
        df['amplitude'] = (df['high'] - df['low']) / df['close'].replace(0, 0.0001)
        log_msg("âœ… æŒ¯å¹…è®¡ç®—å®Œæˆ")
        
        # 3. ç­›é€‰å­—æ®µå¹¶ä¿å­˜ï¼ˆä»…ä¿ç•™å®éªŒæ‰€éœ€ï¼‰
        log_msg("ç­›é€‰å­—æ®µå¹¶ä¿å­˜...")
        keep_cols = [
            # åŸºç¡€å­—æ®µ
            'stock_code', 'date', 'close', 'open', 'volume', 'high', 'low', 'amplitude',
            # è¶‹åŠ¿ä¸ç«ä»·
            'consecutive_up_days', 'auction_rise_ratio', 'auction_volume_ratio', 'is_high_open',
            # å‡çº¿ä¸MACD
            'ma5', 'ma20', 'macd_line', 'signal_line', 'macd_hist',
            # é‡èƒ½ä¸å›è°ƒ
            'volume_ratio_5d', 'high_30d', 'high_date_30d', 'pullback_ratio', 'pullback_days',
            # æ”¯æ’‘ä¸è¶…ä¹°è¶…å–
            'bollinger_lower', 'rsi14'
        ]
        df_final = df[keep_cols].copy()
        # æœ€ç»ˆè¿‡æ»¤å…³é”®æŒ‡æ ‡ï¼ˆç¡®ä¿é€‰è‚¡å¯ç”¨ï¼‰
        df_final = df_final.dropna(subset=['close', 'ma5', 'ma20', 'high_30d', 'rsi14'])
        log_msg(f"âœ… æœ€ç»ˆFactortableï¼š{len(df_final)}æ¡è®°å½•ï¼Œ{df_final['stock_code'].nunique()}åªè‚¡ç¥¨")
        
        # ä¿å­˜åˆ°æœ¬åœ°
        df_final.to_parquet(CONFIG["factortable_output_path"], index=False)
        df_final.sample(5, random_state=42).to_csv(CONFIG["sample_output_path"], index=False, encoding='utf-8-sig')
        log_msg(f"âœ… ä¿å­˜å®Œæˆï¼š")
        log_msg(f"ğŸ“ ä¸»æ–‡ä»¶ï¼š{CONFIG['factortable_output_path']}")
        log_msg(f"ğŸ“ æ ·æœ¬æ–‡ä»¶ï¼š{CONFIG['sample_output_path']}")

    except Exception as e:
        log_msg(f"âŒ è®¡ç®—å¤±è´¥ï¼š{str(e)}")
        raise

# --------------------------
# æ‰§è¡Œå…¥å£
# --------------------------
if __name__ == "__main__":
    calculate_factortable()

