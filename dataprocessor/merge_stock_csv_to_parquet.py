#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°† /d:/workspace/xiaoyao/redis/ ç›®å½•ä¸‹çš„æ‰€æœ‰ stock_***.csv æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ª parquet æ–‡ä»¶
ç¡®ä¿ä¸ç°æœ‰ /d:/workspace/xiaoyao/data/stock_daily_price.parquet ä¿æŒå­—æ®µã€å‹ç¼©æ–¹å¼ä¸€è‡´
"""

import os
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path
import glob


def merge_stock_csv_to_parquet(csv_dir, output_parquet_file):
    """
    åˆå¹¶æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ stock_***.csv æ–‡ä»¶åˆ°å•ä¸ª parquet æ–‡ä»¶
    
    Args:
        csv_dir: CSV æ–‡ä»¶æ‰€åœ¨ç›®å½•
        output_parquet_file: è¾“å‡ºçš„ parquet æ–‡ä»¶è·¯å¾„
    """
    print(f"ğŸ“ å¼€å§‹å¤„ç†ç›®å½•: {csv_dir}")
    
    # è·å–æ‰€æœ‰ stock_***.csv æ–‡ä»¶
    csv_pattern = os.path.join(csv_dir, "stock_*.csv")
    csv_files = glob.glob(csv_pattern)
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ° stock_***.csv æ–‡ä»¶")
        return False
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶")
    
    # æŒ‰æ–‡ä»¶åæ’åºï¼ˆç¡®ä¿æŒ‰æ—¥æœŸé¡ºåºå¤„ç†ï¼‰
    csv_files.sort()
    
    # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰ CSV æ–‡ä»¶
    all_dataframes = []
    total_records = 0
    
    for i, csv_file in enumerate(csv_files, 1):
        filename = os.path.basename(csv_file)
        print(f"æ­£åœ¨å¤„ç† ({i}/{len(csv_files)}): {filename}")
        
        try:
            # è¯»å– CSV æ–‡ä»¶
            df = pd.read_csv(csv_file)
            
            # æ•°æ®éªŒè¯å’Œæ¸…æ´—
            # ç¡®ä¿ date åˆ—æ˜¯ datetime ç±»å‹
            df['date'] = pd.to_datetime(df['date'])
            
            # ç¡®ä¿æ•°å€¼åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®
            numeric_columns = ['open', 'close', 'low', 'high', 'volume', 'money', 
                             'factor', 'high_limit', 'low_limit', 'avg', 'pre_close', 'paused']
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # åˆ é™¤æ— æ•ˆæ•°æ®
            df = df.dropna(subset=['date', 'stock_code'])
            
            all_dataframes.append(df)
            total_records += len(df)
            print(f"  âœ… æˆåŠŸè¯»å– {len(df)} æ¡è®°å½•")
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            continue
    
    if not all_dataframes:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ•°æ®")
        return False
    
    print(f"\nğŸ“Š åˆå¹¶æ‰€æœ‰æ•°æ®...")
    # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†
    combined_df = pd.concat(all_dataframes, ignore_index=True)
    
    # å»é‡ï¼ˆæŒ‰ date + stock_codeï¼‰
    combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code'])
    
    # æŒ‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç æ’åº
    combined_df = combined_df.sort_values(['date', 'stock_code']).reset_index(drop=True)
    
    print(f"ğŸ“ˆ æ€»è®¡ {len(combined_df)} æ¡è®°å½•ï¼ˆå»é‡åï¼‰")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_parquet_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # è½¬æ¢ä¸º pyarrow Table
    table = pa.Table.from_pandas(combined_df)
    
    # ä½¿ç”¨ä¸ç›®æ ‡æ–‡ä»¶ç›¸åŒçš„å‹ç¼©æ–¹å¼ (snappy) å’Œæ ¼å¼å†™å…¥ parquet
    try:
        pq.write_table(
            table, 
            output_parquet_file,
            compression='snappy',
            version='2.6',  # ä½¿ç”¨è¾ƒæ–°çš„ parquet ç‰ˆæœ¬
            use_dictionary=True,
            write_batch_size=64 * 1024 * 1024  # 64MB batch size for better performance
        )
        
        print(f"âœ… æˆåŠŸä¿å­˜åˆ°: {output_parquet_file}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(output_parquet_file) / (1024*1024):.2f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ parquet æ–‡ä»¶å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è·¯å¾„
    csv_directory = "d:/workspace/xiaoyao/redis/"
    output_file = "d:/workspace/xiaoyao/dataprocessor/merged_stock_data.parquet"
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹åˆå¹¶ stock_***.csv æ–‡ä»¶åˆ° parquet")
    print("=" * 60)
    
    # æ‰§è¡Œåˆå¹¶
    success = merge_stock_csv_to_parquet(csv_directory, output_file)
    
    if success:
        print("\nğŸ‰ åˆå¹¶å®Œæˆï¼")
        
        # éªŒè¯ç»“æœ
        try:
            print("\nğŸ“‹ éªŒè¯ç»“æœ:")
            result_df = pd.read_parquet(output_file)
            print(f"   æ€»è¡Œæ•°: {len(result_df)}")
            print(f"   æ—¥æœŸèŒƒå›´: {result_df['date'].min()} åˆ° {result_df['date'].max()}")
            print(f"   è‚¡ç¥¨æ•°é‡: {result_df['stock_code'].nunique()}")
            print(f"   åˆ—å: {list(result_df.columns)}")
            
        except Exception as e:
            print(f"âš ï¸  éªŒè¯å¤±è´¥: {e}")
    
    else:
        print("\nâŒ åˆå¹¶å¤±è´¥ï¼")
    
    print("=" * 60)


if __name__ == "__main__":
    main()