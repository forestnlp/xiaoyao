#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°†æ–°ç”Ÿæˆçš„ merged_stock_data.parquet ä¸ç°æœ‰çš„ stock_daily_price.parquet åˆå¹¶
"""

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import os


def merge_parquet_files(existing_file, new_file, output_file):
    """
    åˆå¹¶ä¸¤ä¸ª parquet æ–‡ä»¶
    
    Args:
        existing_file: ç°æœ‰çš„ parquet æ–‡ä»¶è·¯å¾„
        new_file: æ–°çš„ parquet æ–‡ä»¶è·¯å¾„  
        output_file: è¾“å‡ºçš„åˆå¹¶æ–‡ä»¶è·¯å¾„
    """
    print("ğŸ“Š å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶...")
    
    try:
        # è¯»å–ç°æœ‰æ•°æ®
        print(f"ğŸ“– è¯»å–ç°æœ‰æ–‡ä»¶: {existing_file}")
        existing_df = pd.read_parquet(existing_file)
        print(f"   ç°æœ‰æ•°æ®è¡Œæ•°: {len(existing_df)}")
        
        # è¯»å–æ–°æ•°æ®
        print(f"ğŸ“– è¯»å–æ–°æ–‡ä»¶: {new_file}")
        new_df = pd.read_parquet(new_file)
        print(f"   æ–°æ•°æ®è¡Œæ•°: {len(new_df)}")
        
        # åˆå¹¶æ•°æ®
        print("ğŸ”„ åˆå¹¶æ•°æ®ä¸­...")
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        
        # å»é‡ï¼ˆæŒ‰ date + stock_codeï¼‰
        print("ğŸ§¹ å»é‡å¤„ç†...")
        combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code'])
        
        # æ’åº
        print("ğŸ“… æŒ‰æ—¥æœŸæ’åº...")
        combined_df = combined_df.sort_values(['date', 'stock_code']).reset_index(drop=True)
        
        print(f"ğŸ“ˆ åˆå¹¶åæ€»è¡Œæ•°: {len(combined_df)}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # è½¬æ¢ä¸º pyarrow Table
        table = pa.Table.from_pandas(combined_df)
        
        # å†™å…¥ parquetï¼ˆä½¿ç”¨ä¸æºæ–‡ä»¶ç›¸åŒçš„æ ¼å¼ï¼‰
        print(f"ğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœ: {output_file}")
        pq.write_table(
            table,
            output_file,
            compression='snappy',
            version='2.6',
            use_dictionary=True,
            write_batch_size=64 * 1024 * 1024
        )
        
        print(f"âœ… åˆå¹¶å®Œæˆï¼æ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / (1024*1024):.2f} MB")
        
        # éªŒè¯ç»“æœ
        print("\nğŸ“‹ éªŒè¯ç»“æœ:")
        result_df = pd.read_parquet(output_file)
        print(f"   æœ€ç»ˆè¡Œæ•°: {len(result_df)}")
        print(f"   æ—¥æœŸèŒƒå›´: {result_df['date'].min()} åˆ° {result_df['date'].max()}")
        print(f"   è‚¡ç¥¨æ•°é‡: {result_df['stock_code'].nunique()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ–‡ä»¶è·¯å¾„
    # existing_file = "d:/workspace/xiaoyao/data/stock_daily_price.parquet"
    # new_file = "d:/workspace/xiaoyao/dataprocessor/merged_stock_data.parquet"
    # output_file = "d:/workspace/xiaoyao/dataprocessor/merged_combined_stock_data.parquet"

    existing_file = "d:/workspace/xiaoyao/data/stock_daily_auction.parquet"
    new_file = "d:/workspace/xiaoyao/dataprocessor/merged_stock_data.parquet"
    output_file = "d:/workspace/xiaoyao/dataprocessor/merged_combined_stock_data.parquet"

    print("=" * 60)
    print("ğŸš€ å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶")
    print("=" * 60)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(existing_file):
        print(f"âŒ ç°æœ‰æ–‡ä»¶ä¸å­˜åœ¨: {existing_file}")
        return
    
    if not os.path.exists(new_file):
        print(f"âŒ æ–°æ–‡ä»¶ä¸å­˜åœ¨: {new_file}")
        return
    
    # æ‰§è¡Œåˆå¹¶
    success = merge_parquet_files(existing_file, new_file, output_file)
    
    if success:
        print("\nğŸ‰ åˆå¹¶æˆåŠŸï¼")
    else:
        print("\nâŒ åˆå¹¶å¤±è´¥ï¼")
    
    print("=" * 60)


if __name__ == "__main__":
    main()