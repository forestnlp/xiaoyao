{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c12029",
   "metadata": {},
   "source": [
    "## 1.ä¸‹è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb58c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å‘é€ç«¯ï¼šRedisè¿æ¥æˆåŠŸ\n",
      "âœ… å‘é€ç«¯pandasç‰ˆæœ¬ï¼š2.3.2\n",
      "=== å…±éœ€è·å– 3 å¤©çš„æ•°æ® ===\n",
      "\n",
      "=== æ­£åœ¨å¤„ç† 1/3ï¼š20250926 ===\n",
      "ğŸ“¤ å·²è°ƒç”¨è¿œç¨‹å‡½æ•°ï¼šfetch_daily_stock_dataï¼ˆä»»åŠ¡IDï¼štask_89af16e4ï¼‰\n",
      "âœ… ä¿å­˜æˆåŠŸï¼šstock_daily_price_20250926.csvï¼ˆ5158æ¡è®°å½•ï¼‰\n",
      "\n",
      "=== æ­£åœ¨å¤„ç† 2/3ï¼š20250927 ===\n",
      "ğŸ“¤ å·²è°ƒç”¨è¿œç¨‹å‡½æ•°ï¼šfetch_daily_stock_dataï¼ˆä»»åŠ¡IDï¼štask_7519219bï¼‰\n",
      "âš ï¸ æ•°æ®ä¸ºç©ºï¼Œä¸ä¿å­˜\n",
      "\n",
      "=== æ­£åœ¨å¤„ç† 3/3ï¼š20250928 ===\n",
      "ğŸ“¤ å·²è°ƒç”¨è¿œç¨‹å‡½æ•°ï¼šfetch_daily_stock_dataï¼ˆä»»åŠ¡IDï¼štask_9d76399cï¼‰\n",
      "âš ï¸ æ•°æ®ä¸ºç©ºï¼Œä¸ä¿å­˜\n",
      "\n",
      "=== æ‰€æœ‰æ—¥æœŸå¤„ç†å®Œæˆ ===\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import pickle\n",
    "import time\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from typing import Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RemoteSender:\n",
    "    def __init__(self, host='*', port=6379, password='*'):\n",
    "        self.redis = redis.Redis(\n",
    "            host=host, port=port, password=password,\n",
    "            decode_responses=False\n",
    "        )\n",
    "        self.task_queue = 'function_calls'\n",
    "        self.result_queue = 'function_results'\n",
    "        self._test_connection()\n",
    "        print(f\"âœ… å‘é€ç«¯pandasç‰ˆæœ¬ï¼š{pd.__version__}\")\n",
    "\n",
    "    def _test_connection(self):\n",
    "        try:\n",
    "            self.redis.ping()\n",
    "            print(\"âœ… å‘é€ç«¯ï¼šRedisè¿æ¥æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å‘é€ç«¯ï¼šè¿æ¥å¤±è´¥ - {e}\")\n",
    "            raise\n",
    "\n",
    "    def call_remote_function(self, func_name: str, *args, **kwargs) -> Any:\n",
    "        task_id = f\"task_{uuid.uuid4().hex[:8]}\"\n",
    "        task = {\n",
    "            'func_name': func_name,\n",
    "            'args': args,\n",
    "            'kwargs': kwargs,\n",
    "            'task_id': task_id\n",
    "        }\n",
    "        self.redis.rpush(self.task_queue, pickle.dumps(task))\n",
    "        print(f\"ğŸ“¤ å·²è°ƒç”¨è¿œç¨‹å‡½æ•°ï¼š{func_name}ï¼ˆä»»åŠ¡IDï¼š{task_id}ï¼‰\")\n",
    "        return self._get_result(task_id)\n",
    "\n",
    "    def _get_result(self, task_id: str, timeout=300) -> Any:\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < timeout:\n",
    "            result_data = self.redis.blpop(self.result_queue, timeout=10)\n",
    "            if not result_data:\n",
    "                continue\n",
    "\n",
    "            _, res_bytes = result_data\n",
    "            result = pickle.loads(res_bytes)\n",
    "            if result['task_id'] == task_id:\n",
    "                if result['status'] == 'success':\n",
    "                    return result['result']  # è¿”å›CSVå­—ç¬¦ä¸²\n",
    "                else:\n",
    "                    raise Exception(f\"è¿œç¨‹æ‰§è¡Œå¤±è´¥ï¼š{result['error']}\")\n",
    "            self.redis.rpush(self.result_queue, res_bytes)\n",
    "        raise TimeoutError(\"ä»»åŠ¡è¶…æ—¶\")\n",
    "\n",
    "    def save_to_csv(self, csv_str: Optional[str], filename: str) -> bool:\n",
    "        \"\"\"å°†CSVå­—ç¬¦ä¸²ä¿å­˜ä¸ºæœ¬åœ°CSVæ–‡ä»¶ï¼ˆæ›¿ä»£Parquetï¼‰\"\"\"\n",
    "        if not csv_str:\n",
    "            print(\"âš ï¸ æ•°æ®ä¸ºç©ºï¼Œä¸ä¿å­˜\")\n",
    "            return False\n",
    "        try:\n",
    "            # ä»CSVå­—ç¬¦ä¸²æ¢å¤DataFrameï¼ˆå…¼å®¹æ‰€æœ‰pandasç‰ˆæœ¬ï¼‰\n",
    "            df = pd.read_csv(StringIO(csv_str))\n",
    "            # ä¿å­˜ä¸ºCSVæ–‡ä»¶\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"âœ… ä¿å­˜æˆåŠŸï¼š{filename}ï¼ˆ{len(df)}æ¡è®°å½•ï¼‰\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä¿å­˜å¤±è´¥ï¼š{e}\")\n",
    "            return False\n",
    "\n",
    "def generate_date_range(start_date_str: str, end_date_str: str) -> list:\n",
    "    \"\"\"ç”Ÿæˆä»å¼€å§‹æ—¥æœŸåˆ°ç»“æŸæ—¥æœŸçš„æ‰€æœ‰æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆYYYYMMDDæ ¼å¼ï¼‰\"\"\"\n",
    "    dates = []\n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "        end_date = datetime.strptime(end_date_str, '%Y%m%d')\n",
    "        \n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"å¼€å§‹æ—¥æœŸæ™šäºç»“æŸæ—¥æœŸ\")\n",
    "            \n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            dates.append(current_date.strftime('%Y%m%d'))\n",
    "            current_date += timedelta(days=1)\n",
    "    except Exception as e:\n",
    "        print(f\"æ—¥æœŸå¤„ç†é”™è¯¯ï¼š{e}\")\n",
    "    return dates\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ä»é…ç½®æ–‡ä»¶è¯»å–Redisè¿æ¥ä¿¡æ¯\n",
    "    with open('redis.conf', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('host='):\n",
    "                host = line.split('=')[1].strip()\n",
    "            elif line.startswith('port='):\n",
    "                port = int(line.split('=')[1].strip())\n",
    "            elif line.startswith('password='):\n",
    "                password = line.split('=')[1].strip()\n",
    "    # åˆå§‹åŒ–Rediså‘é€ç«¯\n",
    "    sender = RemoteSender(host=host, port=port, password=password)\n",
    "    \n",
    "    # å®šä¹‰æ—¥æœŸèŒƒå›´ï¼šä»20250516åˆ°20250923\n",
    "    # è¯»å–../data/stock_daily_price.parquetæ–‡ä»¶ï¼Œè·å–æœ€å¤§çš„æ—¥æœŸ+1ï¼Œæ˜¯start_date\n",
    "    df = pd.read_parquet('../data/stock_daily_price.parquet')\n",
    "    start_date = (df['date'].max() + timedelta(days=1)).strftime('%Y%m%d')\n",
    "    # è·å–å½“æ—¥æ—¥æœŸ-1ï¼Œæ˜¯end_date\n",
    "    end_date = (datetime.today() - timedelta(days=1)).strftime('%Y%m%d')\n",
    "    \n",
    "    # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨\n",
    "    date_list = generate_date_range(start_date, end_date)\n",
    "    print(f\"=== å…±éœ€è·å– {len(date_list)} å¤©çš„æ•°æ® ===\")\n",
    "    \n",
    "    # å¾ªç¯è°ƒç”¨è·å–æ¯æ—¥æ•°æ®\n",
    "    for i, date in enumerate(date_list, 1):\n",
    "        print(f\"\\n=== æ­£åœ¨å¤„ç† {i}/{len(date_list)}ï¼š{date} ===\")\n",
    "        try:\n",
    "            # è°ƒç”¨è¿œç¨‹å‡½æ•°è·å–å½“æ—¥æ•°æ®\n",
    "            csv_data = sender.call_remote_function('fetch_daily_stock_data', date)\n",
    "            # ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼Œæ–‡ä»¶ååŒ…å«æ—¥æœŸ\n",
    "            sender.save_to_csv(csv_data, f'stock_daily_price_{date}.csv')\n",
    "            \n",
    "            # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {date} å¤„ç†å¤±è´¥ï¼š{e}\")\n",
    "            # å¤±è´¥åä¹Ÿå»¶è¿Ÿä¸€ä¸‹ï¼Œé¿å…å¿«é€Ÿé‡è¯•å¯¼è‡´çš„é—®é¢˜\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(\"\\n=== æ‰€æœ‰æ—¥æœŸå¤„ç†å®Œæˆ ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47950e5",
   "metadata": {},
   "source": [
    "## å°†csvåˆå¹¶ä¸ºä¸€ä¸ªparquetæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68654520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ å¼€å§‹åˆå¹¶ stock_***.csv æ–‡ä»¶åˆ° parquet\n",
      "============================================================\n",
      "ğŸ“ å¼€å§‹å¤„ç†ç›®å½•: d:/workspace/xiaoyao/redis/\n",
      "ğŸ“Š æ‰¾åˆ° 1 ä¸ª CSV æ–‡ä»¶\n",
      "æ­£åœ¨å¤„ç† (1/1): stock_daily_price_20250926.csv\n",
      "  âœ… æˆåŠŸè¯»å– 5158 æ¡è®°å½•\n",
      "\n",
      "ğŸ“Š åˆå¹¶æ‰€æœ‰æ•°æ®...\n",
      "ğŸ“ˆ æ€»è®¡ 5158 æ¡è®°å½•ï¼ˆå»é‡åï¼‰\n",
      "âœ… æˆåŠŸä¿å­˜åˆ°: d:/workspace/xiaoyao/redis/parquet\\stock_daily_price_to_merged.parquet\n",
      "ğŸ“Š æ–‡ä»¶å¤§å°: 0.38 MB\n",
      "\n",
      "ğŸ‰ åˆå¹¶å®Œæˆï¼\n",
      "\n",
      "ğŸ“‹ éªŒè¯ç»“æœ:\n",
      "   æ€»è¡Œæ•°: 5158\n",
      "   æ—¥æœŸèŒƒå›´: 2025-09-26 00:00:00 åˆ° 2025-09-26 00:00:00\n",
      "   è‚¡ç¥¨æ•°é‡: 5158\n",
      "   åˆ—å: ['date', 'stock_code', 'open', 'close', 'low', 'high', 'volume', 'money', 'factor', 'high_limit', 'low_limit', 'avg', 'pre_close', 'paused']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "å°† /d:/workspace/xiaoyao/redis/ ç›®å½•ä¸‹çš„æ‰€æœ‰ stock_***.csv æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ª parquet æ–‡ä»¶\n",
    "ç¡®ä¿ä¸ç°æœ‰ /d:/workspace/xiaoyao/data/stock_daily_price.parquet ä¿æŒå­—æ®µã€å‹ç¼©æ–¹å¼ä¸€è‡´\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "\n",
    "def merge_stock_csv_to_parquet(csv_dir, output_parquet_file):\n",
    "    \"\"\"\n",
    "    åˆå¹¶æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ stock_***.csv æ–‡ä»¶åˆ°å•ä¸ª parquet æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        csv_dir: CSV æ–‡ä»¶æ‰€åœ¨ç›®å½•\n",
    "        output_parquet_file: è¾“å‡ºçš„ parquet æ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“ å¼€å§‹å¤„ç†ç›®å½•: {csv_dir}\")\n",
    "    \n",
    "    # è·å–æ‰€æœ‰ stock_***.csv æ–‡ä»¶\n",
    "    csv_pattern = os.path.join(csv_dir, \"stock_daily_price_*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"âŒ æœªæ‰¾åˆ° stock_***.csv æ–‡ä»¶\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"ğŸ“Š æ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶\")\n",
    "    \n",
    "    # æŒ‰æ–‡ä»¶åæ’åºï¼ˆç¡®ä¿æŒ‰æ—¥æœŸé¡ºåºå¤„ç†ï¼‰\n",
    "    csv_files.sort()\n",
    "    \n",
    "    # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰ CSV æ–‡ä»¶\n",
    "    all_dataframes = []\n",
    "    total_records = 0\n",
    "    \n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        filename = os.path.basename(csv_file)\n",
    "        print(f\"æ­£åœ¨å¤„ç† ({i}/{len(csv_files)}): {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # è¯»å– CSV æ–‡ä»¶\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # æ•°æ®éªŒè¯å’Œæ¸…æ´—\n",
    "            # ç¡®ä¿ date åˆ—æ˜¯ datetime ç±»å‹\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            # ç¡®ä¿æ•°å€¼åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®\n",
    "            numeric_columns = ['open', 'close', 'low', 'high', 'volume', 'money', \n",
    "                             'factor', 'high_limit', 'low_limit', 'avg', 'pre_close', 'paused']\n",
    "            \n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # åˆ é™¤æ— æ•ˆæ•°æ®\n",
    "            df = df.dropna(subset=['date', 'stock_code'])\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            total_records += len(df)\n",
    "            print(f\"  âœ… æˆåŠŸè¯»å– {len(df)} æ¡è®°å½•\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ å¤„ç†å¤±è´¥: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ•°æ®\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\nğŸ“Š åˆå¹¶æ‰€æœ‰æ•°æ®...\")\n",
    "    # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # å»é‡ï¼ˆæŒ‰ date + stock_codeï¼‰\n",
    "    combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code'])\n",
    "    \n",
    "    # æŒ‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç æ’åº\n",
    "    combined_df = combined_df.sort_values(['date', 'stock_code']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ æ€»è®¡ {len(combined_df)} æ¡è®°å½•ï¼ˆå»é‡åï¼‰\")\n",
    "    \n",
    "    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "    output_dir = os.path.dirname(output_parquet_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}\")\n",
    "    \n",
    "    # è½¬æ¢ä¸º pyarrow Table\n",
    "    table = pa.Table.from_pandas(combined_df)\n",
    "    \n",
    "    # ä½¿ç”¨ä¸ç›®æ ‡æ–‡ä»¶ç›¸åŒçš„å‹ç¼©æ–¹å¼ (snappy) å’Œæ ¼å¼å†™å…¥ parquet\n",
    "    try:\n",
    "        pq.write_table(\n",
    "            table, \n",
    "            output_parquet_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',  # ä½¿ç”¨è¾ƒæ–°çš„ parquet ç‰ˆæœ¬\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024  # 64MB batch size for better performance\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… æˆåŠŸä¿å­˜åˆ°: {output_parquet_file}\")\n",
    "        print(f\"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(output_parquet_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜ parquet æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    # è®¾ç½®è·¯å¾„\n",
    "    csv_directory = \"d:/workspace/xiaoyao/redis/\"\n",
    "    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "    output_dir = os.path.join(csv_directory, 'parquet')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}\")\n",
    "\n",
    "    output_file = os.path.join(output_dir, 'stock_daily_price_to_merged.parquet')\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸš€ å¼€å§‹åˆå¹¶ stock_***.csv æ–‡ä»¶åˆ° parquet\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æ‰§è¡Œåˆå¹¶\n",
    "    success = merge_stock_csv_to_parquet(csv_directory, output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nğŸ‰ åˆå¹¶å®Œæˆï¼\")\n",
    "        \n",
    "        # éªŒè¯ç»“æœ\n",
    "        try:\n",
    "            print(\"\\nğŸ“‹ éªŒè¯ç»“æœ:\")\n",
    "            result_df = pd.read_parquet(output_file)\n",
    "            print(f\"   æ€»è¡Œæ•°: {len(result_df)}\")\n",
    "            print(f\"   æ—¥æœŸèŒƒå›´: {result_df['date'].min()} åˆ° {result_df['date'].max()}\")\n",
    "            print(f\"   è‚¡ç¥¨æ•°é‡: {result_df['stock_code'].nunique()}\")\n",
    "            print(f\"   åˆ—å: {list(result_df.columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  éªŒè¯å¤±è´¥: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nâŒ åˆå¹¶å¤±è´¥ï¼\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ff005",
   "metadata": {},
   "source": [
    "## å°†æ–°ç”Ÿæˆçš„ merged_stock_data.parquet ä¸ç°æœ‰çš„ stock_daily_price.parquet åˆå¹¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0e4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶\n",
      "============================================================\n",
      "ğŸ“Š å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶...\n",
      "ğŸ“– è¯»å–ç°æœ‰æ–‡ä»¶: d:/workspace/xiaoyao/data/stock_daily_price.parquet\n",
      "   ç°æœ‰æ•°æ®è¡Œæ•°: 15039052\n",
      "ğŸ“– è¯»å–æ–°æ–‡ä»¶: d:/workspace/xiaoyao/redis/parquet/stock_daily_price_to_merged.parquet\n",
      "   æ–°æ•°æ®è¡Œæ•°: 5158\n",
      "ğŸ”„ åˆå¹¶æ•°æ®ä¸­...\n",
      "ğŸ§¹ å»é‡å¤„ç†...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 117\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 106\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# æ‰§è¡Œåˆå¹¶\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_parquet_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexisting_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ‰ åˆå¹¶æˆåŠŸï¼\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m, in \u001b[0;36mmerge_parquet_files\u001b[1;34m(existing_file, new_file, output_file)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# å»é‡ï¼ˆæŒ‰ date + stock_codeï¼‰\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ§¹ å»é‡å¤„ç†...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstock_code\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# æ’åº\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“… æŒ‰æ—¥æœŸæ’åº...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\core\\frame.py:6825\u001b[0m, in \u001b[0;36mDataFrame.drop_duplicates\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6822\u001b[0m inplace \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(inplace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6823\u001b[0m ignore_index \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(ignore_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 6825\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduplicated\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   6826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[0;32m   6827\u001b[0m     result\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(result))\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\core\\frame.py:4098\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4096\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   4097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 4098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   4101\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   4102\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\core\\frame.py:4157\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4154\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[0;32m   4156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m-> 4157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   4159\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   4160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_with_is_copy(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\core\\generic.py:6830\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6681\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   6683\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6684\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6685\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6828\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[0;32m   6829\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6830\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(data, axes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   6833\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6834\u001b[0m     )\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\core\\internals\\managers.py:593\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m--> 593\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcopy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:822\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    820\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 822\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "å°†æ–°ç”Ÿæˆçš„ merged_stock_data.parquet ä¸ç°æœ‰çš„ stock_daily_price.parquet åˆå¹¶\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_parquet_files(existing_file, new_file, output_file):\n",
    "    \"\"\"\n",
    "    åˆå¹¶ä¸¤ä¸ª parquet æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        existing_file: ç°æœ‰çš„ parquet æ–‡ä»¶è·¯å¾„\n",
    "        new_file: æ–°çš„ parquet æ–‡ä»¶è·¯å¾„  \n",
    "        output_file: è¾“å‡ºçš„åˆå¹¶æ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶...\")\n",
    "    \n",
    "    try:\n",
    "        # è¯»å–ç°æœ‰æ•°æ®\n",
    "        print(f\"ğŸ“– è¯»å–ç°æœ‰æ–‡ä»¶: {existing_file}\")\n",
    "        existing_df = pd.read_parquet(existing_file)\n",
    "        print(f\"   ç°æœ‰æ•°æ®è¡Œæ•°: {len(existing_df)}\")\n",
    "        \n",
    "        # è¯»å–æ–°æ•°æ®\n",
    "        print(f\"ğŸ“– è¯»å–æ–°æ–‡ä»¶: {new_file}\")\n",
    "        new_df = pd.read_parquet(new_file)\n",
    "        print(f\"   æ–°æ•°æ®è¡Œæ•°: {len(new_df)}\")\n",
    "        \n",
    "        # åˆå¹¶æ•°æ®\n",
    "        print(\"ğŸ”„ åˆå¹¶æ•°æ®ä¸­...\")\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        \n",
    "        # å»é‡ï¼ˆæŒ‰ date + stock_codeï¼‰\n",
    "        print(\"ğŸ§¹ å»é‡å¤„ç†...\")\n",
    "        combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code'])\n",
    "        \n",
    "        # æ’åº\n",
    "        print(\"ğŸ“… æŒ‰æ—¥æœŸæ’åº...\")\n",
    "        combined_df = combined_df.sort_values(['date', 'stock_code']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"ğŸ“ˆ åˆå¹¶åæ€»è¡Œæ•°: {len(combined_df)}\")\n",
    "        \n",
    "        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # è½¬æ¢ä¸º pyarrow Table\n",
    "        table = pa.Table.from_pandas(combined_df)\n",
    "        \n",
    "        # å†™å…¥ parquetï¼ˆä½¿ç”¨ä¸æºæ–‡ä»¶ç›¸åŒçš„æ ¼å¼ï¼‰\n",
    "        print(f\"ğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœ: {output_file}\")\n",
    "        pq.write_table(\n",
    "            table,\n",
    "            output_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… åˆå¹¶å®Œæˆï¼æ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # éªŒè¯ç»“æœ\n",
    "        print(\"\\nğŸ“‹ éªŒè¯ç»“æœ:\")\n",
    "        result_df = pd.read_parquet(output_file)\n",
    "        print(f\"   æœ€ç»ˆè¡Œæ•°: {len(result_df)}\")\n",
    "        print(f\"   æ—¥æœŸèŒƒå›´: {result_df['date'].min()} åˆ° {result_df['date'].max()}\")\n",
    "        print(f\"   è‚¡ç¥¨æ•°é‡: {result_df['stock_code'].nunique()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åˆå¹¶å¤±è´¥: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    # è®¾ç½®æ–‡ä»¶è·¯å¾„\n",
    "\n",
    "    existing_file = \"d:/workspace/xiaoyao/data/stock_daily_price.parquet\"\n",
    "    new_file = \"d:/workspace/xiaoyao/redis/parquet/stock_daily_price_to_merged.parquet\"\n",
    "    output_file = \"d:/workspace/xiaoyao/redis/parquet/stock_daily_price.parquet\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸš€ å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(existing_file):\n",
    "        print(f\"âŒ ç°æœ‰æ–‡ä»¶ä¸å­˜åœ¨: {existing_file}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(new_file):\n",
    "        print(f\"âŒ æ–°æ–‡ä»¶ä¸å­˜åœ¨: {new_file}\")\n",
    "        return\n",
    "    \n",
    "    # æ‰§è¡Œåˆå¹¶\n",
    "    success = merge_parquet_files(existing_file, new_file, output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nğŸ‰ åˆå¹¶æˆåŠŸï¼\")\n",
    "    else:\n",
    "        print(\"\\nâŒ åˆå¹¶å¤±è´¥ï¼\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8770ce2",
   "metadata": {},
   "source": [
    "## åˆ é™¤å·²ä½¿ç”¨çš„csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c223ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²åˆ é™¤ï¼šD:\\workspace\\xiaoyao\\redis\\stock_daily_price_20250925.csv\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "åˆ é™¤æŒ‡å®šç›®å½•ä¸‹çš„ stock_***.csv æ–‡ä»¶\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def delete_stock_csv_files(target_directory, pattern=\"stock_*.csv\"):\n",
    "    #åˆ é™¤æ»¡è¶³æ¨¡å¼çš„æ‰€æœ‰æ–‡ä»¶\n",
    "    files = glob.glob(os.path.join(target_directory, pattern))\n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"å·²åˆ é™¤ï¼š{file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"åˆ é™¤ {file} å¤±è´¥ï¼š{e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    delete_stock_csv_files(r'D:\\workspace\\xiaoyao\\redis','stock_daily_price_*.csv')\n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46e4da",
   "metadata": {},
   "source": [
    "## å°†æ–°çš„parquetæ–‡ä»¶ç§»åŠ¨åˆ°dataç›®å½•è¦†ç›–åŸæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved: stock_daily_price.parquet to D:\\workspace\\xiaoyao\\data\n",
      "Deleted: stock_daily_price.parquet from D:\\workspace\\xiaoyao\\redis\\parquet\n"
     ]
    }
   ],
   "source": [
    "# å°†å­ç›®å½•ä¸‹çš„æŸä¸ªparquetæ–‡ä»¶ç§»åŠ¨åˆ°æŒ‡å®šç›®å½•\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# å®šä¹‰æºç›®å½•å’Œç›®æ ‡ç›®å½•\n",
    "source_dir = \"D:\\\\workspace\\\\xiaoyao\\\\redis\\\\parquet\"\n",
    "target_dir = \"D:\\\\workspace\\\\xiaoyao\\\\data\"\n",
    "\n",
    "# ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# å®šä¹‰è¦ç§»åŠ¨çš„æ–‡ä»¶\n",
    "file_to_move = \"stock_daily_price.parquet\"\n",
    "\n",
    "# æ„å»ºæºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„\n",
    "source_file_path = os.path.join(source_dir, file_to_move)\n",
    "\n",
    "# æ„å»ºç›®æ ‡æ–‡ä»¶çš„å®Œæ•´è·¯å¾„\n",
    "target_file_path = os.path.join(target_dir, file_to_move)\n",
    "\n",
    "# æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "if os.path.exists(source_file_path):\n",
    "    # ç§»åŠ¨æ–‡ä»¶\n",
    "    shutil.move(source_file_path, target_file_path)\n",
    "    print(f\"Moved: {file_to_move} to {target_dir}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_move}\")\n",
    "\n",
    "# åˆ é™¤æŒ‡å®šçš„parquetæ–‡ä»¶\n",
    "file_to_delete = os.path.join(source_dir, 'stock_daily_price_to_merged.parquet')\n",
    "if os.path.exists(file_to_delete):\n",
    "    os.remove(file_to_delete)\n",
    "    print(f\"Deleted: {file_to_move} from {source_dir}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_move} in {source_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d277a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
