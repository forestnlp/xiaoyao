{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf3a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²åŠ è½½Redisé…ç½®ï¼šhost=220.203.1.124, port=6379\n",
      "å¼€å§‹åŠ è½½æ—¥Kçº¿æ•°æ®ï¼šD:\\workspace\\xiaoyao\\data\\stock_daily_price.parquet\n",
      "æ—¥Kçº¿æ•°æ®åŠ è½½å®Œæˆï¼Œæœ‰æ•ˆä»»åŠ¡æ•°ï¼š949049\n",
      "âœ… Redisè¿æ¥æˆåŠŸ\n",
      "å¼€å§‹å‘é€ä»»åŠ¡ï¼Œæ€»æœ‰æ•ˆä»»åŠ¡æ•°ï¼š949049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23128\\1945496700.py:123: DeprecationWarning: Call to deprecated hmset. (Use 'hset' instead.) -- Deprecated since version 4.0.0.\n",
      "  self.redis.hmset(self.redis_metadata_key, metadata_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ å·²å‘é€ 10000/949049 (1.05%)ï¼Œé˜Ÿåˆ—å½“å‰é•¿åº¦ï¼š7186\n",
      "ğŸ“¤ å·²å‘é€ 20000/949049 (2.11%)ï¼Œé˜Ÿåˆ—å½“å‰é•¿åº¦ï¼š14160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 228\u001b[0m\n\u001b[0;32m    225\u001b[0m     valid_tasks \u001b[38;5;241m=\u001b[39m load_valid_daily_data(DAILY_K_PATH)\n\u001b[0;32m    227\u001b[0m     sender \u001b[38;5;241m=\u001b[39m OptimizedTaskSender(redis_config)\n\u001b[1;32m--> 228\u001b[0m     \u001b[43msender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_tasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mç¨‹åºæ‰§è¡Œå¤±è´¥ï¼š\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 197\u001b[0m, in \u001b[0;36mOptimizedTaskSender.send_tasks\u001b[1;34m(self, valid_tasks)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“¤ å·²å‘é€ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msent_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_tasks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)ï¼Œé˜Ÿåˆ—å½“å‰é•¿åº¦ï¼š\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_queue_length()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent_count \u001b[38;5;241m<\u001b[39m total_tasks:\n\u001b[1;32m--> 197\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSEND_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ æ‰¹é‡å‘é€å¤±è´¥ï¼š\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mï¼Œå°†é‡è¯•å½“å‰æ‰¹æ¬¡\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ---------------------- é…ç½®å‚æ•° ----------------------\n",
    "REDIS_CONFIG_PATH = \"redis.conf\"\n",
    "TASK_QUEUE = \"function_calls\"\n",
    "DAILY_K_PATH = r\"D:\\workspace\\xiaoyao\\data\\stock_daily_price.parquet\"\n",
    "\n",
    "# ä¼˜åŒ–çš„é˜Ÿåˆ—æ§åˆ¶å‚æ•°\n",
    "MAX_QUEUE_SIZE = 20000      # é˜Ÿåˆ—æœ€å¤§ç¼“å­˜ä»»åŠ¡æ•°\n",
    "BATCH_SIZE = 2000           # æ¯æ‰¹å‘é€ä»»åŠ¡æ•°\n",
    "SEND_INTERVAL = 0.1         # æ‰¹æ¬¡å‘é€é—´éš”\n",
    "CHECK_INTERVAL_WHEN_FULL = 2 # é˜Ÿåˆ—æ»¡æ—¶çš„æ£€æŸ¥é—´éš”\n",
    "METADATA_BATCH_SIZE = 500   # å…ƒæ•°æ®æ‰¹é‡å†™å…¥å¤§å°\n",
    "MAX_THREADS = 4             # å…ƒæ•°æ®å†™å…¥çº¿ç¨‹æ•°\n",
    "\n",
    "# ---------------------- å·¥å…·å‡½æ•° ----------------------\n",
    "def load_redis_config(config_path):\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Redisé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{config_path}\")\n",
    "    \n",
    "    host = \"localhost\"\n",
    "    port = 6379\n",
    "    password = \"\"\n",
    "    \n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            if line.startswith('host='):\n",
    "                host = line.split('=', 1)[1].strip()\n",
    "            elif line.startswith('port='):\n",
    "                try:\n",
    "                    port = int(line.split('=', 1)[1].strip())\n",
    "                except ValueError:\n",
    "                    print(f\"è­¦å‘Šï¼športé…ç½®æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼{port}\")\n",
    "            elif line.startswith('password='):\n",
    "                password = line.split('=', 1)[1].strip()\n",
    "    \n",
    "    return {\n",
    "        \"host\": host,\n",
    "        \"port\": port,\n",
    "        \"password\": password,\n",
    "        \"decode_responses\": False,\n",
    "        \"socket_timeout\": 30,\n",
    "        \"socket_keepalive\": True\n",
    "    }\n",
    "\n",
    "def load_valid_daily_data(parquet_path):\n",
    "    if not os.path.exists(parquet_path):\n",
    "        raise FileNotFoundError(f\"æ—¥Kçº¿æ–‡ä»¶ä¸å­˜åœ¨ï¼š{parquet_path}\")\n",
    "    \n",
    "    print(f\"å¼€å§‹åŠ è½½æ—¥Kçº¿æ•°æ®ï¼š{parquet_path}\")\n",
    "    df = pd.read_parquet(\n",
    "        parquet_path,\n",
    "        columns=['date', 'stock_code', 'paused']\n",
    "    )\n",
    "    \n",
    "    valid_df = df[df['paused'] == 0].copy()\n",
    "    valid_df = valid_df[valid_df['date'] >= '2025-01-01']\n",
    "    valid_df['date'] = valid_df['date'].dt.strftime('%Y%m%d')\n",
    "    # è·å–20250101 ~ 20250130çš„valid_df\n",
    "    valid_df = valid_df[valid_df['date'] >= '20250101']\n",
    "    valid_df = valid_df[valid_df['date'] <= '20250131']\n",
    "\n",
    "    valid_tasks = valid_df.groupby(['date', 'stock_code']).size().reset_index()\n",
    "    valid_tasks = valid_tasks[['date', 'stock_code']]\n",
    "    \n",
    "    print(f\"æ—¥Kçº¿æ•°æ®åŠ è½½å®Œæˆï¼Œæœ‰æ•ˆä»»åŠ¡æ•°ï¼š{len(valid_tasks)}\")\n",
    "    return valid_tasks\n",
    "\n",
    "# ---------------------- ä»»åŠ¡å‘é€ç±» ----------------------\n",
    "class OptimizedTaskSender:\n",
    "    def __init__(self, redis_config):\n",
    "        self.redis = redis.Redis(** redis_config)\n",
    "        self._test_connection()\n",
    "        self.redis_metadata_key = \"task_metadata\"\n",
    "        \n",
    "        # åˆå§‹åŒ–çº¿ç¨‹æ± ç”¨äºå¹¶è¡Œå†™å…¥å…ƒæ•°æ®\n",
    "        self.metadata_pool = ThreadPoolExecutor(max_workers=MAX_THREADS)\n",
    "        self.metadata_futures = []\n",
    "        \n",
    "        # æ£€æµ‹Rediså®¢æˆ·ç«¯ç‰ˆæœ¬ï¼Œç¡®å®šä½¿ç”¨hmsetè¿˜æ˜¯hset\n",
    "        self.use_hmset = self._check_redis_version()\n",
    "\n",
    "    def _test_connection(self):\n",
    "        try:\n",
    "            self.redis.ping()\n",
    "            print(\"âœ… Redisè¿æ¥æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Redisè¿æ¥å¤±è´¥ï¼š{e}\")\n",
    "            raise SystemExit(1)\n",
    "\n",
    "    def _check_redis_version(self):\n",
    "        \"\"\"æ£€æŸ¥Rediså®¢æˆ·ç«¯ç‰ˆæœ¬ï¼Œå†³å®šä½¿ç”¨hmsetè¿˜æ˜¯hset\"\"\"\n",
    "        try:\n",
    "            import redis.version\n",
    "            # redis-py 3.0+ æ¨èä½¿ç”¨hset(mapping=)ï¼Œä½†ä¸ºå…¼å®¹æ—§ç‰ˆæœ¬ä¿ç•™hmseté€‰é¡¹\n",
    "            return getattr(redis.version, 'VERSION', (0,0,0)) < (3,0,0)\n",
    "        except:\n",
    "            return True  # æ— æ³•æ£€æµ‹ç‰ˆæœ¬æ—¶é»˜è®¤ä½¿ç”¨hmset\n",
    "\n",
    "    def _get_queue_length(self):\n",
    "        try:\n",
    "            return self.redis.llen(TASK_QUEUE)\n",
    "        except Exception as e:\n",
    "            print(f\"è·å–é˜Ÿåˆ—é•¿åº¦å¤±è´¥ï¼š{e}\")\n",
    "            return MAX_QUEUE_SIZE\n",
    "\n",
    "    def _batch_write_metadata(self, metadata_dict):\n",
    "        \"\"\"ä¿®å¤ï¼šæ‰¹é‡å†™å…¥å…ƒæ•°æ®ï¼Œå…¼å®¹æ–°æ—§ç‰ˆæœ¬Rediså®¢æˆ·ç«¯\"\"\"\n",
    "        try:\n",
    "            if not metadata_dict:\n",
    "                return 0\n",
    "            \n",
    "            # æ ¹æ®ç‰ˆæœ¬é€‰æ‹©åˆé€‚çš„æ‰¹é‡å†™å…¥æ–¹æ³•\n",
    "            if self.use_hmset:\n",
    "                # æ—§ç‰ˆæœ¬ï¼šä½¿ç”¨hmset\n",
    "                self.redis.hmset(self.redis_metadata_key, metadata_dict)\n",
    "            else:\n",
    "                # æ–°ç‰ˆæœ¬ï¼šä½¿ç”¨hset + mappingå‚æ•°\n",
    "                self.redis.hset(self.redis_metadata_key, mapping=metadata_dict)\n",
    "            \n",
    "            return len(metadata_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å…ƒæ•°æ®æ‰¹é‡å†™å…¥å¤±è´¥ï¼š{str(e)}\")\n",
    "            print(f\"  - å¾…å†™å…¥æ•°é‡ï¼š{len(metadata_dict)}\")\n",
    "            # åªæ‰“å°ç¬¬ä¸€ä¸ªé”®å€¼å¯¹ä½œä¸ºç¤ºä¾‹ï¼Œé¿å…ä¿¡æ¯è¿‡å¤š\n",
    "            if metadata_dict:\n",
    "                first_key = next(iter(metadata_dict.keys()))\n",
    "                print(f\"  - ç¤ºä¾‹é”®ï¼š{first_key}, å€¼é•¿åº¦ï¼š{len(metadata_dict[first_key])}\")\n",
    "            return 0\n",
    "\n",
    "    def send_tasks(self, valid_tasks):\n",
    "        total_tasks = len(valid_tasks)\n",
    "        sent_count = 0\n",
    "        print(f\"å¼€å§‹å‘é€ä»»åŠ¡ï¼Œæ€»æœ‰æ•ˆä»»åŠ¡æ•°ï¼š{total_tasks}\")\n",
    "        \n",
    "        # å…ƒæ•°æ®ç¼“å†²åŒº\n",
    "        metadata_buffer = {}\n",
    "\n",
    "        while sent_count < total_tasks:\n",
    "            current_len = self._get_queue_length()\n",
    "            if current_len >= MAX_QUEUE_SIZE:\n",
    "                print(f\"âš ï¸ é˜Ÿåˆ—å·²æ»¡ï¼ˆå½“å‰{current_len}/{MAX_QUEUE_SIZE}ï¼‰ï¼Œæš‚åœ{CHECK_INTERVAL_WHEN_FULL}ç§’...\")\n",
    "                time.sleep(CHECK_INTERVAL_WHEN_FULL)\n",
    "                continue\n",
    "\n",
    "            # è®¡ç®—æœ¬æ¬¡å¯å‘é€æ•°é‡\n",
    "            remaining = total_tasks - sent_count\n",
    "            available = MAX_QUEUE_SIZE - current_len\n",
    "            batch_count = min(remaining, available, BATCH_SIZE)\n",
    "\n",
    "            # æå–æ‰¹æ¬¡ä»»åŠ¡\n",
    "            batch = valid_tasks.iloc[sent_count:sent_count + batch_count]\n",
    "            task_bytes_list = []\n",
    "            \n",
    "            for idx in range(len(batch)):\n",
    "                task_id = f\"task_{sent_count + idx}\"\n",
    "                trade_date = batch.iloc[idx]['date']\n",
    "                stock_code = batch.iloc[idx]['stock_code']\n",
    "                \n",
    "                # æ„å»ºä»»åŠ¡\n",
    "                task = {\n",
    "                    \"func_name\": \"fetch_minute_stock_data\",\n",
    "                    \"args\": (trade_date, [stock_code]),\n",
    "                    \"kwargs\": {},\n",
    "                    \"task_id\": task_id\n",
    "                }\n",
    "                task_bytes_list.append(pickle.dumps(task))\n",
    "                \n",
    "                # æš‚å­˜å…ƒæ•°æ®åˆ°ç¼“å†²åŒº\n",
    "                metadata_buffer[task_id] = pickle.dumps((trade_date, stock_code))\n",
    "                \n",
    "                # å½“ç¼“å†²åŒºè¾¾åˆ°é˜ˆå€¼æ—¶ï¼Œæäº¤åˆ°çº¿ç¨‹æ± æ‰¹é‡å†™å…¥\n",
    "                if len(metadata_buffer) >= METADATA_BATCH_SIZE:\n",
    "                    self.metadata_futures.append(\n",
    "                        self.metadata_pool.submit(self._batch_write_metadata, metadata_buffer.copy())\n",
    "                    )\n",
    "                    metadata_buffer.clear()\n",
    "\n",
    "            # æ‰¹é‡å‘é€ä»»åŠ¡\n",
    "            try:\n",
    "                self.redis.rpush(TASK_QUEUE, *task_bytes_list)\n",
    "                sent_count += batch_count\n",
    "                \n",
    "                # è¿›åº¦æ‰“å°ï¼šæ¯10000ä»»åŠ¡æˆ–æœ€åä¸€æ‰¹æ‰“å°ä¸€æ¬¡\n",
    "                if sent_count % 10000 == 0 or sent_count == total_tasks:\n",
    "                    progress = (sent_count / total_tasks) * 100\n",
    "                    print(f\"ğŸ“¤ å·²å‘é€ {sent_count}/{total_tasks} ({progress:.2f}%)ï¼Œé˜Ÿåˆ—å½“å‰é•¿åº¦ï¼š{self._get_queue_length()}\")\n",
    "                \n",
    "                if sent_count < total_tasks:\n",
    "                    time.sleep(SEND_INTERVAL)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ æ‰¹é‡å‘é€å¤±è´¥ï¼š{e}ï¼Œå°†é‡è¯•å½“å‰æ‰¹æ¬¡\")\n",
    "                time.sleep(2)\n",
    "\n",
    "        # å¤„ç†å‰©ä½™çš„å…ƒæ•°æ®\n",
    "        if metadata_buffer:\n",
    "            self.metadata_futures.append(\n",
    "                self.metadata_pool.submit(self._batch_write_metadata, metadata_buffer)\n",
    "            )\n",
    "        \n",
    "        # ç­‰å¾…æ‰€æœ‰å…ƒæ•°æ®å†™å…¥å®Œæˆ\n",
    "        print(\"ç­‰å¾…å‰©ä½™å…ƒæ•°æ®å†™å…¥...\")\n",
    "        for future in self.metadata_futures:\n",
    "            future.result()\n",
    "        \n",
    "        # å…³é—­çº¿ç¨‹æ± \n",
    "        self.metadata_pool.shutdown()\n",
    "        \n",
    "        print(\"âœ… æ‰€æœ‰ä»»åŠ¡å‘é€å®Œæˆ\")\n",
    "\n",
    "# ---------------------- ä¸»å‡½æ•° ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        redis_config = load_redis_config(REDIS_CONFIG_PATH)\n",
    "        print(f\"å·²åŠ è½½Redisé…ç½®ï¼šhost={redis_config['host']}, port={redis_config['port']}\")\n",
    "        \n",
    "        valid_tasks = load_valid_daily_data(DAILY_K_PATH)\n",
    "        \n",
    "        sender = OptimizedTaskSender(redis_config)\n",
    "        sender.send_tasks(valid_tasks)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼š{e}\")\n",
    "        raise SystemExit(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99083ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
