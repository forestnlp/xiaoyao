{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c12029",
   "metadata": {},
   "source": [
    "## 1.下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb58c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 发送端：Redis连接成功\n",
      "✅ 发送端pandas版本：2.3.2\n",
      "=== 共需获取 1 天的数据 ===\n",
      "\n",
      "=== 正在处理 1/1：20251016 ===\n",
      "📤 已调用远程函数：fetch_daily_market_cap（任务ID：task_e6b99326）\n",
      "✅ 保存成功：stock_daily_marketcap_20251016.csv（5158条记录）\n",
      "\n",
      "=== 所有日期处理完成 ===\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import pickle\n",
    "import time\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from typing import Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RemoteSender:\n",
    "    def __init__(self, host='*', port=6379, password='*'):\n",
    "        self.redis = redis.Redis(\n",
    "            host=host, port=port, password=password,\n",
    "            decode_responses=False\n",
    "        )\n",
    "        self.task_queue = 'function_calls'\n",
    "        self.result_queue = 'function_results'\n",
    "        self._test_connection()\n",
    "        print(f\"✅ 发送端pandas版本：{pd.__version__}\")\n",
    "\n",
    "    def _test_connection(self):\n",
    "        try:\n",
    "            self.redis.ping()\n",
    "            print(\"✅ 发送端：Redis连接成功\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 发送端：连接失败 - {e}\")\n",
    "            raise\n",
    "\n",
    "    def call_remote_function(self, func_name: str, *args, **kwargs) -> Any:\n",
    "        task_id = f\"task_{uuid.uuid4().hex[:8]}\"\n",
    "        task = {\n",
    "            'func_name': func_name,\n",
    "            'args': args,\n",
    "            'kwargs': kwargs,\n",
    "            'task_id': task_id\n",
    "        }\n",
    "        self.redis.rpush(self.task_queue, pickle.dumps(task))\n",
    "        print(f\"📤 已调用远程函数：{func_name}（任务ID：{task_id}）\")\n",
    "        return self._get_result(task_id)\n",
    "\n",
    "    def _get_result(self, task_id: str, timeout=300) -> Any:\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < timeout:\n",
    "            result_data = self.redis.blpop(self.result_queue, timeout=10)\n",
    "            if not result_data:\n",
    "                continue\n",
    "\n",
    "            _, res_bytes = result_data\n",
    "            result = pickle.loads(res_bytes)\n",
    "            if result['task_id'] == task_id:\n",
    "                if result['status'] == 'success':\n",
    "                    return result['result']  # 返回CSV字符串\n",
    "                else:\n",
    "                    raise Exception(f\"远程执行失败：{result['error']}\")\n",
    "            self.redis.rpush(self.result_queue, res_bytes)\n",
    "        raise TimeoutError(\"任务超时\")\n",
    "\n",
    "    def save_to_csv(self, csv_str: Optional[str], filename: str) -> bool:\n",
    "        \"\"\"将CSV字符串保存为本地CSV文件（替代Parquet）\"\"\"\n",
    "        if not csv_str:\n",
    "            print(\"⚠️ 数据为空，不保存\")\n",
    "            return False\n",
    "        try:\n",
    "            # 从CSV字符串恢复DataFrame（兼容所有pandas版本）\n",
    "            df = pd.read_csv(StringIO(csv_str))\n",
    "            # 保存为CSV文件\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"✅ 保存成功：{filename}（{len(df)}条记录）\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 保存失败：{e}\")\n",
    "            return False\n",
    "\n",
    "def generate_date_range(start_date_str: str, end_date_str: str) -> list:\n",
    "    \"\"\"生成从开始日期到结束日期的所有日期字符串（YYYYMMDD格式）\"\"\"\n",
    "    dates = []\n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "        end_date = datetime.strptime(end_date_str, '%Y%m%d')\n",
    "        \n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"开始日期晚于结束日期\")\n",
    "            \n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            dates.append(current_date.strftime('%Y%m%d'))\n",
    "            current_date += timedelta(days=1)\n",
    "    except Exception as e:\n",
    "        print(f\"日期处理错误：{e}\")\n",
    "    return dates\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 从配置文件读取Redis连接信息\n",
    "    with open('redis.conf', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('host='):\n",
    "                host = line.split('=')[1].strip()\n",
    "            elif line.startswith('port='):\n",
    "                port = int(line.split('=')[1].strip())\n",
    "            elif line.startswith('password='):\n",
    "                password = line.split('=')[1].strip()\n",
    "    # 初始化Redis发送端\n",
    "    sender = RemoteSender(host=host, port=port, password=password)\n",
    "    \n",
    "    # 定义日期范围：从20250516到20250923\n",
    "    # 读取../data/stock_daily_price.parquet文件，获取最大的日期+1，是start_date\n",
    "    df = pd.read_parquet('../data/stock_daily_marketcap.parquet')\n",
    "    start_date = (df['date'].max() + timedelta(days=1)).strftime('%Y%m%d')\n",
    "    # 获取当日日期-1，是end_date\n",
    "    end_date = (datetime.today() - timedelta(days=1)).strftime('%Y%m%d')\n",
    "    \n",
    "    # 生成日期列表\n",
    "    date_list = generate_date_range(start_date, end_date)\n",
    "    print(f\"=== 共需获取 {len(date_list)} 天的数据 ===\")\n",
    "    \n",
    "    # 循环调用获取每日数据\n",
    "    for i, date in enumerate(date_list, 1):\n",
    "        print(f\"\\n=== 正在处理 {i}/{len(date_list)}：{date} ===\")\n",
    "        try:\n",
    "            # 调用远程函数获取当日数据\n",
    "            csv_data = sender.call_remote_function('fetch_daily_market_cap', date)\n",
    "            # 保存为CSV文件，文件名包含日期\n",
    "            sender.save_to_csv(csv_data, f'stock_daily_marketcap_{date}.csv')\n",
    "            \n",
    "            # 适当延迟，避免请求过于频繁\n",
    "            time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {date} 处理失败：{e}\")\n",
    "            # 失败后也延迟一下，避免快速重试导致的问题\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(\"\\n=== 所有日期处理完成 ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47950e5",
   "metadata": {},
   "source": [
    "## 将csv合并为一个parquet文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68654520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🚀 开始合并 stock_***.csv 文件到 parquet\n",
      "============================================================\n",
      "📁 开始处理目录: d:/workspace/xiaoyao/redis/\n",
      "📊 找到 1 个 CSV 文件\n",
      "正在处理 (1/1): stock_daily_marketcap_20251016.csv\n",
      "  ✅ 成功读取 5158 条记录\n",
      "\n",
      "📊 合并所有数据...\n",
      "📈 总计 5158 条记录（去重后）\n",
      "✅ 成功保存到: d:/workspace/xiaoyao/redis/parquet\\stock_daily_marketcap_to_merged.parquet\n",
      "📊 文件大小: 0.44 MB\n",
      "\n",
      "🎉 合并完成！\n",
      "\n",
      "📋 验证结果:\n",
      "   总行数: 5158\n",
      "   日期范围: 2025-10-16 00:00:00 到 2025-10-16 00:00:00\n",
      "   股票数量: 5158\n",
      "   列名: ['date', 'stock_code', 'capitalization', 'circulating_cap', 'market_cap', 'circulating_market_cap', 'turnover_ratio', 'pe_ratio', 'pe_ratio_lyr', 'pb_ratio', 'ps_ratio', 'pcf_ratio']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "将 /d:/workspace/xiaoyao/redis/ 目录下的所有 stock_***.csv 文件合并为一个 parquet 文件\n",
    "确保与现有 /d:/workspace/xiaoyao/data/stock_daily_price.parquet 保持字段、压缩方式一致\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "\n",
    "def merge_stock_csv_to_parquet(csv_dir, output_parquet_file):\n",
    "    \"\"\"\n",
    "    合并指定目录下的所有 stock_***.csv 文件到单个 parquet 文件\n",
    "    \n",
    "    Args:\n",
    "        csv_dir: CSV 文件所在目录\n",
    "        output_parquet_file: 输出的 parquet 文件路径\n",
    "    \"\"\"\n",
    "    print(f\"📁 开始处理目录: {csv_dir}\")\n",
    "    \n",
    "    # 获取所有 stock_***.csv 文件\n",
    "    csv_pattern = os.path.join(csv_dir, \"stock_daily_marketcap_*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"❌ 未找到 stock_***.csv 文件\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"📊 找到 {len(csv_files)} 个 CSV 文件\")\n",
    "    \n",
    "    # 按文件名排序（确保按日期顺序处理）\n",
    "    csv_files.sort()\n",
    "    \n",
    "    # 读取并合并所有 CSV 文件\n",
    "    all_dataframes = []\n",
    "    total_records = 0\n",
    "    \n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        filename = os.path.basename(csv_file)\n",
    "        print(f\"正在处理 ({i}/{len(csv_files)}): {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取 CSV 文件\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # 数据验证和清洗\n",
    "            # 确保 date 列是 datetime 类型\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            # 确保数值列的数据类型正确\n",
    "            numeric_columns = ['capitalization','circulating_cap','market_cap','circulating_market_cap','turnover_ratio','pe_ratio','pe_ratio_lyr','pb_ratio','ps_ratio','pcf_ratio']\n",
    "            \n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # 删除无效数据\n",
    "            df = df.dropna(subset=['date', 'stock_code'])\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            total_records += len(df)\n",
    "            print(f\"  ✅ 成功读取 {len(df)} 条记录\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ 处理失败: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"❌ 没有成功读取任何数据\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\n📊 合并所有数据...\")\n",
    "    # 合并所有数据框\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # 去重（按 date + stock_code）\n",
    "    combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code'])\n",
    "    \n",
    "    # 按日期和股票代码排序\n",
    "    combined_df = combined_df.sort_values(['date', 'stock_code']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"📈 总计 {len(combined_df)} 条记录（去重后）\")\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    output_dir = os.path.dirname(output_parquet_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"📁 创建输出目录: {output_dir}\")\n",
    "    \n",
    "    # 转换为 pyarrow Table\n",
    "    table = pa.Table.from_pandas(combined_df)\n",
    "    \n",
    "    # 使用与目标文件相同的压缩方式 (snappy) 和格式写入 parquet\n",
    "    try:\n",
    "        pq.write_table(\n",
    "            table, \n",
    "            output_parquet_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',  # 使用较新的 parquet 版本\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024  # 64MB batch size for better performance\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 成功保存到: {output_parquet_file}\")\n",
    "        print(f\"📊 文件大小: {os.path.getsize(output_parquet_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 保存 parquet 文件失败: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置路径\n",
    "    csv_directory = \"d:/workspace/xiaoyao/redis/\"\n",
    "    # 确保输出目录存在\n",
    "    output_dir = os.path.join(csv_directory, 'parquet')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"📁 创建输出目录: {output_dir}\")\n",
    "\n",
    "    output_file = os.path.join(output_dir, 'stock_daily_marketcap_to_merged.parquet')\n",
    "    \n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"🚀 开始合并 stock_***.csv 文件到 parquet\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 执行合并\n",
    "    success = merge_stock_csv_to_parquet(csv_directory, output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎉 合并完成！\")\n",
    "        \n",
    "        # 验证结果\n",
    "        try:\n",
    "            print(\"\\n📋 验证结果:\")\n",
    "            result_df = pd.read_parquet(output_file)\n",
    "            print(f\"   总行数: {len(result_df)}\")\n",
    "            print(f\"   日期范围: {result_df['date'].min()} 到 {result_df['date'].max()}\")\n",
    "            print(f\"   股票数量: {result_df['stock_code'].nunique()}\")\n",
    "            print(f\"   列名: {list(result_df.columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  验证失败: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n❌ 合并失败！\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab418c",
   "metadata": {},
   "source": [
    "## 将新生成的 stock_daily_auction_to_merged.parquet.parquet 与现有的 stock_daily_auction.parquet 合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0e4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🚀 开始合并 parquet 文件\n",
      "============================================================\n",
      "📊 开始合并 parquet 文件...\n",
      "📖 读取现有文件: d:/workspace/xiaoyao/data/stock_daily_marketcap.parquet\n",
      "   现有数据行数: 967167\n",
      "📖 读取新文件: d:/workspace/xiaoyao/redis/parquet/stock_daily_marketcap_to_merged.parquet\n",
      "   新数据行数: 5158\n",
      "🔄 合并数据中...\n",
      "🧹 去重处理...\n",
      "📅 按日期排序...\n",
      "📈 合并后总行数: 972325\n",
      "💾 保存合并结果: d:/workspace/xiaoyao/redis/parquet/stock_daily_marketcap.parquet\n",
      "✅ 合并完成！文件大小: 44.87 MB\n",
      "\n",
      "📋 验证结果:\n",
      "   最终行数: 972325\n",
      "   日期范围: 2025-01-02 00:00:00 到 2025-10-16 00:00:00\n",
      "   股票数量: 5185\n",
      "\n",
      "🎉 合并成功！\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "将新生成的 stock_daily_marketcap_to_merged.parquet.parquet 与现有的 stock_daily_marketcap.parquet 合并\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_parquet_files(existing_file, new_file, output_file):\n",
    "    \"\"\"\n",
    "    合并两个 parquet 文件\n",
    "    \n",
    "    Args:\n",
    "        existing_file: 现有的 parquet 文件路径\n",
    "        new_file: 新的 parquet 文件路径  \n",
    "        output_file: 输出的合并文件路径\n",
    "    \"\"\"\n",
    "    print(\"📊 开始合并 parquet 文件...\")\n",
    "    \n",
    "    try:\n",
    "        # 读取现有数据\n",
    "        print(f\"📖 读取现有文件: {existing_file}\")\n",
    "        existing_df = pd.read_parquet(existing_file)\n",
    "        print(f\"   现有数据行数: {len(existing_df)}\")\n",
    "        \n",
    "        # 读取新数据\n",
    "        print(f\"📖 读取新文件: {new_file}\")\n",
    "        new_df = pd.read_parquet(new_file)\n",
    "        print(f\"   新数据行数: {len(new_df)}\")\n",
    "        \n",
    "        # 合并数据\n",
    "        print(\"🔄 合并数据中...\")\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        \n",
    "        # 去重（按 date + stock_code）\n",
    "        print(\"🧹 去重处理...\")\n",
    "        combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code'])\n",
    "        \n",
    "        # 排序\n",
    "        print(\"📅 按日期排序...\")\n",
    "        combined_df = combined_df.sort_values(['date', 'stock_code']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"📈 合并后总行数: {len(combined_df)}\")\n",
    "        \n",
    "        # 确保输出目录存在\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # 转换为 pyarrow Table\n",
    "        table = pa.Table.from_pandas(combined_df)\n",
    "        \n",
    "        # 写入 parquet（使用与源文件相同的格式）\n",
    "        print(f\"💾 保存合并结果: {output_file}\")\n",
    "        pq.write_table(\n",
    "            table,\n",
    "            output_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 合并完成！文件大小: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # 验证结果\n",
    "        print(\"\\n📋 验证结果:\")\n",
    "        result_df = pd.read_parquet(output_file)\n",
    "        print(f\"   最终行数: {len(result_df)}\")\n",
    "        print(f\"   日期范围: {result_df['date'].min()} 到 {result_df['date'].max()}\")\n",
    "        print(f\"   股票数量: {result_df['stock_code'].nunique()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 合并失败: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置文件路径\n",
    "\n",
    "    existing_file = \"d:/workspace/xiaoyao/data/stock_daily_marketcap.parquet\"\n",
    "    new_file = \"d:/workspace/xiaoyao/redis/parquet/stock_daily_marketcap_to_merged.parquet\"\n",
    "    output_file = \"d:/workspace/xiaoyao/redis/parquet/stock_daily_marketcap.parquet\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🚀 开始合并 parquet 文件\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(existing_file):\n",
    "        print(f\"❌ 现有文件不存在: {existing_file}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(new_file):\n",
    "        print(f\"❌ 新文件不存在: {new_file}\")\n",
    "        return\n",
    "    \n",
    "    # 执行合并\n",
    "    success = merge_parquet_files(existing_file, new_file, output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎉 合并成功！\")\n",
    "    else:\n",
    "        print(\"\\n❌ 合并失败！\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8770ce2",
   "metadata": {},
   "source": [
    "## 删除已使用的csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c223ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已删除：D:\\workspace\\xiaoyao\\redis\\stock_daily_marketcap_20251016.csv\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "删除指定目录下的 stock_***.csv 文件\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def delete_stock_csv_files(target_directory, pattern=\"stock_*.csv\"):\n",
    "    #删除满足模式的所有文件\n",
    "    files = glob.glob(os.path.join(target_directory, pattern))\n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"已删除：{file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"删除 {file} 失败：{e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    delete_stock_csv_files(r'D:\\workspace\\xiaoyao\\redis','stock_daily_marketcap_*.csv')\n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46e4da",
   "metadata": {},
   "source": [
    "## 将新的parquet文件移动到data目录覆盖原文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2571397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved: stock_daily_marketcap.parquet to D:\\workspace\\xiaoyao\\data\n",
      "Deleted: stock_daily_marketcap.parquet from D:\\workspace\\xiaoyao\\redis\\parquet\n"
     ]
    }
   ],
   "source": [
    "# 将子目录下的某个parquet文件移动到指定目录\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 定义源目录和目标目录\n",
    "source_dir = \"D:\\\\workspace\\\\xiaoyao\\\\redis\\\\parquet\"\n",
    "target_dir = \"D:\\\\workspace\\\\xiaoyao\\\\data\"\n",
    "\n",
    "# 确保目标目录存在\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# 定义要移动的文件\n",
    "file_to_move = \"stock_daily_marketcap.parquet\"\n",
    "\n",
    "# 构建源文件的完整路径\n",
    "source_file_path = os.path.join(source_dir, file_to_move)\n",
    "\n",
    "# 构建目标文件的完整路径\n",
    "target_file_path = os.path.join(target_dir, file_to_move)\n",
    "\n",
    "# 检查源文件是否存在\n",
    "if os.path.exists(source_file_path):\n",
    "    # 移动文件\n",
    "    shutil.move(source_file_path, target_file_path)\n",
    "    print(f\"Moved: {file_to_move} to {target_dir}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_move}\")\n",
    "\n",
    "# 删除指定的parquet文件\n",
    "file_to_delete = os.path.join(source_dir, 'stock_daily_marketcap_to_merged.parquet')\n",
    "if os.path.exists(file_to_delete):\n",
    "    os.remove(file_to_delete)\n",
    "    print(f\"Deleted: {file_to_move} from {source_dir}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_move} in {source_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d277a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
