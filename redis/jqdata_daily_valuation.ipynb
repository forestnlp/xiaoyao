{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c12029",
   "metadata": {},
   "source": [
    "## 1.ä¸‹è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb58c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å‘é€ç«¯ï¼šRedisè¿æ¥æˆåŠŸ\n",
      "âœ… å‘é€ç«¯pandasç‰ˆæœ¬ï¼š2.3.2\n",
      "=== å…±éœ€è·å– 1 å¤©çš„æ•°æ® ===\n",
      "\n",
      "=== æ­£åœ¨å¤„ç† 1/1ï¼š20251016 ===\n",
      "ğŸ“¤ å·²è°ƒç”¨è¿œç¨‹å‡½æ•°ï¼šfetch_daily_market_capï¼ˆä»»åŠ¡IDï¼štask_e6b99326ï¼‰\n",
      "âœ… ä¿å­˜æˆåŠŸï¼šstock_daily_marketcap_20251016.csvï¼ˆ5158æ¡è®°å½•ï¼‰\n",
      "\n",
      "=== æ‰€æœ‰æ—¥æœŸå¤„ç†å®Œæˆ ===\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import pickle\n",
    "import time\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from typing import Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RemoteSender:\n",
    "    def __init__(self, host='*', port=6379, password='*'):\n",
    "        self.redis = redis.Redis(\n",
    "            host=host, port=port, password=password,\n",
    "            decode_responses=False\n",
    "        )\n",
    "        self.task_queue = 'function_calls'\n",
    "        self.result_queue = 'function_results'\n",
    "        self._test_connection()\n",
    "        print(f\"âœ… å‘é€ç«¯pandasç‰ˆæœ¬ï¼š{pd.__version__}\")\n",
    "\n",
    "    def _test_connection(self):\n",
    "        try:\n",
    "            self.redis.ping()\n",
    "            print(\"âœ… å‘é€ç«¯ï¼šRedisè¿æ¥æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å‘é€ç«¯ï¼šè¿æ¥å¤±è´¥ - {e}\")\n",
    "            raise\n",
    "\n",
    "    def call_remote_function(self, func_name: str, *args, **kwargs) -> Any:\n",
    "        task_id = f\"task_{uuid.uuid4().hex[:8]}\"\n",
    "        task = {\n",
    "            'func_name': func_name,\n",
    "            'args': args,\n",
    "            'kwargs': kwargs,\n",
    "            'task_id': task_id\n",
    "        }\n",
    "        self.redis.rpush(self.task_queue, pickle.dumps(task))\n",
    "        print(f\"ğŸ“¤ å·²è°ƒç”¨è¿œç¨‹å‡½æ•°ï¼š{func_name}ï¼ˆä»»åŠ¡IDï¼š{task_id}ï¼‰\")\n",
    "        return self._get_result(task_id)\n",
    "\n",
    "    def _get_result(self, task_id: str, timeout=300) -> Any:\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < timeout:\n",
    "            result_data = self.redis.blpop(self.result_queue, timeout=10)\n",
    "            if not result_data:\n",
    "                continue\n",
    "\n",
    "            _, res_bytes = result_data\n",
    "            result = pickle.loads(res_bytes)\n",
    "            if result['task_id'] == task_id:\n",
    "                if result['status'] == 'success':\n",
    "                    return result['result']  # è¿”å›CSVå­—ç¬¦ä¸²\n",
    "                else:\n",
    "                    raise Exception(f\"è¿œç¨‹æ‰§è¡Œå¤±è´¥ï¼š{result['error']}\")\n",
    "            self.redis.rpush(self.result_queue, res_bytes)\n",
    "        raise TimeoutError(\"ä»»åŠ¡è¶…æ—¶\")\n",
    "\n",
    "    def save_to_csv(self, csv_str: Optional[str], filename: str) -> bool:\n",
    "        \"\"\"å°†CSVå­—ç¬¦ä¸²ä¿å­˜ä¸ºæœ¬åœ°CSVæ–‡ä»¶ï¼ˆæ›¿ä»£Parquetï¼‰\"\"\"\n",
    "        if not csv_str:\n",
    "            print(\"âš ï¸ æ•°æ®ä¸ºç©ºï¼Œä¸ä¿å­˜\")\n",
    "            return False\n",
    "        try:\n",
    "            # ä»CSVå­—ç¬¦ä¸²æ¢å¤DataFrameï¼ˆå…¼å®¹æ‰€æœ‰pandasç‰ˆæœ¬ï¼‰\n",
    "            df = pd.read_csv(StringIO(csv_str))\n",
    "            # ä¿å­˜ä¸ºCSVæ–‡ä»¶\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"âœ… ä¿å­˜æˆåŠŸï¼š{filename}ï¼ˆ{len(df)}æ¡è®°å½•ï¼‰\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä¿å­˜å¤±è´¥ï¼š{e}\")\n",
    "            return False\n",
    "\n",
    "def generate_date_range(start_date_str: str, end_date_str: str) -> list:\n",
    "    \"\"\"ç”Ÿæˆä»å¼€å§‹æ—¥æœŸåˆ°ç»“æŸæ—¥æœŸçš„æ‰€æœ‰æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆYYYYMMDDæ ¼å¼ï¼‰\"\"\"\n",
    "    dates = []\n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "        end_date = datetime.strptime(end_date_str, '%Y%m%d')\n",
    "        \n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"å¼€å§‹æ—¥æœŸæ™šäºç»“æŸæ—¥æœŸ\")\n",
    "            \n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            dates.append(current_date.strftime('%Y%m%d'))\n",
    "            current_date += timedelta(days=1)\n",
    "    except Exception as e:\n",
    "        print(f\"æ—¥æœŸå¤„ç†é”™è¯¯ï¼š{e}\")\n",
    "    return dates\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ä»é…ç½®æ–‡ä»¶è¯»å–Redisè¿æ¥ä¿¡æ¯\n",
    "    with open('redis.conf', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('host='):\n",
    "                host = line.split('=')[1].strip()\n",
    "            elif line.startswith('port='):\n",
    "                port = int(line.split('=')[1].strip())\n",
    "            elif line.startswith('password='):\n",
    "                password = line.split('=')[1].strip()\n",
    "    # åˆå§‹åŒ–Rediså‘é€ç«¯\n",
    "    sender = RemoteSender(host=host, port=port, password=password)\n",
    "    \n",
    "    # å®šä¹‰æ—¥æœŸèŒƒå›´ï¼šä»20250516åˆ°20250923\n",
    "    # è¯»å–../data/stock_daily_price.parquetæ–‡ä»¶ï¼Œè·å–æœ€å¤§çš„æ—¥æœŸ+1ï¼Œæ˜¯start_date\n",
    "    df = pd.read_parquet('../data/stock_daily_marketcap.parquet')\n",
    "    start_date = (df['date'].max() + timedelta(days=1)).strftime('%Y%m%d')\n",
    "    # è·å–å½“æ—¥æ—¥æœŸ-1ï¼Œæ˜¯end_date\n",
    "    end_date = (datetime.today() - timedelta(days=1)).strftime('%Y%m%d')\n",
    "    \n",
    "    # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨\n",
    "    date_list = generate_date_range(start_date, end_date)\n",
    "    print(f\"=== å…±éœ€è·å– {len(date_list)} å¤©çš„æ•°æ® ===\")\n",
    "    \n",
    "    # å¾ªç¯è°ƒç”¨è·å–æ¯æ—¥æ•°æ®\n",
    "    for i, date in enumerate(date_list, 1):\n",
    "        print(f\"\\n=== æ­£åœ¨å¤„ç† {i}/{len(date_list)}ï¼š{date} ===\")\n",
    "        try:\n",
    "            # è°ƒç”¨è¿œç¨‹å‡½æ•°è·å–å½“æ—¥æ•°æ®\n",
    "            csv_data = sender.call_remote_function('fetch_daily_market_cap', date)\n",
    "            # ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼Œæ–‡ä»¶ååŒ…å«æ—¥æœŸ\n",
    "            sender.save_to_csv(csv_data, f'stock_daily_marketcap_{date}.csv')\n",
    "            \n",
    "            # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹\n",
    "            time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {date} å¤„ç†å¤±è´¥ï¼š{e}\")\n",
    "            # å¤±è´¥åä¹Ÿå»¶è¿Ÿä¸€ä¸‹ï¼Œé¿å…å¿«é€Ÿé‡è¯•å¯¼è‡´çš„é—®é¢˜\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(\"\\n=== æ‰€æœ‰æ—¥æœŸå¤„ç†å®Œæˆ ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47950e5",
   "metadata": {},
   "source": [
    "## å°†csvåˆå¹¶ä¸ºä¸€ä¸ªparquetæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68654520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ å¼€å§‹åˆå¹¶ stock_***.csv æ–‡ä»¶åˆ° parquet\n",
      "============================================================\n",
      "ğŸ“ å¼€å§‹å¤„ç†ç›®å½•: d:/workspace/xiaoyao/redis/\n",
      "ğŸ“Š æ‰¾åˆ° 1 ä¸ª CSV æ–‡ä»¶\n",
      "æ­£åœ¨å¤„ç† (1/1): stock_daily_marketcap_20251016.csv\n",
      "  âœ… æˆåŠŸè¯»å– 5158 æ¡è®°å½•\n",
      "\n",
      "ğŸ“Š åˆå¹¶æ‰€æœ‰æ•°æ®...\n",
      "ğŸ“ˆ æ€»è®¡ 5158 æ¡è®°å½•ï¼ˆå»é‡åï¼‰\n",
      "âœ… æˆåŠŸä¿å­˜åˆ°: d:/workspace/xiaoyao/redis/parquet\\stock_daily_marketcap_to_merged.parquet\n",
      "ğŸ“Š æ–‡ä»¶å¤§å°: 0.44 MB\n",
      "\n",
      "ğŸ‰ åˆå¹¶å®Œæˆï¼\n",
      "\n",
      "ğŸ“‹ éªŒè¯ç»“æœ:\n",
      "   æ€»è¡Œæ•°: 5158\n",
      "   æ—¥æœŸèŒƒå›´: 2025-10-16 00:00:00 åˆ° 2025-10-16 00:00:00\n",
      "   è‚¡ç¥¨æ•°é‡: 5158\n",
      "   åˆ—å: ['date', 'stock_code', 'capitalization', 'circulating_cap', 'market_cap', 'circulating_market_cap', 'turnover_ratio', 'pe_ratio', 'pe_ratio_lyr', 'pb_ratio', 'ps_ratio', 'pcf_ratio']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "å°† /d:/workspace/xiaoyao/redis/ ç›®å½•ä¸‹çš„æ‰€æœ‰ stock_***.csv æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ª parquet æ–‡ä»¶\n",
    "ç¡®ä¿ä¸ç°æœ‰ /d:/workspace/xiaoyao/data/stock_daily_price.parquet ä¿æŒå­—æ®µã€å‹ç¼©æ–¹å¼ä¸€è‡´\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "\n",
    "def merge_stock_csv_to_parquet(csv_dir, output_parquet_file):\n",
    "    \"\"\"\n",
    "    åˆå¹¶æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ stock_***.csv æ–‡ä»¶åˆ°å•ä¸ª parquet æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        csv_dir: CSV æ–‡ä»¶æ‰€åœ¨ç›®å½•\n",
    "        output_parquet_file: è¾“å‡ºçš„ parquet æ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“ å¼€å§‹å¤„ç†ç›®å½•: {csv_dir}\")\n",
    "    \n",
    "    # è·å–æ‰€æœ‰ stock_***.csv æ–‡ä»¶\n",
    "    csv_pattern = os.path.join(csv_dir, \"stock_daily_marketcap_*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"âŒ æœªæ‰¾åˆ° stock_***.csv æ–‡ä»¶\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"ğŸ“Š æ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶\")\n",
    "    \n",
    "    # æŒ‰æ–‡ä»¶åæ’åºï¼ˆç¡®ä¿æŒ‰æ—¥æœŸé¡ºåºå¤„ç†ï¼‰\n",
    "    csv_files.sort()\n",
    "    \n",
    "    # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰ CSV æ–‡ä»¶\n",
    "    all_dataframes = []\n",
    "    total_records = 0\n",
    "    \n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        filename = os.path.basename(csv_file)\n",
    "        print(f\"æ­£åœ¨å¤„ç† ({i}/{len(csv_files)}): {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # è¯»å– CSV æ–‡ä»¶\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # æ•°æ®éªŒè¯å’Œæ¸…æ´—\n",
    "            # ç¡®ä¿ date åˆ—æ˜¯ datetime ç±»å‹\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            # ç¡®ä¿æ•°å€¼åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®\n",
    "            numeric_columns = ['capitalization','circulating_cap','market_cap','circulating_market_cap','turnover_ratio','pe_ratio','pe_ratio_lyr','pb_ratio','ps_ratio','pcf_ratio']\n",
    "            \n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # åˆ é™¤æ— æ•ˆæ•°æ®\n",
    "            df = df.dropna(subset=['date', 'stock_code'])\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            total_records += len(df)\n",
    "            print(f\"  âœ… æˆåŠŸè¯»å– {len(df)} æ¡è®°å½•\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ å¤„ç†å¤±è´¥: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ•°æ®\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\nğŸ“Š åˆå¹¶æ‰€æœ‰æ•°æ®...\")\n",
    "    # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # å»é‡ï¼ˆæŒ‰ date + stock_codeï¼‰\n",
    "    combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code'])\n",
    "    \n",
    "    # æŒ‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç æ’åº\n",
    "    combined_df = combined_df.sort_values(['date', 'stock_code']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ æ€»è®¡ {len(combined_df)} æ¡è®°å½•ï¼ˆå»é‡åï¼‰\")\n",
    "    \n",
    "    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "    output_dir = os.path.dirname(output_parquet_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}\")\n",
    "    \n",
    "    # è½¬æ¢ä¸º pyarrow Table\n",
    "    table = pa.Table.from_pandas(combined_df)\n",
    "    \n",
    "    # ä½¿ç”¨ä¸ç›®æ ‡æ–‡ä»¶ç›¸åŒçš„å‹ç¼©æ–¹å¼ (snappy) å’Œæ ¼å¼å†™å…¥ parquet\n",
    "    try:\n",
    "        pq.write_table(\n",
    "            table, \n",
    "            output_parquet_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',  # ä½¿ç”¨è¾ƒæ–°çš„ parquet ç‰ˆæœ¬\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024  # 64MB batch size for better performance\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… æˆåŠŸä¿å­˜åˆ°: {output_parquet_file}\")\n",
    "        print(f\"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(output_parquet_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜ parquet æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    # è®¾ç½®è·¯å¾„\n",
    "    csv_directory = \"d:/workspace/xiaoyao/redis/\"\n",
    "    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "    output_dir = os.path.join(csv_directory, 'parquet')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}\")\n",
    "\n",
    "    output_file = os.path.join(output_dir, 'stock_daily_marketcap_to_merged.parquet')\n",
    "    \n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸš€ å¼€å§‹åˆå¹¶ stock_***.csv æ–‡ä»¶åˆ° parquet\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æ‰§è¡Œåˆå¹¶\n",
    "    success = merge_stock_csv_to_parquet(csv_directory, output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nğŸ‰ åˆå¹¶å®Œæˆï¼\")\n",
    "        \n",
    "        # éªŒè¯ç»“æœ\n",
    "        try:\n",
    "            print(\"\\nğŸ“‹ éªŒè¯ç»“æœ:\")\n",
    "            result_df = pd.read_parquet(output_file)\n",
    "            print(f\"   æ€»è¡Œæ•°: {len(result_df)}\")\n",
    "            print(f\"   æ—¥æœŸèŒƒå›´: {result_df['date'].min()} åˆ° {result_df['date'].max()}\")\n",
    "            print(f\"   è‚¡ç¥¨æ•°é‡: {result_df['stock_code'].nunique()}\")\n",
    "            print(f\"   åˆ—å: {list(result_df.columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  éªŒè¯å¤±è´¥: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nâŒ åˆå¹¶å¤±è´¥ï¼\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab418c",
   "metadata": {},
   "source": [
    "## å°†æ–°ç”Ÿæˆçš„ stock_daily_auction_to_merged.parquet.parquet ä¸ç°æœ‰çš„ stock_daily_auction.parquet åˆå¹¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0e4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶\n",
      "============================================================\n",
      "ğŸ“Š å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶...\n",
      "ğŸ“– è¯»å–ç°æœ‰æ–‡ä»¶: d:/workspace/xiaoyao/data/stock_daily_marketcap.parquet\n",
      "   ç°æœ‰æ•°æ®è¡Œæ•°: 967167\n",
      "ğŸ“– è¯»å–æ–°æ–‡ä»¶: d:/workspace/xiaoyao/redis/parquet/stock_daily_marketcap_to_merged.parquet\n",
      "   æ–°æ•°æ®è¡Œæ•°: 5158\n",
      "ğŸ”„ åˆå¹¶æ•°æ®ä¸­...\n",
      "ğŸ§¹ å»é‡å¤„ç†...\n",
      "ğŸ“… æŒ‰æ—¥æœŸæ’åº...\n",
      "ğŸ“ˆ åˆå¹¶åæ€»è¡Œæ•°: 972325\n",
      "ğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœ: d:/workspace/xiaoyao/redis/parquet/stock_daily_marketcap.parquet\n",
      "âœ… åˆå¹¶å®Œæˆï¼æ–‡ä»¶å¤§å°: 44.87 MB\n",
      "\n",
      "ğŸ“‹ éªŒè¯ç»“æœ:\n",
      "   æœ€ç»ˆè¡Œæ•°: 972325\n",
      "   æ—¥æœŸèŒƒå›´: 2025-01-02 00:00:00 åˆ° 2025-10-16 00:00:00\n",
      "   è‚¡ç¥¨æ•°é‡: 5185\n",
      "\n",
      "ğŸ‰ åˆå¹¶æˆåŠŸï¼\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "å°†æ–°ç”Ÿæˆçš„ stock_daily_marketcap_to_merged.parquet.parquet ä¸ç°æœ‰çš„ stock_daily_marketcap.parquet åˆå¹¶\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_parquet_files(existing_file, new_file, output_file):\n",
    "    \"\"\"\n",
    "    åˆå¹¶ä¸¤ä¸ª parquet æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        existing_file: ç°æœ‰çš„ parquet æ–‡ä»¶è·¯å¾„\n",
    "        new_file: æ–°çš„ parquet æ–‡ä»¶è·¯å¾„  \n",
    "        output_file: è¾“å‡ºçš„åˆå¹¶æ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶...\")\n",
    "    \n",
    "    try:\n",
    "        # è¯»å–ç°æœ‰æ•°æ®\n",
    "        print(f\"ğŸ“– è¯»å–ç°æœ‰æ–‡ä»¶: {existing_file}\")\n",
    "        existing_df = pd.read_parquet(existing_file)\n",
    "        print(f\"   ç°æœ‰æ•°æ®è¡Œæ•°: {len(existing_df)}\")\n",
    "        \n",
    "        # è¯»å–æ–°æ•°æ®\n",
    "        print(f\"ğŸ“– è¯»å–æ–°æ–‡ä»¶: {new_file}\")\n",
    "        new_df = pd.read_parquet(new_file)\n",
    "        print(f\"   æ–°æ•°æ®è¡Œæ•°: {len(new_df)}\")\n",
    "        \n",
    "        # åˆå¹¶æ•°æ®\n",
    "        print(\"ğŸ”„ åˆå¹¶æ•°æ®ä¸­...\")\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        \n",
    "        # å»é‡ï¼ˆæŒ‰ date + stock_codeï¼‰\n",
    "        print(\"ğŸ§¹ å»é‡å¤„ç†...\")\n",
    "        combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code'])\n",
    "        \n",
    "        # æ’åº\n",
    "        print(\"ğŸ“… æŒ‰æ—¥æœŸæ’åº...\")\n",
    "        combined_df = combined_df.sort_values(['date', 'stock_code']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"ğŸ“ˆ åˆå¹¶åæ€»è¡Œæ•°: {len(combined_df)}\")\n",
    "        \n",
    "        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # è½¬æ¢ä¸º pyarrow Table\n",
    "        table = pa.Table.from_pandas(combined_df)\n",
    "        \n",
    "        # å†™å…¥ parquetï¼ˆä½¿ç”¨ä¸æºæ–‡ä»¶ç›¸åŒçš„æ ¼å¼ï¼‰\n",
    "        print(f\"ğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœ: {output_file}\")\n",
    "        pq.write_table(\n",
    "            table,\n",
    "            output_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… åˆå¹¶å®Œæˆï¼æ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # éªŒè¯ç»“æœ\n",
    "        print(\"\\nğŸ“‹ éªŒè¯ç»“æœ:\")\n",
    "        result_df = pd.read_parquet(output_file)\n",
    "        print(f\"   æœ€ç»ˆè¡Œæ•°: {len(result_df)}\")\n",
    "        print(f\"   æ—¥æœŸèŒƒå›´: {result_df['date'].min()} åˆ° {result_df['date'].max()}\")\n",
    "        print(f\"   è‚¡ç¥¨æ•°é‡: {result_df['stock_code'].nunique()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åˆå¹¶å¤±è´¥: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    # è®¾ç½®æ–‡ä»¶è·¯å¾„\n",
    "\n",
    "    existing_file = \"d:/workspace/xiaoyao/data/stock_daily_marketcap.parquet\"\n",
    "    new_file = \"d:/workspace/xiaoyao/redis/parquet/stock_daily_marketcap_to_merged.parquet\"\n",
    "    output_file = \"d:/workspace/xiaoyao/redis/parquet/stock_daily_marketcap.parquet\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸš€ å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(existing_file):\n",
    "        print(f\"âŒ ç°æœ‰æ–‡ä»¶ä¸å­˜åœ¨: {existing_file}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(new_file):\n",
    "        print(f\"âŒ æ–°æ–‡ä»¶ä¸å­˜åœ¨: {new_file}\")\n",
    "        return\n",
    "    \n",
    "    # æ‰§è¡Œåˆå¹¶\n",
    "    success = merge_parquet_files(existing_file, new_file, output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nğŸ‰ åˆå¹¶æˆåŠŸï¼\")\n",
    "    else:\n",
    "        print(\"\\nâŒ åˆå¹¶å¤±è´¥ï¼\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8770ce2",
   "metadata": {},
   "source": [
    "## åˆ é™¤å·²ä½¿ç”¨çš„csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c223ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²åˆ é™¤ï¼šD:\\workspace\\xiaoyao\\redis\\stock_daily_marketcap_20251016.csv\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "åˆ é™¤æŒ‡å®šç›®å½•ä¸‹çš„ stock_***.csv æ–‡ä»¶\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def delete_stock_csv_files(target_directory, pattern=\"stock_*.csv\"):\n",
    "    #åˆ é™¤æ»¡è¶³æ¨¡å¼çš„æ‰€æœ‰æ–‡ä»¶\n",
    "    files = glob.glob(os.path.join(target_directory, pattern))\n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"å·²åˆ é™¤ï¼š{file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"åˆ é™¤ {file} å¤±è´¥ï¼š{e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    delete_stock_csv_files(r'D:\\workspace\\xiaoyao\\redis','stock_daily_marketcap_*.csv')\n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46e4da",
   "metadata": {},
   "source": [
    "## å°†æ–°çš„parquetæ–‡ä»¶ç§»åŠ¨åˆ°dataç›®å½•è¦†ç›–åŸæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2571397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved: stock_daily_marketcap.parquet to D:\\workspace\\xiaoyao\\data\n",
      "Deleted: stock_daily_marketcap.parquet from D:\\workspace\\xiaoyao\\redis\\parquet\n"
     ]
    }
   ],
   "source": [
    "# å°†å­ç›®å½•ä¸‹çš„æŸä¸ªparquetæ–‡ä»¶ç§»åŠ¨åˆ°æŒ‡å®šç›®å½•\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# å®šä¹‰æºç›®å½•å’Œç›®æ ‡ç›®å½•\n",
    "source_dir = \"D:\\\\workspace\\\\xiaoyao\\\\redis\\\\parquet\"\n",
    "target_dir = \"D:\\\\workspace\\\\xiaoyao\\\\data\"\n",
    "\n",
    "# ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# å®šä¹‰è¦ç§»åŠ¨çš„æ–‡ä»¶\n",
    "file_to_move = \"stock_daily_marketcap.parquet\"\n",
    "\n",
    "# æ„å»ºæºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„\n",
    "source_file_path = os.path.join(source_dir, file_to_move)\n",
    "\n",
    "# æ„å»ºç›®æ ‡æ–‡ä»¶çš„å®Œæ•´è·¯å¾„\n",
    "target_file_path = os.path.join(target_dir, file_to_move)\n",
    "\n",
    "# æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "if os.path.exists(source_file_path):\n",
    "    # ç§»åŠ¨æ–‡ä»¶\n",
    "    shutil.move(source_file_path, target_file_path)\n",
    "    print(f\"Moved: {file_to_move} to {target_dir}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_move}\")\n",
    "\n",
    "# åˆ é™¤æŒ‡å®šçš„parquetæ–‡ä»¶\n",
    "file_to_delete = os.path.join(source_dir, 'stock_daily_marketcap_to_merged.parquet')\n",
    "if os.path.exists(file_to_delete):\n",
    "    os.remove(file_to_delete)\n",
    "    print(f\"Deleted: {file_to_move} from {source_dir}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_move} in {source_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d277a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
