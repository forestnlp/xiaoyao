{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2539bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²åŠ è½½Redisé…ç½®ï¼šhost=220.203.1.124, port=6379\n",
      "âœ… ä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼šD:\\workspace\\xiaoyao\\redis\\temp_csv\n",
      "âœ… æœ€ç»ˆå­˜å‚¨ç›®å½•ï¼šD:\\workspace\\xiaoyao\\data\\stock_minutely_price\n",
      "âœ… Redisè¿æ¥æˆåŠŸ\n",
      "âœ… ä»Redisè·å–ä»»åŠ¡å…ƒä¿¡æ¯ï¼Œå…±71718ä¸ªä»»åŠ¡\n",
      "âŒ ç¨‹åºå¯åŠ¨å¤±è´¥ï¼š'ResultProcessor' object has no attribute 'receive_and_process_results'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jay\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import threading\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor  # çº¿ç¨‹æ± \n",
    "\n",
    "# ---------------------- é…ç½®å‚æ•° ----------------------\n",
    "REDIS_CONFIG_PATH = \"redis.conf\"\n",
    "RESULT_QUEUE = \"function_results\"\n",
    "STORAGE_ROOT = r\"D:\\workspace\\xiaoyao\\data\\stock_minutely_price\"\n",
    "MAX_CONVERT_THREADS = 20  # çº¿ç¨‹æ± å¤§å°ï¼Œä¸workeræ•°é‡åŒ¹é…\n",
    "TEMP_DIR = r\"D:\\workspace\\xiaoyao\\redis\\temp_csv\"  # ä¸´æ—¶æ–‡ä»¶ç›®å½•\n",
    "\n",
    "# ---------------------- å·¥å…·å‡½æ•° ----------------------\n",
    "def load_redis_config(config_path):\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Redisé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{config_path}\")\n",
    "    \n",
    "    host = \"localhost\"\n",
    "    port = 6379\n",
    "    password = \"\"\n",
    "    \n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            if line.startswith('host='):\n",
    "                host = line.split('=', 1)[1].strip()\n",
    "            elif line.startswith('port='):\n",
    "                try:\n",
    "                    port = int(line.split('=', 1)[1].strip())\n",
    "                except ValueError:\n",
    "                    print(f\"è­¦å‘Šï¼športé…ç½®æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼{port}\")\n",
    "            elif line.startswith('password='):\n",
    "                password = line.split('=', 1)[1].strip()\n",
    "    \n",
    "    return {\n",
    "        \"host\": host,\n",
    "        \"port\": port,\n",
    "        \"password\": password,\n",
    "        \"decode_responses\": False,\n",
    "        \"socket_timeout\": 30,\n",
    "        \"socket_keepalive\": True\n",
    "    }\n",
    "\n",
    "# ---------------------- ç»“æœå¤„ç†ç±» ----------------------\n",
    "class ResultProcessor:\n",
    "    def __init__(self, redis_config):\n",
    "        # 1. åˆå§‹åŒ–ç›®å½•ï¼ˆä¸´æ—¶ç›®å½•+å­˜å‚¨ç›®å½•ï¼‰\n",
    "        os.makedirs(STORAGE_ROOT, exist_ok=True)\n",
    "        os.makedirs(TEMP_DIR, exist_ok=True)  # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨\n",
    "        print(f\"âœ… ä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼š{TEMP_DIR}\")\n",
    "        print(f\"âœ… æœ€ç»ˆå­˜å‚¨ç›®å½•ï¼š{STORAGE_ROOT}\")\n",
    "        \n",
    "        # 2. Redisè¿æ¥åˆå§‹åŒ–\n",
    "        self.redis_config = redis_config\n",
    "        self.redis = redis.Redis(** redis_config)\n",
    "        self._test_connection()\n",
    "        \n",
    "        # 3. å…ƒä¿¡æ¯å­˜å‚¨åœ¨Redisä¸­\n",
    "        self.redis_metadata_key = \"task_metadata\"\n",
    "        self.total_tasks = self.redis.hlen(self.redis_metadata_key)\n",
    "        print(f\"âœ… ä»Redisè·å–ä»»åŠ¡å…ƒä¿¡æ¯ï¼Œå…±{self.total_tasks}ä¸ªä»»åŠ¡\")\n",
    "        \n",
    "        # 4. ç»Ÿè®¡ä¸çº¿ç¨‹å®‰å…¨é…ç½®\n",
    "        self.processed_count = 0\n",
    "        self.success_count = 0\n",
    "        self.failed_count = 0\n",
    "        self.lock = threading.Lock()  # çº¿ç¨‹é”ï¼ˆç¡®ä¿ç»Ÿè®¡è®¡æ•°å®‰å…¨ï¼‰\n",
    "        \n",
    "        # 5. åˆå§‹åŒ–çº¿ç¨‹æ± æ›¿ä»£æ‰‹åŠ¨ç®¡ç†çº¿ç¨‹\n",
    "        self.thread_pool = ThreadPoolExecutor(max_workers=MAX_CONVERT_THREADS)\n",
    "        \n",
    "        # 6. å¤±è´¥ä»»åŠ¡è®°å½•\n",
    "        self.failed_tasks = []\n",
    "        self.failed_tasks_lock = threading.Lock()\n",
    "\n",
    "    def _test_connection(self):\n",
    "        try:\n",
    "            self.redis.ping()\n",
    "            print(\"âœ… Redisè¿æ¥æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Redisè¿æ¥å¤±è´¥ï¼š{e}\")\n",
    "            raise SystemExit(1)\n",
    "\n",
    "    def _create_temp_file(self, task_id, csv_str):\n",
    "        \"\"\"å°†CSVå­—ç¬¦ä¸²å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œè¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„\"\"\"\n",
    "        try:\n",
    "            # ç”Ÿæˆå”¯ä¸€ä¸´æ—¶æ–‡ä»¶åï¼ˆé¿å…å†²çªï¼šä»»åŠ¡ID+æ—¶é—´æˆ³ï¼‰\n",
    "            temp_filename = f\"temp_{task_id}_{datetime.now().strftime('%H%M%S%f')}.csv\"\n",
    "            temp_file_path = os.path.join(TEMP_DIR, temp_filename)\n",
    "            \n",
    "            # å†™å…¥CSVæ•°æ®ï¼ˆUTF-8ç¼–ç ï¼Œé¿å…ä¸­æ–‡ä¹±ç ï¼‰\n",
    "            with open(temp_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(csv_str)\n",
    "            \n",
    "            return temp_file_path  # è¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä»»åŠ¡ {task_id} åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼š{str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _append_to_parquet(self, task_id, temp_file_path, trade_date, stock_code):\n",
    "        \"\"\"ä»ä¸´æ—¶æ–‡ä»¶è¯»å–CSVï¼Œè¿½åŠ åˆ°å¯¹åº”åˆ†åŒºçš„Parquet\"\"\"\n",
    "        try:\n",
    "            # 1. ä»ä¸´æ—¶æ–‡ä»¶è¯»å–CSVï¼ˆå®¹é”™å¤„ç†ï¼‰\n",
    "            df = pd.read_csv(\n",
    "                temp_file_path,\n",
    "                encoding='utf-8',\n",
    "                sep=',',\n",
    "                on_bad_lines='skip',  # è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ\n",
    "                dtype={\n",
    "                    \"date\": \"str\",\n",
    "                    \"stock_code\": \"str\",\n",
    "                    \"time\": \"str\",\n",
    "                    \"open\": \"float64\",\n",
    "                    \"close\": \"float64\",\n",
    "                    \"high\": \"float64\",\n",
    "                    \"low\": \"float64\",\n",
    "                    \"volume\": \"int64\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # 2. æ•°æ®æ ¡éªŒï¼ˆç¡®ä¿å­—æ®µå®Œæ•´ï¼‰\n",
    "            required_cols = [\"date\", \"stock_code\", \"time\", \"open\", \"close\", \"high\", \"low\", \"volume\"]\n",
    "            if not all(col in df.columns for col in required_cols):\n",
    "                missing = [col for col in required_cols if col not in df.columns]\n",
    "                raise ValueError(f\"ç¼ºå°‘å¿…è¦å­—æ®µï¼š{missing}\")\n",
    "            \n",
    "            # 3. åˆ›å»ºParquetåˆ†åŒºç›®å½•\n",
    "            # å…³é”®ä¿®æ”¹ï¼šå°† stock= æ”¹ä¸º stock_code=ï¼Œä¿æŒä¸æ•°æ®å­—æ®µä¸€è‡´\n",
    "            partition_dir = os.path.join(STORAGE_ROOT, f\"date={trade_date}\", f\"stock_code={stock_code}\")\n",
    "            os.makedirs(partition_dir, exist_ok=True)\n",
    "            parquet_path = os.path.join(partition_dir, \"data.parquet\")\n",
    "            \n",
    "            # 4. è¿½åŠ åˆ°Parquetï¼ˆç¡®ä¿ä¸æ·»åŠ é¢å¤–å­—æ®µï¼‰\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            \n",
    "            # æ£€æŸ¥å¹¶åˆ é™¤å¯èƒ½å­˜åœ¨çš„å¤šä½™å­—æ®µ\n",
    "            if 'stock' in table.column_names:\n",
    "                table = table.drop(['stock'])\n",
    "            \n",
    "            if os.path.exists(parquet_path):\n",
    "                existing_table = pq.read_table(parquet_path)\n",
    "                # æ¸…ç†ç°æœ‰æ–‡ä»¶ä¸­å¯èƒ½çš„å¤šä½™å­—æ®µ\n",
    "                if 'stock' in existing_table.column_names:\n",
    "                    existing_table = existing_table.drop(['stock'])\n",
    "                combined_table = pa.concat_tables([existing_table, table])\n",
    "                pq.write_table(combined_table, parquet_path, compression=\"snappy\")\n",
    "            else:\n",
    "                pq.write_table(table, parquet_path, compression=\"snappy\")\n",
    "            \n",
    "            print(f\"âœ… ä»»åŠ¡ {task_id}ï¼šæˆåŠŸè¿½åŠ åˆ° {parquet_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä»»åŠ¡ {task_id} è¿½åŠ Parquetå¤±è´¥ï¼š{str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _delete_temp_file(self, task_id, temp_file_path):\n",
    "        \"\"\"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼ˆç¡®ä¿æ•°æ®å·²æˆåŠŸè¿½åŠ åè°ƒç”¨ï¼‰\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.remove(temp_file_path)\n",
    "                print(f\"â„¹ï¸  ä»»åŠ¡ {task_id}ï¼šå·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {os.path.basename(temp_file_path)}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  ä»»åŠ¡ {task_id} åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼š{str(e)}ï¼ˆéœ€æ‰‹åŠ¨æ¸…ç†ï¼‰\")\n",
    "            return False\n",
    "\n",
    "    def _process_task(self, task_id, csv_str):\n",
    "        \"\"\"å¤„ç†å•ä¸ªä»»åŠ¡çš„å‡½æ•°ï¼Œä¾›çº¿ç¨‹æ± è°ƒç”¨\"\"\"\n",
    "        temp_file_path = None\n",
    "        \n",
    "        try:\n",
    "            # 1. åˆ›å»ºä¸´æ—¶æ–‡ä»¶\n",
    "            temp_file_path = self._create_temp_file(task_id, csv_str)\n",
    "            if not temp_file_path:\n",
    "                raise Exception(\"åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥\")\n",
    "            \n",
    "            # 2. ä»Redisè·å–ä»»åŠ¡å…ƒä¿¡æ¯\n",
    "            metadata_value = self.redis.hget(self.redis_metadata_key, task_id)\n",
    "            if not metadata_value:\n",
    "                raise KeyError(f\"ä»»åŠ¡{task_id}çš„å…ƒä¿¡æ¯åœ¨Redisä¸­ä¸å­˜åœ¨\")\n",
    "            trade_date, stock_code = pickle.loads(metadata_value)\n",
    "            \n",
    "            # 3. è¿½åŠ åˆ°Parquet\n",
    "            append_success = self._append_to_parquet(task_id, temp_file_path, trade_date, stock_code)\n",
    "            if not append_success:\n",
    "                raise Exception(\"è¿½åŠ Parquetå¤±è´¥\")\n",
    "            \n",
    "            # 4. åˆ é™¤ä¸´æ—¶æ–‡ä»¶å’ŒRedisä¸­çš„å…ƒä¿¡æ¯\n",
    "            self._delete_temp_file(task_id, temp_file_path)\n",
    "            self.redis.hdel(self.redis_metadata_key, task_id)\n",
    "            \n",
    "            # 5. æ›´æ–°æˆåŠŸè®¡æ•°\n",
    "            with self.lock:\n",
    "                self.success_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            # è®°å½•å¤±è´¥ä¿¡æ¯\n",
    "            with self.failed_tasks_lock:\n",
    "                self.failed_tasks.append({\n",
    "                    \"task_id\": task_id,\n",
    "                    \"error\": str(e),\n",
    "                    \"traceback\": traceback.format_exc()[:1000]\n",
    "                })\n",
    "            \n",
    "            # æ›´æ–°å¤±è´¥è®¡æ•°\n",
    "            with self.lock:\n",
    "                self.failed_count += 1\n",
    "            \n",
    "            print(f\"âŒ ä»»åŠ¡ {task_id} å¤„ç†å¤±è´¥ï¼š{str(e)}\")\n",
    "            \n",
    "            # å¤±è´¥æ—¶ä¹Ÿå°è¯•åˆ é™¤ä¸´æ—¶æ–‡ä»¶\n",
    "            if temp_file_path:\n",
    "                self._delete_temp_file(task_id, temp_file_path)\n",
    "        \n",
    "        finally:\n",
    "            # æ›´æ–°å·²å¤„ç†è®¡æ•°\n",
    "            with self.lock:\n",
    "                self.processed_count += 1\n",
    "                # æ¯100ä¸ªä»»åŠ¡æ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    "                if self.processed_count % 100 == 0:\n",
    "                    if self.total_tasks > 0:\n",
    "                        progress = (self.processed_count / self.total_tasks) * 100\n",
    "                        print(f\"ğŸ“Š å¤„ç†ä¸­ï¼šè¿›åº¦ {progress:.2f}%ï¼ŒæˆåŠŸ{self.success_count}/å¤±è´¥{self.failed_count}/æ€»è®¡{self.processed_count}\")\n",
    "                    else:\n",
    "                        print(f\"ğŸ“Š å¤„ç†ä¸­ï¼šæˆåŠŸ{self.success_count}/å¤±è´¥{self.failed_count}/æ€»è®¡{self.processed_count}\")\n",
    "\n",
    "    def receive_and_process_results(self):\n",
    "        \"\"\"æ¥æ”¶Redisç»“æœï¼Œæäº¤åˆ°çº¿ç¨‹æ± å¤„ç†\"\"\"\n",
    "        print(f\"âœ… å¼€å§‹æ¥æ”¶ç»“æœï¼Œæ€»ä»»åŠ¡æ•°ï¼š{self.total_tasks}\")\n",
    "        \n",
    "        # å¦‚æœæ€»ä»»åŠ¡æ•°ä¸º0ï¼Œç›´æ¥ç­‰å¾…å¹¶é€€å‡º\n",
    "        if self.total_tasks == 0:\n",
    "            print(\"â„¹ï¸  æ²¡æœ‰ä»»åŠ¡éœ€è¦å¤„ç†ï¼Œå°†ç­‰å¾…10ç§’åé€€å‡º\")\n",
    "            time.sleep(10)\n",
    "            print(\"âœ… é€€å‡ºç¨‹åº\")\n",
    "            return\n",
    "            \n",
    "        last_report_time = time.time()\n",
    "        \n",
    "        # ç”¨äºè·Ÿè¸ªçº¿ç¨‹æ± ä¸­çš„ä»»åŠ¡\n",
    "        futures = []\n",
    "        \n",
    "        while True:\n",
    "            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡å·²å¤„ç†å®Œæˆ\n",
    "            with self.lock:\n",
    "                if self.processed_count >= self.total_tasks:\n",
    "                    break\n",
    "            \n",
    "            # ä»Redisè·å–ç»“æœï¼ˆæ”¯æŒé‡è¿ï¼‰\n",
    "            try:\n",
    "                result_data = self.redis.blpop(RESULT_QUEUE, timeout=60)\n",
    "            except redis.exceptions.ConnectionError:\n",
    "                print(f\"âš ï¸ Redisè¿æ¥æ–­å¼€ï¼Œå°è¯•é‡è¿...\")\n",
    "                self.redis = redis.Redis(** self.redis_config)\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è·å–Redisç»“æœå¤±è´¥ï¼š{str(e)}\")\n",
    "                time.sleep(3)\n",
    "                continue\n",
    "            \n",
    "            if not result_data:\n",
    "                # å®šæœŸæ‰“å°è¿›åº¦ï¼ˆæ¯30ç§’ä¸€æ¬¡ï¼‰\n",
    "                with self.lock:\n",
    "                    if self.total_tasks > 0:\n",
    "                        progress = (self.processed_count / self.total_tasks) * 100\n",
    "                        print(f\"â³ å¤„ç†ä¸­ï¼šè¿›åº¦ {progress:.2f}%ï¼ŒæˆåŠŸ{self.success_count}/å¤±è´¥{self.failed_count}/æ€»è®¡{self.processed_count}\")\n",
    "                    else:\n",
    "                        print(f\"â³ å¤„ç†ä¸­ï¼šæˆåŠŸ{self.success_count}/å¤±è´¥{self.failed_count}/æ€»è®¡{self.processed_count}\")\n",
    "                last_report_time = time.time()\n",
    "                continue\n",
    "            \n",
    "            # è§£æRedisç»“æœï¼Œæäº¤åˆ°çº¿ç¨‹æ± å¤„ç†\n",
    "            _, result_bytes = result_data\n",
    "            try:\n",
    "                result = pickle.loads(result_bytes)\n",
    "                task_id = result.get(\"task_id\", \"æœªçŸ¥\")\n",
    "                csv_str = result.get(\"result\", \"\")\n",
    "                \n",
    "                if result[\"status\"] == \"success\" and csv_str.strip():\n",
    "                    # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± å¤„ç†\n",
    "                    future = self.thread_pool.submit(self._process_task, task_id, csv_str)\n",
    "                    futures.append(future)\n",
    "                    print(f\"â„¹ï¸  ä»»åŠ¡ {task_id}ï¼šå·²æäº¤åˆ°çº¿ç¨‹æ± å¤„ç†\")\n",
    "                else:\n",
    "                    # è¿œç«¯ç›´æ¥è¿”å›å¤±è´¥ï¼Œè®°å½•é”™è¯¯\n",
    "                    with self.failed_tasks_lock:\n",
    "                        self.failed_tasks.append({\n",
    "                            \"task_id\": task_id,\n",
    "                            \"error\": result.get(\"error\", \"è¿œç«¯æ‰§è¡Œå¤±è´¥\")\n",
    "                        })\n",
    "                    \n",
    "                    with self.lock:\n",
    "                        self.failed_count += 1\n",
    "                        self.processed_count += 1\n",
    "                    \n",
    "                    print(f\"âŒ ä»»åŠ¡ {task_id}ï¼šè¿œç«¯æ‰§è¡Œå¤±è´¥ â†’ {result.get('error', 'æœªçŸ¥åŸå› ')}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                # ç»“æœè§£æå¤±è´¥\n",
    "                with self.failed_tasks_lock:\n",
    "                    self.failed_tasks.append({\n",
    "                        \"task_id\": \"æœªçŸ¥\",\n",
    "                        \"error\": f\"ç»“æœè§£æå¤±è´¥ï¼š{str(e)}\",\n",
    "                        \"traceback\": traceback.format_exc()[:500]\n",
    "                    })\n",
    "                \n",
    "                with self.lock:\n",
    "                    self.processed_count += 1\n",
    "                    self.failed_count += 1\n",
    "                \n",
    "                print(f\"âŒ ç»“æœè§£æå¤±è´¥ï¼š{str(e)}\")\n",
    "        \n",
    "        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹æ± ä»»åŠ¡å®Œæˆ\n",
    "        print(\"âœ… æ‰€æœ‰ä»»åŠ¡å·²æ¥æ”¶ï¼Œç­‰å¾…çº¿ç¨‹æ± å¤„ç†å®Œæˆ...\")\n",
    "        for future in futures:\n",
    "            future.result()  # ç­‰å¾…ä»»åŠ¡å®Œæˆ\n",
    "        \n",
    "        # å…³é—­çº¿ç¨‹æ± \n",
    "        self.thread_pool.shutdown()\n",
    "        \n",
    "        # ä¿å­˜å¤±è´¥ä»»åŠ¡åˆ—è¡¨\n",
    "        if self.failed_tasks:\n",
    "            failed_path = f\"failed_tasks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "            with open(failed_path, \"wb\") as f:\n",
    "                pickle.dump(self.failed_tasks, f)\n",
    "            print(f\"âš ï¸  å·²ä¿å­˜ {len(self.failed_tasks)} ä¸ªå¤±è´¥ä»»åŠ¡åˆ° {failed_path}\")\n",
    "        \n",
    "        # æœ€ç»ˆç»Ÿè®¡\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"å¤„ç†å®Œæˆï¼šæ€»ä»»åŠ¡æ•° {self.total_tasks}\")\n",
    "        if self.total_tasks > 0:\n",
    "            print(f\"æˆåŠŸï¼š{self.success_count} ({self.success_count/self.total_tasks*100:.2f}%)\")\n",
    "            print(f\"å¤±è´¥ï¼š{self.failed_count} ({self.failed_count/self.total_tasks*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"æˆåŠŸï¼š{self.success_count}\")\n",
    "            print(f\"å¤±è´¥ï¼š{self.failed_count}\")\n",
    "        print(f\"ä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼š{TEMP_DIR}ï¼ˆæ®‹ç•™æ–‡ä»¶éœ€æ‰‹åŠ¨æ¸…ç†ï¼‰\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# ---------------------- ä¸»å‡½æ•° ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 1. åŠ è½½Redisé…ç½®\n",
    "        redis_config = load_redis_config(REDIS_CONFIG_PATH)\n",
    "        print(f\"âœ… å·²åŠ è½½Redisé…ç½®ï¼šhost={redis_config['host']}, port={redis_config['port']}\")\n",
    "        \n",
    "        # 2. åˆå§‹åŒ–å¤„ç†å™¨å¹¶å¯åŠ¨\n",
    "        processor = ResultProcessor(redis_config)\n",
    "        processor.receive_and_process_results()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥ï¼š{str(e)}\")\n",
    "        raise SystemExit(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
