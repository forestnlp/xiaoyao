{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12369b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "\n",
    "class MinuteDataProcessor:\n",
    "    def __init__(self, config_path: str = \"redis.conf\"):\n",
    "        \"\"\"初始化处理器，匹配正式数据分区结构\"\"\"\n",
    "        self.redis_config = self._load_redis_config(config_path)\n",
    "        self.redis = redis.Redis(\n",
    "            host=self.redis_config[\"host\"],\n",
    "            port=self.redis_config[\"port\"],\n",
    "            password=self.redis_config[\"password\"],\n",
    "            decode_responses=False\n",
    "        )\n",
    "        self.result_queue = \"function_results\"\n",
    "        self.task_metadata = \"task_metadata\"\n",
    "        # 正式数据分区根目录\n",
    "        self.formal_data_root = r\"D:\\workspace\\xiaoyao\\data\\stock_minutely_price\"\n",
    "        # 临时CSV目录（当前目录下）\n",
    "        self.temp_download_dir = os.path.join(os.getcwd(), \"temp_minute_downloads\")  \n",
    "        self.idle_timeout = 1800  # 30分钟无任务退出\n",
    "        self._test_redis_connection()\n",
    "        self._init_storage()\n",
    "\n",
    "    def _load_redis_config(self, config_path: str) -> Dict[str, str]:\n",
    "        \"\"\"加载Redis配置\"\"\"\n",
    "        config = {\"host\": \"localhost\", \"port\": 6379, \"password\": \"\"}\n",
    "        try:\n",
    "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line.startswith(\"host=\"):\n",
    "                        config[\"host\"] = line.split(\"=\", 1)[1].strip()\n",
    "                    elif line.startswith(\"port=\"):\n",
    "                        config[\"port\"] = int(line.split(\"=\", 1)[1].strip())\n",
    "                    elif line.startswith(\"password=\"):\n",
    "                        config[\"password\"] = line.split(\"=\", 1)[1].strip()\n",
    "            return config\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 配置文件读取失败，使用默认配置: {e}\")\n",
    "            return config\n",
    "\n",
    "    def _test_redis_connection(self):\n",
    "        \"\"\"测试Redis连接\"\"\"\n",
    "        try:\n",
    "            self.redis.ping()\n",
    "            print(f\"✅ 处理器Redis连接成功 | {self.redis_config['host']}:{self.redis_config['port']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 处理器Redis连接失败: {e}\")\n",
    "            raise SystemExit(1)\n",
    "\n",
    "    def _init_storage(self):\n",
    "        \"\"\"初始化正式目录和临时目录\"\"\"\n",
    "        os.makedirs(self.formal_data_root, exist_ok=True)\n",
    "        os.makedirs(self.temp_download_dir, exist_ok=True)\n",
    "        print(f\"✅ 正式数据分区根目录: {self.formal_data_root}\")\n",
    "        print(f\"✅ 临时CSV目录: {self.temp_download_dir}\")\n",
    "\n",
    "    # 第一阶段：下载CSV并保存到临时目录\n",
    "    def _stage1_download_to_temp(self, csv_str: str, task_id: str) -> Tuple[Optional[pd.DataFrame], Optional[str]]:\n",
    "        \"\"\"返回DataFrame和临时文件路径，便于后续删除\"\"\"\n",
    "        if not csv_str.strip():\n",
    "            print(f\"⚠️ 任务{task_id}返回空数据，跳过\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            # 读取CSV数据\n",
    "            df = pd.read_csv(StringIO(csv_str))\n",
    "            # 校验核心字段（匹配正式数据结构）\n",
    "            required_cols = ['date', 'stock_code', 'time', 'open', 'close', 'high', 'low', 'volume']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"缺少核心字段: {missing_cols}\")\n",
    "\n",
    "            # 数据类型统一（空值保留，不填充0）\n",
    "            numeric_cols = ['open', 'close', 'high', 'low', 'volume']\n",
    "            for col in numeric_cols:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')  # 空值保留为NaN\n",
    "            # 统一时间格式\n",
    "            df['time'] = pd.to_datetime(df['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # 保存到临时目录\n",
    "            temp_file_path = os.path.join(self.temp_download_dir, f\"{task_id}_raw.csv\")\n",
    "            df.to_csv(temp_file_path, index=False, encoding='utf-8')\n",
    "            print(f\"📥 任务{task_id}临时CSV已保存: {temp_file_path}\")\n",
    "            return df, temp_file_path\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 任务{task_id}CSV处理失败: {str(e)[:100]}\")\n",
    "            return None, None\n",
    "\n",
    "    # 第二阶段：验证临时数据质量（彻底修复null_cols引用错误）\n",
    "    def _stage2_verify_temp_data(self, df: pd.DataFrame, task_id: str) -> bool:\n",
    "        \"\"\"验证数据完整性，修复null_cols定义逻辑\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n📊 任务{task_id}数据校验:\")\n",
    "            print(f\"总记录数: {len(df)} | 涉及股票数: {df['stock_code'].nunique()}\")\n",
    "            \n",
    "            # 1. 检查单股票单日记录数\n",
    "            stock_records = df.groupby('stock_code').size()\n",
    "            abnormal_stocks = stock_records[stock_records != 240].index.tolist()\n",
    "            if abnormal_stocks:\n",
    "                print(f\"⚠️ 异常股票（记录数≠240）: {abnormal_stocks[:5]}（共{len(abnormal_stocks)}只）\")\n",
    "            else:\n",
    "                print(\"✅ 所有股票记录数正常（单股票单日240条）\")\n",
    "            \n",
    "            # 2. 检查空值（核心修复：基于null_summary计算null_cols，而非引用自身）\n",
    "            null_summary = df.isnull().sum()  # 先计算每列空值数\n",
    "            # 筛选空值数>0的列，生成null_cols（修复前误写为null_summary[null_cols > 0]）\n",
    "            null_cols = null_summary[null_summary > 0].index.tolist()  \n",
    "            if null_cols:\n",
    "                # 格式化空值统计（转为整数，避免科学计数法）\n",
    "                null_stats = {col: int(null_summary[col]) for col in null_cols}\n",
    "                print(f\"ℹ️ 存在空值的字段: {null_cols} | 空值统计: {null_stats}\")\n",
    "            else:\n",
    "                print(\"✅ 无空值数据\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 任务{task_id}数据校验失败: {str(e)[:100]}\")\n",
    "            return False\n",
    "\n",
    "    # 第三阶段：合并到正式分区Parquet + 删除临时CSV\n",
    "    def _stage3_merge_and_clean(self, df: pd.DataFrame, task_id: str, temp_file_path: str) -> bool:\n",
    "        \"\"\"合并数据到正式Parquet，完成后删除临时CSV\"\"\"\n",
    "        try:\n",
    "            # 按股票代码分组，逐个合并\n",
    "            for stock_code in df['stock_code'].unique():\n",
    "                stock_df = df[df['stock_code'] == stock_code].copy()\n",
    "                # 构建正式分区路径（匹配 \"stock_code=XXX.XSHE\" 结构）\n",
    "                stock_dir = os.path.join(self.formal_data_root, f\"stock_code={stock_code}\")\n",
    "                formal_parquet_path = os.path.join(stock_dir, \"data.parquet\")\n",
    "\n",
    "                # 创建股票目录（不存在则新建）\n",
    "                os.makedirs(stock_dir, exist_ok=True)\n",
    "                print(f\"\\n🔄 处理股票: {stock_code} | 正式文件: {formal_parquet_path}\")\n",
    "\n",
    "                # 合并数据（保留空值）\n",
    "                if os.path.exists(formal_parquet_path):\n",
    "                    # 读取已有数据并去重\n",
    "                    existing_df = pd.read_parquet(formal_parquet_path)\n",
    "                    combined_df = pd.concat([existing_df, stock_df], ignore_index=True)\n",
    "                    # 按唯一键去重，避免重复记录\n",
    "                    combined_df = combined_df.drop_duplicates(subset=['date', 'time', 'stock_code'])\n",
    "                    # 写入合并后的数据\n",
    "                    table = pa.Table.from_pandas(combined_df)\n",
    "                    pq.write_table(table, formal_parquet_path, compression=\"snappy\")\n",
    "                    print(f\"✅ 已追加数据 | 原记录数: {len(existing_df)} | 新记录数: {len(stock_df)} | 合并后: {len(combined_df)}\")\n",
    "                else:\n",
    "                    # 新建Parquet文件\n",
    "                    table = pa.Table.from_pandas(stock_df)\n",
    "                    pq.write_table(table, formal_parquet_path, compression=\"snappy\")\n",
    "                    print(f\"✅ 新建Parquet文件 | 初始记录数: {len(stock_df)}\")\n",
    "\n",
    "            # 合并完成，删除临时CSV\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.remove(temp_file_path)\n",
    "                print(f\"\\n🗑️ 任务{task_id}临时CSV已删除: {temp_file_path}\")\n",
    "            else:\n",
    "                print(f\"\\n⚠️ 任务{task_id}临时CSV不存在，无需删除\")\n",
    "\n",
    "            print(f\"🎉 任务{task_id}所有股票已合并到正式分区\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 任务{task_id}合并/清理失败: {str(e)[:100]}\")\n",
    "            # 合并失败时保留临时文件，便于排查\n",
    "            print(f\"⚠️ 合并失败，临时CSV已保留: {temp_file_path}\")\n",
    "            return False\n",
    "\n",
    "    # 整合完整处理流程\n",
    "    def _process_full_flow(self, csv_str: str, task_id: str) -> bool:\n",
    "        # 阶段1：下载到临时目录\n",
    "        temp_df, temp_file = self._stage1_download_to_temp(csv_str, task_id)\n",
    "        if temp_df is None or temp_file is None:\n",
    "            return False\n",
    "        # 阶段2：数据质量验证（已修复null_cols错误）\n",
    "        if not self._stage2_verify_temp_data(temp_df, task_id):\n",
    "            return False\n",
    "        # 阶段3：合并到正式Parquet + 清理临时文件\n",
    "        if not self._stage3_merge_and_clean(temp_df, task_id, temp_file):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # 监听队列并执行处理（优化blpop解包逻辑）\n",
    "    def listen_and_process(self):\n",
    "        print(f\"✅ 开始监听结果队列（{self.idle_timeout}秒无任务退出）\")\n",
    "        stats = {\"success\": 0, \"failed\": 0, \"last_active\": time.time()}\n",
    "\n",
    "        while True:\n",
    "            # 超时退出逻辑\n",
    "            if time.time() - stats[\"last_active\"] > self.idle_timeout:\n",
    "                print(\"\\n⏰ 长时间无新任务，退出处理器\")\n",
    "                # 清理残留临时文件\n",
    "                self._clean_residual_temp_files()\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                # 优化：先接收blpop结果，避免直接解包None\n",
    "                queue_result = self.redis.blpop(self.result_queue, timeout=30)\n",
    "                if queue_result is None:\n",
    "                    continue  # 无数据，继续等待\n",
    "                _, result_bytes = queue_result  # 有数据时再解包\n",
    "\n",
    "                # 更新活动时间\n",
    "                stats[\"last_active\"] = time.time()\n",
    "                # 反序列化结果\n",
    "                result = pickle.loads(result_bytes)\n",
    "                task_id = result.get(\"task_id\", \"未知任务\")\n",
    "\n",
    "                # 处理成功的任务\n",
    "                if result.get(\"status\") == \"success\":\n",
    "                    csv_data = result.get(\"result\", \"\")\n",
    "                    if self._process_full_flow(csv_data, task_id):\n",
    "                        stats[\"success\"] += 1\n",
    "                        # 清理Redis任务元信息\n",
    "                        self.redis.hdel(self.task_metadata, task_id)\n",
    "                        print(f\"\\n🏆 任务{task_id[:8]}...处理完成 | 累计成功: {stats['success']}\")\n",
    "                    else:\n",
    "                        stats[\"failed\"] += 1\n",
    "                        print(f\"❌ 任务{task_id[:8]}...处理失败 | 累计失败: {stats['failed']}\")\n",
    "                else:\n",
    "                    # 处理远程执行失败的任务\n",
    "                    stats[\"failed\"] += 1\n",
    "                    error_msg = result.get(\"error\", \"无错误信息\")\n",
    "                    print(f\"❌ 任务{task_id[:8]}...远程失败: {error_msg} | 累计失败: {stats['failed']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 处理器异常: {str(e)[:80]}，等待10秒重试\")\n",
    "                time.sleep(10)\n",
    "\n",
    "        # 输出最终统计\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"处理结果汇总\")\n",
    "        print(f\"总任务数: {stats['success'] + stats['failed']} | 成功: {stats['success']} | 失败: {stats['failed']}\")\n",
    "        if stats[\"success\"] + stats[\"failed\"] > 0:\n",
    "            print(f\"成功率: {stats['success']/(stats['success']+stats['failed'])*100:.1f}%\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def _clean_residual_temp_files(self):\n",
    "        \"\"\"清理临时目录中残留的CSV文件\"\"\"\n",
    "        residual_files = [f for f in os.listdir(self.temp_download_dir) if f.endswith(\"_raw.csv\")]\n",
    "        if not residual_files:\n",
    "            print(f\"✅ 临时目录无残留文件: {self.temp_download_dir}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n🗑️ 开始清理临时目录残留文件（共{len(residual_files)}个）\")\n",
    "        for file_name in residual_files:\n",
    "            file_path = os.path.join(self.temp_download_dir, file_name)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"✅ 删除残留文件: {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 删除残留文件{file_name}失败: {str(e)[:50]}\")\n",
    "        print(f\"✅ 临时目录清理完成: {self.temp_download_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 初始化并启动处理器\n",
    "        processor = MinuteDataProcessor(config_path=\"redis.conf\")\n",
    "        processor.listen_and_process()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 处理器启动失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2539bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import redis\n",
    "# import pickle\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# from io import StringIO\n",
    "# from datetime import datetime\n",
    "# from typing import Dict, Optional\n",
    "\n",
    "# class MinuteDataProcessor:\n",
    "#     def __init__(self, config_path: str = \"redis.conf\"):\n",
    "#         \"\"\"初始化处理器，与发布器保持相同的Redis配置逻辑\"\"\"\n",
    "#         self.redis_config = self._load_redis_config(config_path)\n",
    "#         self.redis = redis.Redis(\n",
    "#             host=self.redis_config[\"host\"],\n",
    "#             port=self.redis_config[\"port\"],\n",
    "#             password=self.redis_config[\"password\"],\n",
    "#             decode_responses=False\n",
    "#         )\n",
    "#         self.result_queue = \"function_results\"  # 与发布器对应\n",
    "#         self.task_metadata = \"task_metadata\"     # 与发布器存储元信息的键一致\n",
    "#         self.storage_root = r\"D:\\workspace\\xiaoyao\\data\\stock_minutely_price\"\n",
    "#         self.idle_timeout = 1800  # 30分钟无任务退出\n",
    "#         self._test_redis_connection()\n",
    "#         self._init_storage()\n",
    "\n",
    "#     def _load_redis_config(self, config_path: str) -> Dict[str, str]:\n",
    "#         \"\"\"复用发布器的Redis配置加载逻辑，确保一致\"\"\"\n",
    "#         config = {\"host\": \"localhost\", \"port\": 6379, \"password\": \"\"}\n",
    "#         try:\n",
    "#             with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 for line in f:\n",
    "#                     line = line.strip()\n",
    "#                     if line.startswith(\"host=\"):\n",
    "#                         config[\"host\"] = line.split(\"=\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"port=\"):\n",
    "#                         config[\"port\"] = int(line.split(\"=\", 1)[1].strip())\n",
    "#                     elif line.startswith(\"password=\"):\n",
    "#                         config[\"password\"] = line.split(\"=\", 1)[1].strip()\n",
    "#             return config\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️ 配置文件读取失败，使用默认配置: {e}\")\n",
    "#             return config\n",
    "\n",
    "#     def _test_redis_connection(self):\n",
    "#         \"\"\"测试Redis连接，与发布器逻辑一致\"\"\"\n",
    "#         try:\n",
    "#             self.redis.ping()\n",
    "#             print(f\"✅ 处理器Redis连接成功 | {self.redis_config['host']}:{self.redis_config['port']}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ 处理器Redis连接失败: {e}\")\n",
    "#             raise SystemExit(1)\n",
    "\n",
    "#     def _init_storage(self):\n",
    "#         \"\"\"初始化存储目录\"\"\"\n",
    "#         os.makedirs(self.storage_root, exist_ok=True)\n",
    "#         print(f\"✅ 数据存储目录: {self.storage_root}\")\n",
    "\n",
    "#     def _process_csv_data(self, csv_str: str, task_id: str) -> bool:\n",
    "#         \"\"\"处理CSV数据并按股票存储（仅按股票分区）\"\"\"\n",
    "#         if not csv_str.strip():\n",
    "#             print(f\"⚠️ 任务{task_id}返回空数据，跳过处理\")\n",
    "#             return False\n",
    "\n",
    "#         try:\n",
    "#             # 从CSV字符串读取数据（与发布器的CSV处理逻辑兼容）\n",
    "#             df = pd.read_csv(StringIO(csv_str))\n",
    "            \n",
    "#             # 必要字段校验\n",
    "#             required_cols = ['date', 'stock_code', 'time', 'open', 'close', 'high', 'low', 'volume']\n",
    "#             missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "#             if missing_cols:\n",
    "#                 raise ValueError(f\"缺少必要字段: {missing_cols}\")\n",
    "\n",
    "#             # 按股票代码存储（仅股票分区，无日期分区）\n",
    "#             for stock_code in df['stock_code'].unique():\n",
    "#                 stock_data = df[df['stock_code'] == stock_code].copy()\n",
    "#                 stock_dir = os.path.join(self.storage_root, f\"stock_code={stock_code}\")\n",
    "#                 os.makedirs(stock_dir, exist_ok=True)\n",
    "#                 parquet_path = os.path.join(stock_dir, \"data.parquet\")\n",
    "\n",
    "#                 # 转换为Arrow表并追加/创建文件\n",
    "#                 table = pa.Table.from_pandas(stock_data)\n",
    "#                 if os.path.exists(parquet_path):\n",
    "#                     existing_table = pq.read_table(parquet_path)\n",
    "#                     combined_table = pa.concat_tables([existing_table, table])\n",
    "#                     pq.write_table(combined_table, parquet_path, compression=\"snappy\")\n",
    "#                 else:\n",
    "#                     pq.write_table(table, parquet_path, compression=\"snappy\")\n",
    "\n",
    "#             return True\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ 任务{task_id}数据处理失败: {str(e)[:100]}\")\n",
    "#             return False\n",
    "\n",
    "#     def listen_and_process(self):\n",
    "#         \"\"\"监听结果队列并处理数据\"\"\"\n",
    "#         print(f\"✅ 开始监听结果队列（{self.idle_timeout}秒无任务退出）\")\n",
    "#         stats = {\"success\": 0, \"failed\": 0, \"last_active\": time.time()}\n",
    "\n",
    "#         while True:\n",
    "#             # 检查超时退出\n",
    "#             if time.time() - stats[\"last_active\"] > self.idle_timeout:\n",
    "#                 print(\"\\n⏰ 长时间无新任务，退出处理器\")\n",
    "#                 break\n",
    "\n",
    "#             # 从队列获取结果（与发布器的序列化方式匹配）\n",
    "#             try:\n",
    "#                 _, result_bytes = self.redis.blpop(self.result_queue, timeout=30)\n",
    "#                 if not result_bytes:\n",
    "#                     continue  # 无数据，继续等待\n",
    "\n",
    "#                 # 更新活动时间\n",
    "#                 stats[\"last_active\"] = time.time()\n",
    "\n",
    "#                 # 反序列化结果（使用pickle，与发布器一致）\n",
    "#                 result = pickle.loads(result_bytes)\n",
    "#                 task_id = result.get(\"task_id\", \"未知任务\")\n",
    "\n",
    "#                 # 处理结果（与发布器的任务结构对应）\n",
    "#                 if result.get(\"status\") == \"success\":\n",
    "#                     # 处理成功结果\n",
    "#                     csv_data = result.get(\"result\", \"\")\n",
    "#                     if self._process_csv_data(csv_data, task_id):\n",
    "#                         stats[\"success\"] += 1\n",
    "#                         # 清理任务元信息（与发布器存储的元信息键对应）\n",
    "#                         self.redis.hdel(self.task_metadata, task_id)\n",
    "#                         print(f\"✅ 任务{task_id[:8]}...处理成功 | 累计成功: {stats['success']}\")\n",
    "#                     else:\n",
    "#                         stats[\"failed\"] += 1\n",
    "#                         print(f\"❌ 任务{task_id[:8]}...数据处理失败 | 累计失败: {stats['failed']}\")\n",
    "#                 else:\n",
    "#                     # 处理远程执行失败的任务\n",
    "#                     stats[\"failed\"] += 1\n",
    "#                     error_msg = result.get(\"error\", \"无错误信息\")\n",
    "#                     print(f\"❌ 任务{task_id[:8]}...远程执行失败: {error_msg} | 累计失败: {stats['failed']}\")\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"⚠️ 处理器异常: {str(e)[:80]}，等待10秒重试\")\n",
    "#                 time.sleep(10)\n",
    "\n",
    "#         # 输出最终统计\n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "#         print(\"结果处理总结\")\n",
    "#         print(f\"总处理任务数: {stats['success'] + stats['failed']}\")\n",
    "#         print(f\"成功: {stats['success']} | 失败: {stats['failed']}\")\n",
    "#         if stats[\"success\"] + stats[\"failed\"] > 0:\n",
    "#             print(f\"成功率: {stats['success']/(stats['success']+stats['failed'])*100:.1f}%\")\n",
    "#         print(\"=\"*50)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         processor = MinuteDataProcessor(config_path=\"redis.conf\")\n",
    "#         processor.listen_and_process()\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ 处理器执行失败: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d0819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
