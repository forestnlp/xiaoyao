{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12369b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "\n",
    "class MinuteDataProcessor:\n",
    "    def __init__(self, config_path: str = \"redis.conf\"):\n",
    "        \"\"\"ÂàùÂßãÂåñÂ§ÑÁêÜÂô®ÔºåÂåπÈÖçÊ≠£ÂºèÊï∞ÊçÆÂàÜÂå∫ÁªìÊûÑ\"\"\"\n",
    "        self.redis_config = self._load_redis_config(config_path)\n",
    "        self.redis = redis.Redis(\n",
    "            host=self.redis_config[\"host\"],\n",
    "            port=self.redis_config[\"port\"],\n",
    "            password=self.redis_config[\"password\"],\n",
    "            decode_responses=False\n",
    "        )\n",
    "        self.result_queue = \"function_results\"\n",
    "        self.task_metadata = \"task_metadata\"\n",
    "        # Ê≠£ÂºèÊï∞ÊçÆÂàÜÂå∫Ê†πÁõÆÂΩï\n",
    "        self.formal_data_root = r\"D:\\workspace\\xiaoyao\\data\\stock_minutely_price\"\n",
    "        # ‰∏¥Êó∂CSVÁõÆÂΩïÔºàÂΩìÂâçÁõÆÂΩï‰∏ãÔºâ\n",
    "        self.temp_download_dir = os.path.join(os.getcwd(), \"temp_minute_downloads\")  \n",
    "        self.idle_timeout = 1800  # 30ÂàÜÈíüÊó†‰ªªÂä°ÈÄÄÂá∫\n",
    "        self._test_redis_connection()\n",
    "        self._init_storage()\n",
    "\n",
    "    def _load_redis_config(self, config_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Âä†ËΩΩRedisÈÖçÁΩÆ\"\"\"\n",
    "        config = {\"host\": \"localhost\", \"port\": 6379, \"password\": \"\"}\n",
    "        try:\n",
    "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line.startswith(\"host=\"):\n",
    "                        config[\"host\"] = line.split(\"=\", 1)[1].strip()\n",
    "                    elif line.startswith(\"port=\"):\n",
    "                        config[\"port\"] = int(line.split(\"=\", 1)[1].strip())\n",
    "                    elif line.startswith(\"password=\"):\n",
    "                        config[\"password\"] = line.split(\"=\", 1)[1].strip()\n",
    "            return config\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ÈÖçÁΩÆÊñá‰ª∂ËØªÂèñÂ§±Ë¥•Ôºå‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆ: {e}\")\n",
    "            return config\n",
    "\n",
    "    def _test_redis_connection(self):\n",
    "        \"\"\"ÊµãËØïRedisËøûÊé•\"\"\"\n",
    "        try:\n",
    "            self.redis.ping()\n",
    "            print(f\"‚úÖ Â§ÑÁêÜÂô®RedisËøûÊé•ÊàêÂäü | {self.redis_config['host']}:{self.redis_config['port']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Â§ÑÁêÜÂô®RedisËøûÊé•Â§±Ë¥•: {e}\")\n",
    "            raise SystemExit(1)\n",
    "\n",
    "    def _init_storage(self):\n",
    "        \"\"\"ÂàùÂßãÂåñÊ≠£ÂºèÁõÆÂΩïÂíå‰∏¥Êó∂ÁõÆÂΩï\"\"\"\n",
    "        os.makedirs(self.formal_data_root, exist_ok=True)\n",
    "        os.makedirs(self.temp_download_dir, exist_ok=True)\n",
    "        print(f\"‚úÖ Ê≠£ÂºèÊï∞ÊçÆÂàÜÂå∫Ê†πÁõÆÂΩï: {self.formal_data_root}\")\n",
    "        print(f\"‚úÖ ‰∏¥Êó∂CSVÁõÆÂΩï: {self.temp_download_dir}\")\n",
    "\n",
    "    # Á¨¨‰∏ÄÈò∂ÊÆµÔºö‰∏ãËΩΩCSVÂπ∂‰øùÂ≠òÂà∞‰∏¥Êó∂ÁõÆÂΩï\n",
    "    def _stage1_download_to_temp(self, csv_str: str, task_id: str) -> Tuple[Optional[pd.DataFrame], Optional[str]]:\n",
    "        \"\"\"ËøîÂõûDataFrameÂíå‰∏¥Êó∂Êñá‰ª∂Ë∑ØÂæÑÔºå‰æø‰∫éÂêéÁª≠Âà†Èô§\"\"\"\n",
    "        if not csv_str.strip():\n",
    "            print(f\"‚ö†Ô∏è ‰ªªÂä°{task_id}ËøîÂõûÁ©∫Êï∞ÊçÆÔºåË∑≥Ëøá\")\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            # ËØªÂèñCSVÊï∞ÊçÆ\n",
    "            df = pd.read_csv(StringIO(csv_str))\n",
    "            # Ê†°È™åÊ†∏ÂøÉÂ≠óÊÆµÔºàÂåπÈÖçÊ≠£ÂºèÊï∞ÊçÆÁªìÊûÑÔºâ\n",
    "            required_cols = ['date', 'stock_code', 'time', 'open', 'close', 'high', 'low', 'volume']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Áº∫Â∞ëÊ†∏ÂøÉÂ≠óÊÆµ: {missing_cols}\")\n",
    "\n",
    "            # Êï∞ÊçÆÁ±ªÂûãÁªü‰∏ÄÔºàÁ©∫ÂÄº‰øùÁïôÔºå‰∏çÂ°´ÂÖÖ0Ôºâ\n",
    "            numeric_cols = ['open', 'close', 'high', 'low', 'volume']\n",
    "            for col in numeric_cols:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')  # Á©∫ÂÄº‰øùÁïô‰∏∫NaN\n",
    "            # Áªü‰∏ÄÊó∂Èó¥Ê†ºÂºè\n",
    "            df['time'] = pd.to_datetime(df['time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            # ‰øùÂ≠òÂà∞‰∏¥Êó∂ÁõÆÂΩï\n",
    "            temp_file_path = os.path.join(self.temp_download_dir, f\"{task_id}_raw.csv\")\n",
    "            df.to_csv(temp_file_path, index=False, encoding='utf-8')\n",
    "            print(f\"üì• ‰ªªÂä°{task_id}‰∏¥Êó∂CSVÂ∑≤‰øùÂ≠ò: {temp_file_path}\")\n",
    "            return df, temp_file_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ‰ªªÂä°{task_id}CSVÂ§ÑÁêÜÂ§±Ë¥•: {str(e)[:100]}\")\n",
    "            return None, None\n",
    "\n",
    "    # Á¨¨‰∫åÈò∂ÊÆµÔºöÈ™åËØÅ‰∏¥Êó∂Êï∞ÊçÆË¥®ÈáèÔºàÂΩªÂ∫ï‰øÆÂ§çnull_colsÂºïÁî®ÈîôËØØÔºâ\n",
    "    def _stage2_verify_temp_data(self, df: pd.DataFrame, task_id: str) -> bool:\n",
    "        \"\"\"È™åËØÅÊï∞ÊçÆÂÆåÊï¥ÊÄßÔºå‰øÆÂ§çnull_colsÂÆö‰πâÈÄªËæë\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nüìä ‰ªªÂä°{task_id}Êï∞ÊçÆÊ†°È™å:\")\n",
    "            print(f\"ÊÄªËÆ∞ÂΩïÊï∞: {len(df)} | Ê∂âÂèäËÇ°Á•®Êï∞: {df['stock_code'].nunique()}\")\n",
    "            \n",
    "            # 1. Ê£ÄÊü•ÂçïËÇ°Á•®ÂçïÊó•ËÆ∞ÂΩïÊï∞\n",
    "            stock_records = df.groupby('stock_code').size()\n",
    "            abnormal_stocks = stock_records[stock_records != 240].index.tolist()\n",
    "            if abnormal_stocks:\n",
    "                print(f\"‚ö†Ô∏è ÂºÇÂ∏∏ËÇ°Á•®ÔºàËÆ∞ÂΩïÊï∞‚â†240Ôºâ: {abnormal_stocks[:5]}ÔºàÂÖ±{len(abnormal_stocks)}Âè™Ôºâ\")\n",
    "            else:\n",
    "                print(\"‚úÖ ÊâÄÊúâËÇ°Á•®ËÆ∞ÂΩïÊï∞Ê≠£Â∏∏ÔºàÂçïËÇ°Á•®ÂçïÊó•240Êù°Ôºâ\")\n",
    "            \n",
    "            # 2. Ê£ÄÊü•Á©∫ÂÄºÔºàÊ†∏ÂøÉ‰øÆÂ§çÔºöÂü∫‰∫énull_summaryËÆ°ÁÆónull_colsÔºåËÄåÈùûÂºïÁî®Ëá™Ë∫´Ôºâ\n",
    "            null_summary = df.isnull().sum()  # ÂÖàËÆ°ÁÆóÊØèÂàóÁ©∫ÂÄºÊï∞\n",
    "            # Á≠õÈÄâÁ©∫ÂÄºÊï∞>0ÁöÑÂàóÔºåÁîüÊàênull_colsÔºà‰øÆÂ§çÂâçËØØÂÜô‰∏∫null_summary[null_cols > 0]Ôºâ\n",
    "            null_cols = null_summary[null_summary > 0].index.tolist()  \n",
    "            if null_cols:\n",
    "                # Ê†ºÂºèÂåñÁ©∫ÂÄºÁªüËÆ°ÔºàËΩ¨‰∏∫Êï¥Êï∞ÔºåÈÅøÂÖçÁßëÂ≠¶ËÆ°Êï∞Ê≥ïÔºâ\n",
    "                null_stats = {col: int(null_summary[col]) for col in null_cols}\n",
    "                print(f\"‚ÑπÔ∏è Â≠òÂú®Á©∫ÂÄºÁöÑÂ≠óÊÆµ: {null_cols} | Á©∫ÂÄºÁªüËÆ°: {null_stats}\")\n",
    "            else:\n",
    "                print(\"‚úÖ Êó†Á©∫ÂÄºÊï∞ÊçÆ\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ‰ªªÂä°{task_id}Êï∞ÊçÆÊ†°È™åÂ§±Ë¥•: {str(e)[:100]}\")\n",
    "            return False\n",
    "\n",
    "    # Á¨¨‰∏âÈò∂ÊÆµÔºöÂêàÂπ∂Âà∞Ê≠£ÂºèÂàÜÂå∫Parquet + Âà†Èô§‰∏¥Êó∂CSV\n",
    "    def _stage3_merge_and_clean(self, df: pd.DataFrame, task_id: str, temp_file_path: str) -> bool:\n",
    "        \"\"\"ÂêàÂπ∂Êï∞ÊçÆÂà∞Ê≠£ÂºèParquetÔºåÂÆåÊàêÂêéÂà†Èô§‰∏¥Êó∂CSV\"\"\"\n",
    "        try:\n",
    "            # ÊåâËÇ°Á•®‰ª£Á†ÅÂàÜÁªÑÔºåÈÄê‰∏™ÂêàÂπ∂\n",
    "            for stock_code in df['stock_code'].unique():\n",
    "                stock_df = df[df['stock_code'] == stock_code].copy()\n",
    "                # ÊûÑÂª∫Ê≠£ÂºèÂàÜÂå∫Ë∑ØÂæÑÔºàÂåπÈÖç \"stock_code=XXX.XSHE\" ÁªìÊûÑÔºâ\n",
    "                stock_dir = os.path.join(self.formal_data_root, f\"stock_code={stock_code}\")\n",
    "                formal_parquet_path = os.path.join(stock_dir, \"data.parquet\")\n",
    "\n",
    "                # ÂàõÂª∫ËÇ°Á•®ÁõÆÂΩïÔºà‰∏çÂ≠òÂú®ÂàôÊñ∞Âª∫Ôºâ\n",
    "                os.makedirs(stock_dir, exist_ok=True)\n",
    "                print(f\"\\nüîÑ Â§ÑÁêÜËÇ°Á•®: {stock_code} | Ê≠£ÂºèÊñá‰ª∂: {formal_parquet_path}\")\n",
    "\n",
    "                # ÂêàÂπ∂Êï∞ÊçÆÔºà‰øùÁïôÁ©∫ÂÄºÔºâ\n",
    "                if os.path.exists(formal_parquet_path):\n",
    "                    # ËØªÂèñÂ∑≤ÊúâÊï∞ÊçÆÂπ∂ÂéªÈáç\n",
    "                    existing_df = pd.read_parquet(formal_parquet_path)\n",
    "                    combined_df = pd.concat([existing_df, stock_df], ignore_index=True)\n",
    "                    # ÊåâÂîØ‰∏ÄÈîÆÂéªÈáçÔºåÈÅøÂÖçÈáçÂ§çËÆ∞ÂΩï\n",
    "                    combined_df = combined_df.drop_duplicates(subset=['date', 'time', 'stock_code'])\n",
    "                    # ÂÜôÂÖ•ÂêàÂπ∂ÂêéÁöÑÊï∞ÊçÆ\n",
    "                    table = pa.Table.from_pandas(combined_df)\n",
    "                    pq.write_table(table, formal_parquet_path, compression=\"snappy\")\n",
    "                    print(f\"‚úÖ Â∑≤ËøΩÂä†Êï∞ÊçÆ | ÂéüËÆ∞ÂΩïÊï∞: {len(existing_df)} | Êñ∞ËÆ∞ÂΩïÊï∞: {len(stock_df)} | ÂêàÂπ∂Âêé: {len(combined_df)}\")\n",
    "                else:\n",
    "                    # Êñ∞Âª∫ParquetÊñá‰ª∂\n",
    "                    table = pa.Table.from_pandas(stock_df)\n",
    "                    pq.write_table(table, formal_parquet_path, compression=\"snappy\")\n",
    "                    print(f\"‚úÖ Êñ∞Âª∫ParquetÊñá‰ª∂ | ÂàùÂßãËÆ∞ÂΩïÊï∞: {len(stock_df)}\")\n",
    "\n",
    "            # ÂêàÂπ∂ÂÆåÊàêÔºåÂà†Èô§‰∏¥Êó∂CSV\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.remove(temp_file_path)\n",
    "                print(f\"\\nüóëÔ∏è ‰ªªÂä°{task_id}‰∏¥Êó∂CSVÂ∑≤Âà†Èô§: {temp_file_path}\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è ‰ªªÂä°{task_id}‰∏¥Êó∂CSV‰∏çÂ≠òÂú®ÔºåÊó†ÈúÄÂà†Èô§\")\n",
    "\n",
    "            print(f\"üéâ ‰ªªÂä°{task_id}ÊâÄÊúâËÇ°Á•®Â∑≤ÂêàÂπ∂Âà∞Ê≠£ÂºèÂàÜÂå∫\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ‰ªªÂä°{task_id}ÂêàÂπ∂/Ê∏ÖÁêÜÂ§±Ë¥•: {str(e)[:100]}\")\n",
    "            # ÂêàÂπ∂Â§±Ë¥•Êó∂‰øùÁïô‰∏¥Êó∂Êñá‰ª∂Ôºå‰æø‰∫éÊéíÊü•\n",
    "            print(f\"‚ö†Ô∏è ÂêàÂπ∂Â§±Ë¥•Ôºå‰∏¥Êó∂CSVÂ∑≤‰øùÁïô: {temp_file_path}\")\n",
    "            return False\n",
    "\n",
    "    # Êï¥ÂêàÂÆåÊï¥Â§ÑÁêÜÊµÅÁ®ã\n",
    "    def _process_full_flow(self, csv_str: str, task_id: str) -> bool:\n",
    "        # Èò∂ÊÆµ1Ôºö‰∏ãËΩΩÂà∞‰∏¥Êó∂ÁõÆÂΩï\n",
    "        temp_df, temp_file = self._stage1_download_to_temp(csv_str, task_id)\n",
    "        if temp_df is None or temp_file is None:\n",
    "            return False\n",
    "        # Èò∂ÊÆµ2ÔºöÊï∞ÊçÆË¥®ÈáèÈ™åËØÅÔºàÂ∑≤‰øÆÂ§çnull_colsÈîôËØØÔºâ\n",
    "        if not self._stage2_verify_temp_data(temp_df, task_id):\n",
    "            return False\n",
    "        # Èò∂ÊÆµ3ÔºöÂêàÂπ∂Âà∞Ê≠£ÂºèParquet + Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂\n",
    "        if not self._stage3_merge_and_clean(temp_df, task_id, temp_file):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # ÁõëÂê¨ÈòüÂàóÂπ∂ÊâßË°åÂ§ÑÁêÜÔºà‰ºòÂåñblpopËß£ÂåÖÈÄªËæëÔºâ\n",
    "    def listen_and_process(self):\n",
    "        print(f\"‚úÖ ÂºÄÂßãÁõëÂê¨ÁªìÊûúÈòüÂàóÔºà{self.idle_timeout}ÁßíÊó†‰ªªÂä°ÈÄÄÂá∫Ôºâ\")\n",
    "        stats = {\"success\": 0, \"failed\": 0, \"last_active\": time.time()}\n",
    "\n",
    "        while True:\n",
    "            # Ë∂ÖÊó∂ÈÄÄÂá∫ÈÄªËæë\n",
    "            if time.time() - stats[\"last_active\"] > self.idle_timeout:\n",
    "                print(\"\\n‚è∞ ÈïøÊó∂Èó¥Êó†Êñ∞‰ªªÂä°ÔºåÈÄÄÂá∫Â§ÑÁêÜÂô®\")\n",
    "                # Ê∏ÖÁêÜÊÆãÁïô‰∏¥Êó∂Êñá‰ª∂\n",
    "                self._clean_residual_temp_files()\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                # ‰ºòÂåñÔºöÂÖàÊé•Êî∂blpopÁªìÊûúÔºåÈÅøÂÖçÁõ¥Êé•Ëß£ÂåÖNone\n",
    "                queue_result = self.redis.blpop(self.result_queue, timeout=30)\n",
    "                if queue_result is None:\n",
    "                    continue  # Êó†Êï∞ÊçÆÔºåÁªßÁª≠Á≠âÂæÖ\n",
    "                _, result_bytes = queue_result  # ÊúâÊï∞ÊçÆÊó∂ÂÜçËß£ÂåÖ\n",
    "\n",
    "                # Êõ¥Êñ∞Ê¥ªÂä®Êó∂Èó¥\n",
    "                stats[\"last_active\"] = time.time()\n",
    "                # ÂèçÂ∫èÂàóÂåñÁªìÊûú\n",
    "                result = pickle.loads(result_bytes)\n",
    "                task_id = result.get(\"task_id\", \"Êú™Áü•‰ªªÂä°\")\n",
    "\n",
    "                # Â§ÑÁêÜÊàêÂäüÁöÑ‰ªªÂä°\n",
    "                if result.get(\"status\") == \"success\":\n",
    "                    csv_data = result.get(\"result\", \"\")\n",
    "                    if self._process_full_flow(csv_data, task_id):\n",
    "                        stats[\"success\"] += 1\n",
    "                        # Ê∏ÖÁêÜRedis‰ªªÂä°ÂÖÉ‰ø°ÊÅØ\n",
    "                        self.redis.hdel(self.task_metadata, task_id)\n",
    "                        print(f\"\\nüèÜ ‰ªªÂä°{task_id[:8]}...Â§ÑÁêÜÂÆåÊàê | Á¥ØËÆ°ÊàêÂäü: {stats['success']}\")\n",
    "                    else:\n",
    "                        stats[\"failed\"] += 1\n",
    "                        print(f\"‚ùå ‰ªªÂä°{task_id[:8]}...Â§ÑÁêÜÂ§±Ë¥• | Á¥ØËÆ°Â§±Ë¥•: {stats['failed']}\")\n",
    "                else:\n",
    "                    # Â§ÑÁêÜËøúÁ®ãÊâßË°åÂ§±Ë¥•ÁöÑ‰ªªÂä°\n",
    "                    stats[\"failed\"] += 1\n",
    "                    error_msg = result.get(\"error\", \"Êó†ÈîôËØØ‰ø°ÊÅØ\")\n",
    "                    print(f\"‚ùå ‰ªªÂä°{task_id[:8]}...ËøúÁ®ãÂ§±Ë¥•: {error_msg} | Á¥ØËÆ°Â§±Ë¥•: {stats['failed']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Â§ÑÁêÜÂô®ÂºÇÂ∏∏: {str(e)[:80]}ÔºåÁ≠âÂæÖ10ÁßíÈáçËØï\")\n",
    "                time.sleep(10)\n",
    "\n",
    "        # ËæìÂá∫ÊúÄÁªàÁªüËÆ°\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Â§ÑÁêÜÁªìÊûúÊ±áÊÄª\")\n",
    "        print(f\"ÊÄª‰ªªÂä°Êï∞: {stats['success'] + stats['failed']} | ÊàêÂäü: {stats['success']} | Â§±Ë¥•: {stats['failed']}\")\n",
    "        if stats[\"success\"] + stats[\"failed\"] > 0:\n",
    "            print(f\"ÊàêÂäüÁéá: {stats['success']/(stats['success']+stats['failed'])*100:.1f}%\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def _clean_residual_temp_files(self):\n",
    "        \"\"\"Ê∏ÖÁêÜ‰∏¥Êó∂ÁõÆÂΩï‰∏≠ÊÆãÁïôÁöÑCSVÊñá‰ª∂\"\"\"\n",
    "        residual_files = [f for f in os.listdir(self.temp_download_dir) if f.endswith(\"_raw.csv\")]\n",
    "        if not residual_files:\n",
    "            print(f\"‚úÖ ‰∏¥Êó∂ÁõÆÂΩïÊó†ÊÆãÁïôÊñá‰ª∂: {self.temp_download_dir}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nüóëÔ∏è ÂºÄÂßãÊ∏ÖÁêÜ‰∏¥Êó∂ÁõÆÂΩïÊÆãÁïôÊñá‰ª∂ÔºàÂÖ±{len(residual_files)}‰∏™Ôºâ\")\n",
    "        for file_name in residual_files:\n",
    "            file_path = os.path.join(self.temp_download_dir, file_name)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"‚úÖ Âà†Èô§ÊÆãÁïôÊñá‰ª∂: {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Âà†Èô§ÊÆãÁïôÊñá‰ª∂{file_name}Â§±Ë¥•: {str(e)[:50]}\")\n",
    "        print(f\"‚úÖ ‰∏¥Êó∂ÁõÆÂΩïÊ∏ÖÁêÜÂÆåÊàê: {self.temp_download_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # ÂàùÂßãÂåñÂπ∂ÂêØÂä®Â§ÑÁêÜÂô®\n",
    "        processor = MinuteDataProcessor(config_path=\"redis.conf\")\n",
    "        processor.listen_and_process()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Â§ÑÁêÜÂô®ÂêØÂä®Â§±Ë¥•: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2539bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import redis\n",
    "# import pickle\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "# from io import StringIO\n",
    "# from datetime import datetime\n",
    "# from typing import Dict, Optional\n",
    "\n",
    "# class MinuteDataProcessor:\n",
    "#     def __init__(self, config_path: str = \"redis.conf\"):\n",
    "#         \"\"\"ÂàùÂßãÂåñÂ§ÑÁêÜÂô®Ôºå‰∏éÂèëÂ∏ÉÂô®‰øùÊåÅÁõ∏ÂêåÁöÑRedisÈÖçÁΩÆÈÄªËæë\"\"\"\n",
    "#         self.redis_config = self._load_redis_config(config_path)\n",
    "#         self.redis = redis.Redis(\n",
    "#             host=self.redis_config[\"host\"],\n",
    "#             port=self.redis_config[\"port\"],\n",
    "#             password=self.redis_config[\"password\"],\n",
    "#             decode_responses=False\n",
    "#         )\n",
    "#         self.result_queue = \"function_results\"  # ‰∏éÂèëÂ∏ÉÂô®ÂØπÂ∫î\n",
    "#         self.task_metadata = \"task_metadata\"     # ‰∏éÂèëÂ∏ÉÂô®Â≠òÂÇ®ÂÖÉ‰ø°ÊÅØÁöÑÈîÆ‰∏ÄËá¥\n",
    "#         self.storage_root = r\"D:\\workspace\\xiaoyao\\data\\stock_minutely_price\"\n",
    "#         self.idle_timeout = 1800  # 30ÂàÜÈíüÊó†‰ªªÂä°ÈÄÄÂá∫\n",
    "#         self._test_redis_connection()\n",
    "#         self._init_storage()\n",
    "\n",
    "#     def _load_redis_config(self, config_path: str) -> Dict[str, str]:\n",
    "#         \"\"\"Â§çÁî®ÂèëÂ∏ÉÂô®ÁöÑRedisÈÖçÁΩÆÂä†ËΩΩÈÄªËæëÔºåÁ°Æ‰øù‰∏ÄËá¥\"\"\"\n",
    "#         config = {\"host\": \"localhost\", \"port\": 6379, \"password\": \"\"}\n",
    "#         try:\n",
    "#             with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 for line in f:\n",
    "#                     line = line.strip()\n",
    "#                     if line.startswith(\"host=\"):\n",
    "#                         config[\"host\"] = line.split(\"=\", 1)[1].strip()\n",
    "#                     elif line.startswith(\"port=\"):\n",
    "#                         config[\"port\"] = int(line.split(\"=\", 1)[1].strip())\n",
    "#                     elif line.startswith(\"password=\"):\n",
    "#                         config[\"password\"] = line.split(\"=\", 1)[1].strip()\n",
    "#             return config\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ö†Ô∏è ÈÖçÁΩÆÊñá‰ª∂ËØªÂèñÂ§±Ë¥•Ôºå‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆ: {e}\")\n",
    "#             return config\n",
    "\n",
    "#     def _test_redis_connection(self):\n",
    "#         \"\"\"ÊµãËØïRedisËøûÊé•Ôºå‰∏éÂèëÂ∏ÉÂô®ÈÄªËæë‰∏ÄËá¥\"\"\"\n",
    "#         try:\n",
    "#             self.redis.ping()\n",
    "#             print(f\"‚úÖ Â§ÑÁêÜÂô®RedisËøûÊé•ÊàêÂäü | {self.redis_config['host']}:{self.redis_config['port']}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå Â§ÑÁêÜÂô®RedisËøûÊé•Â§±Ë¥•: {e}\")\n",
    "#             raise SystemExit(1)\n",
    "\n",
    "#     def _init_storage(self):\n",
    "#         \"\"\"ÂàùÂßãÂåñÂ≠òÂÇ®ÁõÆÂΩï\"\"\"\n",
    "#         os.makedirs(self.storage_root, exist_ok=True)\n",
    "#         print(f\"‚úÖ Êï∞ÊçÆÂ≠òÂÇ®ÁõÆÂΩï: {self.storage_root}\")\n",
    "\n",
    "#     def _process_csv_data(self, csv_str: str, task_id: str) -> bool:\n",
    "#         \"\"\"Â§ÑÁêÜCSVÊï∞ÊçÆÂπ∂ÊåâËÇ°Á•®Â≠òÂÇ®Ôºà‰ªÖÊåâËÇ°Á•®ÂàÜÂå∫Ôºâ\"\"\"\n",
    "#         if not csv_str.strip():\n",
    "#             print(f\"‚ö†Ô∏è ‰ªªÂä°{task_id}ËøîÂõûÁ©∫Êï∞ÊçÆÔºåË∑≥ËøáÂ§ÑÁêÜ\")\n",
    "#             return False\n",
    "\n",
    "#         try:\n",
    "#             # ‰ªéCSVÂ≠óÁ¨¶‰∏≤ËØªÂèñÊï∞ÊçÆÔºà‰∏éÂèëÂ∏ÉÂô®ÁöÑCSVÂ§ÑÁêÜÈÄªËæëÂÖºÂÆπÔºâ\n",
    "#             df = pd.read_csv(StringIO(csv_str))\n",
    "            \n",
    "#             # ÂøÖË¶ÅÂ≠óÊÆµÊ†°È™å\n",
    "#             required_cols = ['date', 'stock_code', 'time', 'open', 'close', 'high', 'low', 'volume']\n",
    "#             missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "#             if missing_cols:\n",
    "#                 raise ValueError(f\"Áº∫Â∞ëÂøÖË¶ÅÂ≠óÊÆµ: {missing_cols}\")\n",
    "\n",
    "#             # ÊåâËÇ°Á•®‰ª£Á†ÅÂ≠òÂÇ®Ôºà‰ªÖËÇ°Á•®ÂàÜÂå∫ÔºåÊó†Êó•ÊúüÂàÜÂå∫Ôºâ\n",
    "#             for stock_code in df['stock_code'].unique():\n",
    "#                 stock_data = df[df['stock_code'] == stock_code].copy()\n",
    "#                 stock_dir = os.path.join(self.storage_root, f\"stock_code={stock_code}\")\n",
    "#                 os.makedirs(stock_dir, exist_ok=True)\n",
    "#                 parquet_path = os.path.join(stock_dir, \"data.parquet\")\n",
    "\n",
    "#                 # ËΩ¨Êç¢‰∏∫ArrowË°®Âπ∂ËøΩÂä†/ÂàõÂª∫Êñá‰ª∂\n",
    "#                 table = pa.Table.from_pandas(stock_data)\n",
    "#                 if os.path.exists(parquet_path):\n",
    "#                     existing_table = pq.read_table(parquet_path)\n",
    "#                     combined_table = pa.concat_tables([existing_table, table])\n",
    "#                     pq.write_table(combined_table, parquet_path, compression=\"snappy\")\n",
    "#                 else:\n",
    "#                     pq.write_table(table, parquet_path, compression=\"snappy\")\n",
    "\n",
    "#             return True\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå ‰ªªÂä°{task_id}Êï∞ÊçÆÂ§ÑÁêÜÂ§±Ë¥•: {str(e)[:100]}\")\n",
    "#             return False\n",
    "\n",
    "#     def listen_and_process(self):\n",
    "#         \"\"\"ÁõëÂê¨ÁªìÊûúÈòüÂàóÂπ∂Â§ÑÁêÜÊï∞ÊçÆ\"\"\"\n",
    "#         print(f\"‚úÖ ÂºÄÂßãÁõëÂê¨ÁªìÊûúÈòüÂàóÔºà{self.idle_timeout}ÁßíÊó†‰ªªÂä°ÈÄÄÂá∫Ôºâ\")\n",
    "#         stats = {\"success\": 0, \"failed\": 0, \"last_active\": time.time()}\n",
    "\n",
    "#         while True:\n",
    "#             # Ê£ÄÊü•Ë∂ÖÊó∂ÈÄÄÂá∫\n",
    "#             if time.time() - stats[\"last_active\"] > self.idle_timeout:\n",
    "#                 print(\"\\n‚è∞ ÈïøÊó∂Èó¥Êó†Êñ∞‰ªªÂä°ÔºåÈÄÄÂá∫Â§ÑÁêÜÂô®\")\n",
    "#                 break\n",
    "\n",
    "#             # ‰ªéÈòüÂàóËé∑ÂèñÁªìÊûúÔºà‰∏éÂèëÂ∏ÉÂô®ÁöÑÂ∫èÂàóÂåñÊñπÂºèÂåπÈÖçÔºâ\n",
    "#             try:\n",
    "#                 _, result_bytes = self.redis.blpop(self.result_queue, timeout=30)\n",
    "#                 if not result_bytes:\n",
    "#                     continue  # Êó†Êï∞ÊçÆÔºåÁªßÁª≠Á≠âÂæÖ\n",
    "\n",
    "#                 # Êõ¥Êñ∞Ê¥ªÂä®Êó∂Èó¥\n",
    "#                 stats[\"last_active\"] = time.time()\n",
    "\n",
    "#                 # ÂèçÂ∫èÂàóÂåñÁªìÊûúÔºà‰ΩøÁî®pickleÔºå‰∏éÂèëÂ∏ÉÂô®‰∏ÄËá¥Ôºâ\n",
    "#                 result = pickle.loads(result_bytes)\n",
    "#                 task_id = result.get(\"task_id\", \"Êú™Áü•‰ªªÂä°\")\n",
    "\n",
    "#                 # Â§ÑÁêÜÁªìÊûúÔºà‰∏éÂèëÂ∏ÉÂô®ÁöÑ‰ªªÂä°ÁªìÊûÑÂØπÂ∫îÔºâ\n",
    "#                 if result.get(\"status\") == \"success\":\n",
    "#                     # Â§ÑÁêÜÊàêÂäüÁªìÊûú\n",
    "#                     csv_data = result.get(\"result\", \"\")\n",
    "#                     if self._process_csv_data(csv_data, task_id):\n",
    "#                         stats[\"success\"] += 1\n",
    "#                         # Ê∏ÖÁêÜ‰ªªÂä°ÂÖÉ‰ø°ÊÅØÔºà‰∏éÂèëÂ∏ÉÂô®Â≠òÂÇ®ÁöÑÂÖÉ‰ø°ÊÅØÈîÆÂØπÂ∫îÔºâ\n",
    "#                         self.redis.hdel(self.task_metadata, task_id)\n",
    "#                         print(f\"‚úÖ ‰ªªÂä°{task_id[:8]}...Â§ÑÁêÜÊàêÂäü | Á¥ØËÆ°ÊàêÂäü: {stats['success']}\")\n",
    "#                     else:\n",
    "#                         stats[\"failed\"] += 1\n",
    "#                         print(f\"‚ùå ‰ªªÂä°{task_id[:8]}...Êï∞ÊçÆÂ§ÑÁêÜÂ§±Ë¥• | Á¥ØËÆ°Â§±Ë¥•: {stats['failed']}\")\n",
    "#                 else:\n",
    "#                     # Â§ÑÁêÜËøúÁ®ãÊâßË°åÂ§±Ë¥•ÁöÑ‰ªªÂä°\n",
    "#                     stats[\"failed\"] += 1\n",
    "#                     error_msg = result.get(\"error\", \"Êó†ÈîôËØØ‰ø°ÊÅØ\")\n",
    "#                     print(f\"‚ùå ‰ªªÂä°{task_id[:8]}...ËøúÁ®ãÊâßË°åÂ§±Ë¥•: {error_msg} | Á¥ØËÆ°Â§±Ë¥•: {stats['failed']}\")\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"‚ö†Ô∏è Â§ÑÁêÜÂô®ÂºÇÂ∏∏: {str(e)[:80]}ÔºåÁ≠âÂæÖ10ÁßíÈáçËØï\")\n",
    "#                 time.sleep(10)\n",
    "\n",
    "#         # ËæìÂá∫ÊúÄÁªàÁªüËÆ°\n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "#         print(\"ÁªìÊûúÂ§ÑÁêÜÊÄªÁªì\")\n",
    "#         print(f\"ÊÄªÂ§ÑÁêÜ‰ªªÂä°Êï∞: {stats['success'] + stats['failed']}\")\n",
    "#         print(f\"ÊàêÂäü: {stats['success']} | Â§±Ë¥•: {stats['failed']}\")\n",
    "#         if stats[\"success\"] + stats[\"failed\"] > 0:\n",
    "#             print(f\"ÊàêÂäüÁéá: {stats['success']/(stats['success']+stats['failed'])*100:.1f}%\")\n",
    "#         print(\"=\"*50)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         processor = MinuteDataProcessor(config_path=\"redis.conf\")\n",
    "#         processor.listen_and_process()\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Â§ÑÁêÜÂô®ÊâßË°åÂ§±Ë¥•: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d0819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
