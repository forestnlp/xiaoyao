# ä»Jupyter Notebookè½¬æ¢è€Œæ¥çš„Pythonä»£ç 
# åŸå§‹æ–‡ä»¶ï¼šD:\workspace\xiaoyao\redis\jqdata_minutely_price.ipynb



# ----------------------------------------------------------------------#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°† /d:/workspace/xiaoyao/redis/ ç›®å½•ä¸‹çš„æ‰€æœ‰ stock_***.csv æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ª parquet æ–‡ä»¶
ç¡®ä¿ä¸ç°æœ‰ /d:/workspace/xiaoyao/data/stock_daily_price.parquet ä¿æŒå­—æ®µã€å‹ç¼©æ–¹å¼ä¸€è‡´
"""

import os
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path
import glob


def merge_stock_csv_to_parquet(csv_dir,stock_code, output_parquet_file):
    """
    åˆå¹¶æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ stock_***.csv æ–‡ä»¶åˆ°å•ä¸ª parquet æ–‡ä»¶
    
    Args:
        csv_dir: CSV æ–‡ä»¶æ‰€åœ¨ç›®å½•
        output_parquet_file: è¾“å‡ºçš„ parquet æ–‡ä»¶è·¯å¾„
    """
    print(f"ğŸ“ å¼€å§‹å¤„ç†ç›®å½•: {csv_dir}")
    
    # è·å–æ‰€æœ‰ stock_***.csv æ–‡ä»¶
    csv_pattern = os.path.join(csv_dir, f"stock_minutely_price_{stock_code}*.csv")
    csv_files = glob.glob(csv_pattern)
    
    if not csv_files:
        print(f"âŒ æœªæ‰¾åˆ° stock_minutely_price_{stock_code}*.csv æ–‡ä»¶")
        return False
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶")
    
    # æŒ‰æ–‡ä»¶åæ’åºï¼ˆç¡®ä¿æŒ‰æ—¥æœŸé¡ºåºå¤„ç†ï¼‰
    csv_files.sort()
    
    # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰ CSV æ–‡ä»¶
    all_dataframes = []
    total_records = 0
    
    for i, csv_file in enumerate(csv_files, 1):
        filename = os.path.basename(csv_file)
        print(f"æ­£åœ¨å¤„ç† ({i}/{len(csv_files)}): {filename}")
        
        try:
            # è¯»å– CSV æ–‡ä»¶
            df = pd.read_csv(csv_file)
            
            # æ•°æ®éªŒè¯å’Œæ¸…æ´—
            # ç¡®ä¿ date åˆ—æ˜¯ datetime ç±»å‹
            df['date'] = pd.to_datetime(df['date'])
            
            # ç¡®ä¿æ•°å€¼åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®
            numeric_columns = ['open', 'close', 'low', 'high', 'volume']
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # åˆ é™¤æ— æ•ˆæ•°æ®
            df = df.dropna(subset=['date', 'stock_code','time'])
            
            all_dataframes.append(df)
            total_records += len(df)
            print(f"  âœ… æˆåŠŸè¯»å– {len(df)} æ¡è®°å½•")
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            continue
    
    if not all_dataframes:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ•°æ®")
        return False
    
    print(f"\nğŸ“Š åˆå¹¶æ‰€æœ‰æ•°æ®...")
    # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†
    combined_df = pd.concat(all_dataframes, ignore_index=True)
    
    # å»é‡ï¼ˆæŒ‰ date + stock_code + timeï¼‰
    combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code','time'])
    
    # æŒ‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç æ’åº
    combined_df = combined_df.sort_values(['date', 'stock_code','time']).reset_index(drop=True)
    
    print(f"ğŸ“ˆ æ€»è®¡ {len(combined_df)} æ¡è®°å½•ï¼ˆå»é‡åï¼‰")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_parquet_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # è½¬æ¢ä¸º pyarrow Table
    table = pa.Table.from_pandas(combined_df)
    
    # ä½¿ç”¨ä¸ç›®æ ‡æ–‡ä»¶ç›¸åŒçš„å‹ç¼©æ–¹å¼ (snappy) å’Œæ ¼å¼å†™å…¥ parquet
    try:
        pq.write_table(
            table, 
            output_parquet_file,
            compression='snappy',
            version='2.6',  # ä½¿ç”¨è¾ƒæ–°çš„ parquet ç‰ˆæœ¬
            use_dictionary=True,
            write_batch_size=64 * 1024 * 1024  # 64MB batch size for better performance
        )
        
        print(f"âœ… æˆåŠŸä¿å­˜åˆ°: {output_parquet_file}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(output_parquet_file) / (1024*1024):.2f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ parquet æ–‡ä»¶å¤±è´¥: {e}")
        return False

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°†æ–°ç”Ÿæˆçš„ merged_stock_data.parquet ä¸ç°æœ‰çš„ stock_daily_price.parquet åˆå¹¶
"""

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import os
import shutil

def merge_parquet_files(stock_code):
    """
    åˆå¹¶ä¸¤ä¸ª parquet æ–‡ä»¶
    
    Args:
        existing_file: ç°æœ‰çš„ parquet æ–‡ä»¶è·¯å¾„
        new_file: æ–°çš„ parquet æ–‡ä»¶è·¯å¾„  
        output_file: è¾“å‡ºçš„åˆå¹¶æ–‡ä»¶è·¯å¾„
    """

    existing_file = f"d:/workspace/xiaoyao/data/stock_minutely_price/{stock_code}.parquet"
    new_file = f"d:/workspace/xiaoyao/redis/minutely/{stock_code}_merged.parquet"
    output_file = f"d:/workspace/xiaoyao/data/stock_minutely_price/{stock_code}.parquet"


    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(existing_file):
        print(f"âŒ ç°æœ‰æ–‡ä»¶ä¸å­˜åœ¨: {existing_file}")
        #ç›´æ¥ç”¨å…ˆç”¨æ–‡ä»¶å»è¦†ç›–
        shutil.move(new_file, existing_file)
        return
    
    if not os.path.exists(new_file):
        print(f"âŒ æ–°æ–‡ä»¶ä¸å­˜åœ¨: {new_file}")
        return

    print("ğŸ“Š å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶...")
    
    try:
        # è¯»å–ç°æœ‰æ•°æ®
        print(f"ğŸ“– è¯»å–ç°æœ‰æ–‡ä»¶: {existing_file}")
        existing_df = pd.read_parquet(existing_file)
        print(f"   ç°æœ‰æ•°æ®è¡Œæ•°: {len(existing_df)}")
        
        # è¯»å–æ–°æ•°æ®
        print(f"ğŸ“– è¯»å–æ–°æ–‡ä»¶: {new_file}")
        new_df = pd.read_parquet(new_file)
        print(f"   æ–°æ•°æ®è¡Œæ•°: {len(new_df)}")
        
        # åˆå¹¶æ•°æ®
        print("ğŸ”„ åˆå¹¶æ•°æ®ä¸­...")
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        
        # å»é‡ï¼ˆæŒ‰ date + stock_codeï¼‰
        print("ğŸ§¹ å»é‡å¤„ç†...")
        combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code', 'time'])
        
        # æ’åº
        print("ğŸ“… æŒ‰æ—¥æœŸæ’åº...")
        combined_df = combined_df.sort_values(['date', 'stock_code', 'time']).reset_index(drop=True)
        
        print(f"ğŸ“ˆ åˆå¹¶åæ€»è¡Œæ•°: {len(combined_df)}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # è½¬æ¢ä¸º pyarrow Table
        table = pa.Table.from_pandas(combined_df)
        
        # å†™å…¥ parquetï¼ˆä½¿ç”¨ä¸æºæ–‡ä»¶ç›¸åŒçš„æ ¼å¼ï¼‰
        print(f"ğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœ: {output_file}")
        pq.write_table(
            table,
            output_file,
            compression='snappy',
            version='2.6',
            use_dictionary=True,
            write_batch_size=64 * 1024 * 1024
        )
        
        print(f"âœ… åˆå¹¶å®Œæˆï¼æ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / (1024*1024):.2f} MB")
        
        # éªŒè¯ç»“æœ
        print("\nğŸ“‹ éªŒè¯ç»“æœ:")
        result_df = pd.read_parquet(output_file)
        print(f"   æœ€ç»ˆè¡Œæ•°: {len(result_df)}")
        print(f"   æ—¥æœŸèŒƒå›´: {result_df['date'].min()} åˆ° {result_df['date'].max()}")
        print(f"   è‚¡ç¥¨æ•°é‡: {result_df['stock_code'].nunique()}")
        

        # ç§»åŠ¨æ–‡ä»¶ï¼Œæ¸…ç†
        # # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        # source_file_path = f'./minutely/{stock_code}.parquet'
        # target_file_path = f'D:/workspace/xiaoyao/redis/data/minutely/{stock_code}.parquet'
        # if os.path.exists(source_file_path):
        #     # ç§»åŠ¨æ–‡ä»¶
        #     shutil.move(source_file_path, target_file_path)
        #     print(f"Moved: {file_to_move} to {target_dir}")
        # else:
        #     print(f"File not found: {source_file_path}")

        return True
        
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        return False

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ é™¤æŒ‡å®šç›®å½•ä¸‹çš„ stock_***.csv æ–‡ä»¶
"""

import os
import glob
from pathlib import Path


def delete_stock_csv_files(stock_code,target_directory=r'D:\workspace\xiaoyao\redis\minutely'):
    #åˆ é™¤æ»¡è¶³æ¨¡å¼çš„æ‰€æœ‰æ–‡ä»¶
    pattern = f"stock_minutely_price_{stock_code}*.csv"
    files = glob.glob(os.path.join(target_directory, pattern))
    for file in files:
        try:
            os.remove(file)
            print(f"å·²åˆ é™¤ï¼š{file}")
        except Exception as e:
            print(f"åˆ é™¤ {file} å¤±è´¥ï¼š{e}")

import redis
import pickle
import time
import uuid
import pandas as pd
from io import StringIO
from typing import Any, Optional
from datetime import datetime, timedelta

class RemoteSender:
    def __init__(self, host='*', port=6379, password='*'):
        self.redis = redis.Redis(
            host=host, port=port, password=password,
            decode_responses=False
        )
        self.task_queue = 'function_calls'
        self.result_queue = 'function_results'
        self._test_connection()
        print(f"âœ… å‘é€ç«¯pandasç‰ˆæœ¬ï¼š{pd.__version__}")

    def _test_connection(self):
        try:
            self.redis.ping()
            print("âœ… å‘é€ç«¯ï¼šRedisè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å‘é€ç«¯ï¼šè¿æ¥å¤±è´¥ - {e}")
            raise

    def call_remote_function(self, func_name: str, *args, **kwargs) -> Any:
        task_id = f"task_{uuid.uuid4().hex[:8]}"
        task = {
            'func_name': func_name,
            'args': args,
            'kwargs': kwargs,
            'task_id': task_id
        }
        self.redis.rpush(self.task_queue, pickle.dumps(task))
        print(f"ğŸ“¤ å·²è°ƒç”¨è¿œç¨‹å‡½æ•°ï¼š{func_name}ï¼ˆä»»åŠ¡IDï¼š{task_id}ï¼‰")
        return self._get_result(task_id)

    def _get_result(self, task_id: str, timeout=300) -> Any:
        start_time = time.time()
        while time.time() - start_time < timeout:
            result_data = self.redis.blpop(self.result_queue, timeout=10)
            if not result_data:
                continue

            _, res_bytes = result_data
            result = pickle.loads(res_bytes)
            if result['task_id'] == task_id:
                if result['status'] == 'success':
                    return result['result']  # è¿”å›CSVå­—ç¬¦ä¸²
                else:
                    raise Exception(f"è¿œç¨‹æ‰§è¡Œå¤±è´¥ï¼š{result['error']}")
            self.redis.rpush(self.result_queue, res_bytes)
        raise TimeoutError("ä»»åŠ¡è¶…æ—¶")

    def save_to_csv(self, csv_str: Optional[str], filename: str) -> bool:
        """å°†CSVå­—ç¬¦ä¸²ä¿å­˜ä¸ºæœ¬åœ°CSVæ–‡ä»¶ï¼ˆæ›¿ä»£Parquetï¼‰"""
        if not csv_str:
            print("âš ï¸ æ•°æ®ä¸ºç©ºï¼Œä¸ä¿å­˜")
            return False
        try:
            # ä»CSVå­—ç¬¦ä¸²æ¢å¤DataFrameï¼ˆå…¼å®¹æ‰€æœ‰pandasç‰ˆæœ¬ï¼‰
            df = pd.read_csv(StringIO(csv_str))
            # ä¿å­˜ä¸ºCSVæ–‡ä»¶
            df.to_csv(filename, index=False)
            print(f"âœ… ä¿å­˜æˆåŠŸï¼š{filename}ï¼ˆ{len(df)}æ¡è®°å½•ï¼‰")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥ï¼š{e}")
            return False

def generate_date_range(start_date_str: str, end_date_str: str) -> list:
    """ç”Ÿæˆä»å¼€å§‹æ—¥æœŸåˆ°ç»“æŸæ—¥æœŸçš„æ‰€æœ‰æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆYYYYMMDDæ ¼å¼ï¼‰"""
    dates = []
    try:
        start_date = datetime.strptime(start_date_str, '%Y%m%d')
        end_date = datetime.strptime(end_date_str, '%Y%m%d')
        
        if start_date > end_date:
            raise ValueError("å¼€å§‹æ—¥æœŸæ™šäºç»“æŸæ—¥æœŸ")
            
        current_date = start_date
        while current_date <= end_date:
            dates.append(current_date.strftime('%Y%m%d'))
            current_date += timedelta(days=1)
    except Exception as e:
        print(f"æ—¥æœŸå¤„ç†é”™è¯¯ï¼š{e}")
    return dates

from tqdm import tqdm


if __name__ == "__main__":

    # ä»é…ç½®æ–‡ä»¶è¯»å–Redisè¿æ¥ä¿¡æ¯
    with open('redis.conf', 'r') as f:
        for line in f:
            if line.startswith('host='):
                host = line.split('=')[1].strip()
            elif line.startswith('port='):
                port = int(line.split('=')[1].strip())
            elif line.startswith('password='):
                password = line.split('=')[1].strip()
    # åˆå§‹åŒ–Rediså‘é€ç«¯
    sender = RemoteSender(host=host, port=port, password=password)
    
    # å®šä¹‰æ—¥æœŸèŒƒå›´ï¼šä»20250516åˆ°20250923
    start_date = '20250922'#(df['date'].max() + timedelta(days=1)).strftime('%Y%m%d')
    # è·å–å½“æ—¥æ—¥æœŸ-1ï¼Œæ˜¯end_date
    end_date = '20250926'#(datetime.today() - timedelta(days=1)).strftime('%Y%m%d')
    
    
    daily_df = pd.read_parquet('../data/stock_daily_price.parquet')
    stock_code_list = daily_df['stock_code'].unique()

    # å¾ªç¯è°ƒç”¨è·å–æ¯æ—¥æ•°æ®
    for stock_code in tqdm(stock_code_list):
        # è¯»å–å·²å­˜åœ¨çš„parquetæ–‡ä»¶
        start_date2 = start_date

        existing_file = f"d:/workspace/xiaoyao/redis/minutely/{stock_code}.parquet"
        # if os.path.exists(existing_file):
        #     existing_df = pd.read_parquet(existing_file)
        #     # è¯»å–existing_dfçš„dateåˆ—çš„æœ€å¤§date
        #     max_date = existing_df['date'].max()
        #     start_date2 = (datetime.strptime(max_date, '%Y%m%d') + timedelta(days=1)).strftime('%Y%m%d')
        
        date_list = generate_date_range(start_date2, end_date)
        print(f"=== å…±éœ€è·å– {len(date_list)} å¤©çš„æ•°æ® ===")

        for i, date in enumerate(date_list, 1):
            try:
                # è°ƒç”¨è¿œç¨‹å‡½æ•°è·å–å½“æ—¥æ•°æ®
                csv_data = sender.call_remote_function('fetch_minute_stock_data', date,[stock_code])
                # ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼Œæ–‡ä»¶ååŒ…å«æ—¥æœŸ
                sender.save_to_csv(csv_data, f'./minutely/stock_minutely_price_{stock_code}_{date}.csv')
                # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(0.05)
            except Exception as e:
                print(f"âŒ {date} å¤„ç†å¤±è´¥ï¼š{e}")
                # å¤±è´¥åä¹Ÿå»¶è¿Ÿä¸€ä¸‹ï¼Œé¿å…å¿«é€Ÿé‡è¯•å¯¼è‡´çš„é—®é¢˜
                time.sleep(1)
        # ä¸‹è½½å®Œæˆåå¯¹csvè¿›è¡Œåˆå¹¶
        success = merge_stock_csv_to_parquet('./minutely/', stock_code, f'./minutely/{stock_code}_merged.parquet')
        
        if not success:
            print(f"âŒ {stock_code} åˆå¹¶å¤±è´¥")
        
        # å°†ç”Ÿæˆçš„parquetè¿›è¡Œåˆå¹¶
        merge_parquet_files(stock_code)


        # åˆ é™¤æŒ‡å®šçš„parquetæ–‡ä»¶
        file_to_delete = f'./minutely/{stock_code}_merged.parquet'
        if os.path.exists(file_to_delete):
            os.remove(file_to_delete)


        file_to_delete = f'./minutely/{stock_code}.parquet'
        if os.path.exists(file_to_delete):
            os.remove(file_to_delete)

        # åˆ é™¤æ— ç”¨çš„csv
        delete_stock_csv_files(stock_code)

    print("\n=== æ‰€æœ‰æ—¥æœŸå¤„ç†å®Œæˆ ===")

