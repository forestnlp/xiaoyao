{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c12029",
   "metadata": {},
   "source": [
    "## 1.下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 发送端：Redis连接成功\n",
      "✅ 发送端pandas版本：2.3.2\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/stock_daily_concept.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 108\u001b[0m\n\u001b[0;32m    104\u001b[0m sender \u001b[38;5;241m=\u001b[39m RemoteSender(host\u001b[38;5;241m=\u001b[39mhost, port\u001b[38;5;241m=\u001b[39mport, password\u001b[38;5;241m=\u001b[39mpassword)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# 定义日期范围：从20250516到20250923\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# 读取../data/stock_daily_price.parquet文件，获取最大的日期+1，是start_date\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/stock_daily_concept.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m start_date \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# 获取当日日期-1，是end_date\u001b[39;00m\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    666\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    667\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    670\u001b[0m     path,\n\u001b[0;32m    671\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    672\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    673\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    674\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    675\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    676\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    678\u001b[0m )\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    257\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    266\u001b[0m         path_or_handle,\n\u001b[0;32m    267\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    271\u001b[0m     )\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    131\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\sdk\\Anaconda3\\envs\\xiaoyao\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/stock_daily_concept.parquet'"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import pickle\n",
    "import time\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from typing import Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RemoteSender:\n",
    "    def __init__(self, host='*', port=6379, password='*'):\n",
    "        self.redis = redis.Redis(\n",
    "            host=host, port=port, password=password,\n",
    "            decode_responses=False\n",
    "        )\n",
    "        self.task_queue = 'function_calls'\n",
    "        self.result_queue = 'function_results'\n",
    "        self._test_connection()\n",
    "        print(f\"✅ 发送端pandas版本：{pd.__version__}\")\n",
    "\n",
    "    def _test_connection(self):\n",
    "        try:\n",
    "            self.redis.ping()\n",
    "            print(\"✅ 发送端：Redis连接成功\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 发送端：连接失败 - {e}\")\n",
    "            raise\n",
    "\n",
    "    def call_remote_function(self, func_name: str, *args, **kwargs) -> Any:\n",
    "        task_id = f\"task_{uuid.uuid4().hex[:8]}\"\n",
    "        task = {\n",
    "            'func_name': func_name,\n",
    "            'args': args,\n",
    "            'kwargs': kwargs,\n",
    "            'task_id': task_id\n",
    "        }\n",
    "        self.redis.rpush(self.task_queue, pickle.dumps(task))\n",
    "        print(f\"📤 已调用远程函数：{func_name}（任务ID：{task_id}）\")\n",
    "        return self._get_result(task_id)\n",
    "\n",
    "    def _get_result(self, task_id: str, timeout=300) -> Any:\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < timeout:\n",
    "            result_data = self.redis.blpop(self.result_queue, timeout=10)\n",
    "            if not result_data:\n",
    "                continue\n",
    "\n",
    "            _, res_bytes = result_data\n",
    "            result = pickle.loads(res_bytes)\n",
    "            if result['task_id'] == task_id:\n",
    "                if result['status'] == 'success':\n",
    "                    return result['result']  # 返回CSV字符串\n",
    "                else:\n",
    "                    raise Exception(f\"远程执行失败：{result['error']}\")\n",
    "            self.redis.rpush(self.result_queue, res_bytes)\n",
    "        raise TimeoutError(\"任务超时\")\n",
    "\n",
    "    def save_to_csv(self, csv_str: Optional[str], filename: str) -> bool:\n",
    "        \"\"\"将CSV字符串保存为本地CSV文件（替代Parquet）\"\"\"\n",
    "        if not csv_str:\n",
    "            print(\"⚠️ 数据为空，不保存\")\n",
    "            return False\n",
    "        try:\n",
    "            # 从CSV字符串恢复DataFrame（兼容所有pandas版本）\n",
    "            df = pd.read_csv(StringIO(csv_str))\n",
    "            # 保存为CSV文件\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"✅ 保存成功：{filename}（{len(df)}条记录）\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 保存失败：{e}\")\n",
    "            return False\n",
    "\n",
    "def generate_date_range(start_date_str: str, end_date_str: str) -> list:\n",
    "    \"\"\"生成从开始日期到结束日期的所有日期字符串（YYYYMMDD格式）\"\"\"\n",
    "    dates = []\n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "        end_date = datetime.strptime(end_date_str, '%Y%m%d')\n",
    "        \n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"开始日期晚于结束日期\")\n",
    "            \n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            dates.append(current_date.strftime('%Y%m%d'))\n",
    "            current_date += timedelta(days=1)\n",
    "    except Exception as e:\n",
    "        print(f\"日期处理错误：{e}\")\n",
    "    return dates\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 从配置文件读取Redis连接信息\n",
    "    with open('redis.conf', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('host='):\n",
    "                host = line.split('=')[1].strip()\n",
    "            elif line.startswith('port='):\n",
    "                port = int(line.split('=')[1].strip())\n",
    "            elif line.startswith('password='):\n",
    "                password = line.split('=')[1].strip()\n",
    "    # 初始化Redis发送端\n",
    "    sender = RemoteSender(host=host, port=port, password=password)\n",
    "    \n",
    "    # 定义日期范围：从20250516到20250923\n",
    "    # 读取../data/stock_daily_price.parquet文件，获取最大的日期+1，是start_date\n",
    "    parquet_file = '../data/stock_daily_concept.parquet'\n",
    "    # 如果 parquet_file 存在，读取最大日期+1，否则从20250516开始\n",
    "    try:\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "        start_date = (df['date'].max() + timedelta(days=1)).strftime('%Y%m%d')\n",
    "    except FileNotFoundError:\n",
    "        start_date = '20250101'\n",
    "    # 获取当日日期-1，是end_date\n",
    "    end_date = (datetime.today() - timedelta(days=1)).strftime('%Y%m%d')\n",
    "    \n",
    "    # 生成日期列表\n",
    "    date_list = generate_date_range(start_date, end_date)\n",
    "\n",
    "    print(f\"=== 共需获取 {len(date_list)} 天的数据 ===\")\n",
    "    \n",
    "    # 循环调用获取每日数据\n",
    "    for i, date in enumerate(date_list, 1):\n",
    "        print(f\"\\n=== 正在处理 {i}/{len(date_list)}：{date} ===\")\n",
    "        try:\n",
    "            # 调用远程函数获取当日数据\n",
    "            csv_data = sender.call_remote_function('fetch_stock_concept', date)\n",
    "            # 保存为CSV文件，文件名包含日期\n",
    "            sender.save_to_csv(csv_data, f'stock_daily_concept_{date}.csv')\n",
    "            \n",
    "            # 适当延迟，避免请求过于频繁\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {date} 处理失败：{e}\")\n",
    "            # 失败后也延迟一下，避免快速重试导致的问题\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(\"\\n=== 所有日期处理完成 ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47950e5",
   "metadata": {},
   "source": [
    "## 将csv合并为一个parquet文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68654520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🚀 开始合并 stock_***.csv 文件到 parquet\n",
      "============================================================\n",
      "📁 开始处理目录: d:/workspace/xiaoyao/redis/\n",
      "📊 找到 2 个 CSV 文件\n",
      "正在处理 (1/2): stock_daily_concept_20250102.csv\n",
      "  ✅ 成功读取 40401 条记录\n",
      "正在处理 (2/2): stock_daily_concept_20250103.csv\n",
      "  ✅ 成功读取 40411 条记录\n",
      "\n",
      "📊 合并所有数据...\n",
      "📈 总计 10150 条记录（去重后）\n",
      "✅ 成功保存到: d:/workspace/xiaoyao/redis/parquet\\stock_daily_concept_to_merged.parquet\n",
      "📊 文件大小: 0.06 MB\n",
      "\n",
      "🎉 合并完成！\n",
      "\n",
      "📋 验证结果:\n",
      "   总行数: 10150\n",
      "   日期范围: 2025-01-02 00:00:00 到 2025-01-03 00:00:00\n",
      "   股票数量: 5076\n",
      "   列名: ['date', 'stock_code', 'concept_code', 'concept_name']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "将 /d:/workspace/xiaoyao/redis/ 目录下的所有 stock_***.csv 文件合并为一个 parquet 文件\n",
    "确保与现有 /d:/workspace/xiaoyao/data/stock_daily_price.parquet 保持字段、压缩方式一致\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "\n",
    "def merge_stock_csv_to_parquet(csv_dir, output_parquet_file):\n",
    "    \"\"\"\n",
    "    合并指定目录下的所有 stock_***.csv 文件到单个 parquet 文件\n",
    "    \n",
    "    Args:\n",
    "        csv_dir: CSV 文件所在目录\n",
    "        output_parquet_file: 输出的 parquet 文件路径\n",
    "    \"\"\"\n",
    "    print(f\"📁 开始处理目录: {csv_dir}\")\n",
    "    \n",
    "    # 获取所有 stock_***.csv 文件\n",
    "    csv_pattern = os.path.join(csv_dir, \"stock_daily_concept_*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"❌ 未找到 stock_***.csv 文件\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"📊 找到 {len(csv_files)} 个 CSV 文件\")\n",
    "    \n",
    "    # 按文件名排序（确保按日期顺序处理）\n",
    "    csv_files.sort()\n",
    "    \n",
    "    # 读取并合并所有 CSV 文件\n",
    "    all_dataframes = []\n",
    "    total_records = 0\n",
    "    \n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        filename = os.path.basename(csv_file)\n",
    "        print(f\"正在处理 ({i}/{len(csv_files)}): {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取 CSV 文件\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # 数据验证和清洗\n",
    "            # 确保 date 列是 datetime 类型\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            # 确保数值列的数据类型正确\n",
    "            numeric_columns = []\n",
    "            \n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # 删除无效数据\n",
    "            df = df.dropna(subset=['date', 'stock_code'])\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            total_records += len(df)\n",
    "            print(f\"  ✅ 成功读取 {len(df)} 条记录\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ 处理失败: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"❌ 没有成功读取任何数据\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\n📊 合并所有数据...\")\n",
    "    # 合并所有数据框\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # 去重（按 date + stock_code）\n",
    "    combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code','concept_code'])\n",
    "    \n",
    "    # 按日期和股票代码排序\n",
    "    combined_df = combined_df.sort_values(['date', 'stock_code','concept_code']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"📈 总计 {len(combined_df)} 条记录（去重后）\")\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    output_dir = os.path.dirname(output_parquet_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"📁 创建输出目录: {output_dir}\")\n",
    "    \n",
    "    # 转换为 pyarrow Table\n",
    "    table = pa.Table.from_pandas(combined_df)\n",
    "    \n",
    "    # 使用与目标文件相同的压缩方式 (snappy) 和格式写入 parquet\n",
    "    try:\n",
    "        pq.write_table(\n",
    "            table, \n",
    "            output_parquet_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',  # 使用较新的 parquet 版本\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024  # 64MB batch size for better performance\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 成功保存到: {output_parquet_file}\")\n",
    "        print(f\"📊 文件大小: {os.path.getsize(output_parquet_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 保存 parquet 文件失败: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置路径\n",
    "    csv_directory = \"d:/workspace/xiaoyao/redis/\"\n",
    "    # 确保输出目录存在\n",
    "    output_dir = os.path.join(csv_directory, 'parquet')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"📁 创建输出目录: {output_dir}\")\n",
    "\n",
    "    output_file = os.path.join(output_dir, 'stock_daily_concept_to_merged.parquet')\n",
    "    \n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"🚀 开始合并 stock_***.csv 文件到 parquet\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 执行合并\n",
    "    success = merge_stock_csv_to_parquet(csv_directory, output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎉 合并完成！\")\n",
    "        \n",
    "        # 验证结果\n",
    "        try:\n",
    "            print(\"\\n📋 验证结果:\")\n",
    "            result_df = pd.read_parquet(output_file)\n",
    "            print(f\"   总行数: {len(result_df)}\")\n",
    "            print(f\"   日期范围: {result_df['date'].min()} 到 {result_df['date'].max()}\")\n",
    "            print(f\"   股票数量: {result_df['stock_code'].nunique()}\")\n",
    "            print(f\"   列名: {list(result_df.columns)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  验证失败: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n❌ 合并失败！\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab418c",
   "metadata": {},
   "source": [
    "## 将新生成的 stock_daily_auction_to_merged.parquet.parquet 与现有的 stock_daily_auction.parquet 合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🚀 开始合并 parquet 文件\n",
      "============================================================\n",
      "📊 开始合并 parquet 文件...\n",
      "📖 读取现有文件: d:/workspace/xiaoyao/data/stock_daily_industry.parquet\n",
      "   现有数据行数: 3373979\n",
      "📖 读取新文件: D:/workspace/xiaoyao/redis/parquet/stock_daily_industry_to_merged.parquet\n",
      "   新数据行数: 15475\n",
      "🔄 合并数据中...\n",
      "🧹 去重处理...\n",
      "📅 按日期排序...\n",
      "📈 合并后总行数: 3389454\n",
      "💾 保存合并结果: d:/workspace/xiaoyao/redis/parquet/stock_daily_industry.parquet\n",
      "✅ 合并完成！文件大小: 26.71 MB\n",
      "\n",
      "📋 验证结果:\n",
      "   最终行数: 3389454\n",
      "   日期范围: 2023-01-03 00:00:00 到 2025-09-30 00:00:00\n",
      "   股票数量: 5282\n",
      "\n",
      "🎉 合并成功！\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "将新生成的 stock_daily_auction_to_merged.parquet.parquet 与现有的 stock_daily_auction.parquet 合并\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_parquet_files(existing_file, new_file, output_file):\n",
    "    \"\"\"\n",
    "    合并两个 parquet 文件\n",
    "    \n",
    "    Args:\n",
    "        existing_file: 现有的 parquet 文件路径\n",
    "        new_file: 新的 parquet 文件路径  \n",
    "        output_file: 输出的合并文件路径\n",
    "    \"\"\"\n",
    "    print(\"📊 开始合并 parquet 文件...\")\n",
    "    \n",
    "    try:\n",
    "        # 读取现有数据\n",
    "        print(f\"📖 读取现有文件: {existing_file}\")\n",
    "        existing_df = pd.read_parquet(existing_file)\n",
    "        print(f\"   现有数据行数: {len(existing_df)}\")\n",
    "        \n",
    "        # 读取新数据\n",
    "        print(f\"📖 读取新文件: {new_file}\")\n",
    "        new_df = pd.read_parquet(new_file)\n",
    "        print(f\"   新数据行数: {len(new_df)}\")\n",
    "        \n",
    "        # 合并数据\n",
    "        print(\"🔄 合并数据中...\")\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        \n",
    "        # 去重（按 date + stock_code）\n",
    "        print(\"🧹 去重处理...\")\n",
    "        combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code','concept_code'])\n",
    "        \n",
    "        # 排序\n",
    "        print(\"📅 按日期排序...\")\n",
    "        combined_df = combined_df.sort_values(['date', 'stock_code','concept_code']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"📈 合并后总行数: {len(combined_df)}\")\n",
    "        \n",
    "        # 确保输出目录存在\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # 转换为 pyarrow Table\n",
    "        table = pa.Table.from_pandas(combined_df)\n",
    "        \n",
    "        # 写入 parquet（使用与源文件相同的格式）\n",
    "        print(f\"💾 保存合并结果: {output_file}\")\n",
    "        pq.write_table(\n",
    "            table,\n",
    "            output_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 合并完成！文件大小: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # 验证结果\n",
    "        print(\"\\n📋 验证结果:\")\n",
    "        result_df = pd.read_parquet(output_file)\n",
    "        print(f\"   最终行数: {len(result_df)}\")\n",
    "        print(f\"   日期范围: {result_df['date'].min()} 到 {result_df['date'].max()}\")\n",
    "        print(f\"   股票数量: {result_df['stock_code'].nunique()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 合并失败: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置文件路径\n",
    "\n",
    "    existing_file = r\"d:/workspace/xiaoyao/data/stock_daily_concept.parquet\"\n",
    "    new_file = r\"D:/workspace/xiaoyao/redis/parquet/stock_daily_concept_to_merged.parquet\"\n",
    "    output_file = r\"d:/workspace/xiaoyao/redis/parquet/stock_daily_concept.parquet\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🚀 开始合并 parquet 文件\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(existing_file):\n",
    "        print(f\"❌ 现有文件不存在: {existing_file}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(new_file):\n",
    "        print(f\"❌ 新文件不存在: {new_file}\")\n",
    "        return\n",
    "    \n",
    "    # 执行合并\n",
    "    success = merge_parquet_files(existing_file, new_file, output_file)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎉 合并成功！\")\n",
    "    else:\n",
    "        print(\"\\n❌ 合并失败！\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8770ce2",
   "metadata": {},
   "source": [
    "## 删除已使用的csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c223ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已删除：D:\\workspace\\xiaoyao\\redis\\stock_daily_industry_20250926.csv\n",
      "已删除：D:\\workspace\\xiaoyao\\redis\\stock_daily_industry_20250929.csv\n",
      "已删除：D:\\workspace\\xiaoyao\\redis\\stock_daily_industry_20250930.csv\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "删除指定目录下的 stock_***.csv 文件\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def delete_stock_csv_files(target_directory, pattern=\"stock_*.csv\"):\n",
    "    #删除满足模式的所有文件\n",
    "    files = glob.glob(os.path.join(target_directory, pattern))\n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"已删除：{file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"删除 {file} 失败：{e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    delete_stock_csv_files(r'D:\\workspace\\xiaoyao\\redis','stock_daily_concept_*.csv')\n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46e4da",
   "metadata": {},
   "source": [
    "## 将新的parquet文件移动到data目录覆盖原文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved: stock_daily_industry.parquet to D:\\workspace\\xiaoyao\\data\n",
      "Deleted: stock_daily_industry.parquet from D:\\workspace\\xiaoyao\\redis\\parquet\n"
     ]
    }
   ],
   "source": [
    "# 将子目录下的某个parquet文件移动到指定目录\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 定义源目录和目标目录\n",
    "source_dir = \"D:\\\\workspace\\\\xiaoyao\\\\redis\\\\parquet\"\n",
    "target_dir = \"D:\\\\workspace\\\\xiaoyao\\\\data\"\n",
    "\n",
    "# 确保目标目录存在\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# 定义要移动的文件\n",
    "file_to_move = \"stock_daily_concept.parquet\"\n",
    "\n",
    "# 构建源文件的完整路径\n",
    "source_file_path = os.path.join(source_dir, file_to_move)\n",
    "\n",
    "# 构建目标文件的完整路径\n",
    "target_file_path = os.path.join(target_dir, file_to_move)\n",
    "\n",
    "# 检查源文件是否存在\n",
    "if os.path.exists(source_file_path):\n",
    "    # 移动文件\n",
    "    shutil.move(source_file_path, target_file_path)\n",
    "    print(f\"Moved: {file_to_move} to {target_dir}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_move}\")\n",
    "\n",
    "# 删除指定的parquet文件\n",
    "file_to_delete = os.path.join(source_dir, 'stock_daily_concept_to_merged.parquet')\n",
    "if os.path.exists(file_to_delete):\n",
    "    os.remove(file_to_delete)\n",
    "    print(f\"Deleted: {file_to_move} from {source_dir}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_to_move} in {source_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d277a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
