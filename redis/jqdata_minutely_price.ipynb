{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47950e5",
   "metadata": {},
   "source": [
    "## 将csv合并为一个parquet文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68654520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "将 /d:/workspace/xiaoyao/redis/ 目录下的所有 stock_***.csv 文件合并为一个 parquet 文件\n",
    "确保与现有 /d:/workspace/xiaoyao/data/stock_daily_price.parquet 保持字段、压缩方式一致\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "\n",
    "def merge_stock_csv_to_parquet(csv_dir,stock_code, output_parquet_file):\n",
    "    \"\"\"\n",
    "    合并指定目录下的所有 stock_***.csv 文件到单个 parquet 文件\n",
    "    \n",
    "    Args:\n",
    "        csv_dir: CSV 文件所在目录\n",
    "        output_parquet_file: 输出的 parquet 文件路径\n",
    "    \"\"\"\n",
    "    print(f\"📁 开始处理目录: {csv_dir}\")\n",
    "    \n",
    "    # 获取所有 stock_***.csv 文件\n",
    "    csv_pattern = os.path.join(csv_dir, f\"stock_minutely_price_{stock_code}*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"❌ 未找到 stock_minutely_price_{stock_code}*.csv 文件\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"📊 找到 {len(csv_files)} 个 CSV 文件\")\n",
    "    \n",
    "    # 按文件名排序（确保按日期顺序处理）\n",
    "    csv_files.sort()\n",
    "    \n",
    "    # 读取并合并所有 CSV 文件\n",
    "    all_dataframes = []\n",
    "    total_records = 0\n",
    "    \n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        filename = os.path.basename(csv_file)\n",
    "        print(f\"正在处理 ({i}/{len(csv_files)}): {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # 读取 CSV 文件\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # 数据验证和清洗\n",
    "            # 确保 date 列是 datetime 类型\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            # 确保数值列的数据类型正确\n",
    "            numeric_columns = ['open', 'close', 'low', 'high', 'volume']\n",
    "            \n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # 删除无效数据\n",
    "            df = df.dropna(subset=['date', 'stock_code','time'])\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            total_records += len(df)\n",
    "            print(f\"  ✅ 成功读取 {len(df)} 条记录\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ 处理失败: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"❌ 没有成功读取任何数据\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\n📊 合并所有数据...\")\n",
    "    # 合并所有数据框\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # 去重（按 date + stock_code + time）\n",
    "    combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code','time'])\n",
    "    \n",
    "    # 按日期和股票代码排序\n",
    "    combined_df = combined_df.sort_values(['date', 'stock_code','time']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"📈 总计 {len(combined_df)} 条记录（去重后）\")\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    output_dir = os.path.dirname(output_parquet_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"📁 创建输出目录: {output_dir}\")\n",
    "    \n",
    "    # 转换为 pyarrow Table\n",
    "    table = pa.Table.from_pandas(combined_df)\n",
    "    \n",
    "    # 使用与目标文件相同的压缩方式 (snappy) 和格式写入 parquet\n",
    "    try:\n",
    "        pq.write_table(\n",
    "            table, \n",
    "            output_parquet_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',  # 使用较新的 parquet 版本\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024  # 64MB batch size for better performance\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 成功保存到: {output_parquet_file}\")\n",
    "        print(f\"📊 文件大小: {os.path.getsize(output_parquet_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 保存 parquet 文件失败: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ff005",
   "metadata": {},
   "source": [
    "## 将新生成的 merged_stock_data.parquet 与现有的 stock_daily_price.parquet 合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a0e4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "将新生成的 merged_stock_data.parquet 与现有的 stock_daily_price.parquet 合并\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def merge_parquet_files(stock_code):\n",
    "    \"\"\"\n",
    "    合并两个 parquet 文件\n",
    "    \n",
    "    Args:\n",
    "        existing_file: 现有的 parquet 文件路径\n",
    "        new_file: 新的 parquet 文件路径  \n",
    "        output_file: 输出的合并文件路径\n",
    "    \"\"\"\n",
    "\n",
    "    existing_file = f\"d:/workspace/xiaoyao/data/stock_minutely_price/{stock_code}.parquet\"\n",
    "    new_file = f\"d:/workspace/xiaoyao/redis/minutely/{stock_code}_merged.parquet\"\n",
    "    output_file = f\"d:/workspace/xiaoyao/redis/minutely/{stock_code}.parquet\"\n",
    "\n",
    "\n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(existing_file):\n",
    "        print(f\"❌ 现有文件不存在: {existing_file}\")\n",
    "        #直接用先用文件去覆盖\n",
    "        shutil.move(new_file, existing_file)\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(new_file):\n",
    "        print(f\"❌ 新文件不存在: {new_file}\")\n",
    "        return\n",
    "\n",
    "    print(\"📊 开始合并 parquet 文件...\")\n",
    "    \n",
    "    try:\n",
    "        # 读取现有数据\n",
    "        print(f\"📖 读取现有文件: {existing_file}\")\n",
    "        existing_df = pd.read_parquet(existing_file)\n",
    "        print(f\"   现有数据行数: {len(existing_df)}\")\n",
    "        \n",
    "        # 读取新数据\n",
    "        print(f\"📖 读取新文件: {new_file}\")\n",
    "        new_df = pd.read_parquet(new_file)\n",
    "        print(f\"   新数据行数: {len(new_df)}\")\n",
    "        \n",
    "        # 合并数据\n",
    "        print(\"🔄 合并数据中...\")\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        \n",
    "        # 去重（按 date + stock_code）\n",
    "        print(\"🧹 去重处理...\")\n",
    "        combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code', 'time'])\n",
    "        \n",
    "        # 排序\n",
    "        print(\"📅 按日期排序...\")\n",
    "        combined_df = combined_df.sort_values(['date', 'stock_code', 'time']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"📈 合并后总行数: {len(combined_df)}\")\n",
    "        \n",
    "        # 确保输出目录存在\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # 转换为 pyarrow Table\n",
    "        table = pa.Table.from_pandas(combined_df)\n",
    "        \n",
    "        # 写入 parquet（使用与源文件相同的格式）\n",
    "        print(f\"💾 保存合并结果: {output_file}\")\n",
    "        pq.write_table(\n",
    "            table,\n",
    "            output_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 合并完成！文件大小: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # 验证结果\n",
    "        print(\"\\n📋 验证结果:\")\n",
    "        result_df = pd.read_parquet(output_file)\n",
    "        print(f\"   最终行数: {len(result_df)}\")\n",
    "        print(f\"   日期范围: {result_df['date'].min()} 到 {result_df['date'].max()}\")\n",
    "        print(f\"   股票数量: {result_df['stock_code'].nunique()}\")\n",
    "        \n",
    "\n",
    "        # 移动文件，清理\n",
    "        # 检查源文件是否存在\n",
    "        source_file_path = f'./minutely/{stock_code}.parquet'\n",
    "        target_file_path = f'D:/workspace/xiaoyao/redis/data/minutely/{stock_code}.parquet'\n",
    "        if os.path.exists(source_file_path):\n",
    "            # 移动文件\n",
    "            shutil.move(source_file_path, target_file_path)\n",
    "            print(f\"Moved: {file_to_move} to {target_dir}\")\n",
    "        else:\n",
    "            print(f\"File not found: {source_file_path}\")\n",
    "\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 合并失败: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8770ce2",
   "metadata": {},
   "source": [
    "## 删除已使用的csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c223ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "删除指定目录下的 stock_***.csv 文件\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def delete_stock_csv_files(stock_code,target_directory=r'D:\\workspace\\xiaoyao\\redis\\minutely'):\n",
    "    #删除满足模式的所有文件\n",
    "    pattern = f\"stock_minutely_price_{stock_code}*.csv\"\n",
    "    files = glob.glob(os.path.join(target_directory, pattern))\n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"已删除：{file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"删除 {file} 失败：{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d9ab5",
   "metadata": {},
   "source": [
    "## 下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d277a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import pickle\n",
    "import time\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from typing import Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RemoteSender:\n",
    "    def __init__(self, host='*', port=6379, password='*'):\n",
    "        self.redis = redis.Redis(\n",
    "            host=host, port=port, password=password,\n",
    "            decode_responses=False\n",
    "        )\n",
    "        self.task_queue = 'function_calls'\n",
    "        self.result_queue = 'function_results'\n",
    "        self._test_connection()\n",
    "        print(f\"✅ 发送端pandas版本：{pd.__version__}\")\n",
    "\n",
    "    def _test_connection(self):\n",
    "        try:\n",
    "            self.redis.ping()\n",
    "            print(\"✅ 发送端：Redis连接成功\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 发送端：连接失败 - {e}\")\n",
    "            raise\n",
    "\n",
    "    def call_remote_function(self, func_name: str, *args, **kwargs) -> Any:\n",
    "        task_id = f\"task_{uuid.uuid4().hex[:8]}\"\n",
    "        task = {\n",
    "            'func_name': func_name,\n",
    "            'args': args,\n",
    "            'kwargs': kwargs,\n",
    "            'task_id': task_id\n",
    "        }\n",
    "        self.redis.rpush(self.task_queue, pickle.dumps(task))\n",
    "        print(f\"📤 已调用远程函数：{func_name}（任务ID：{task_id}）\")\n",
    "        return self._get_result(task_id)\n",
    "\n",
    "    def _get_result(self, task_id: str, timeout=300) -> Any:\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < timeout:\n",
    "            result_data = self.redis.blpop(self.result_queue, timeout=10)\n",
    "            if not result_data:\n",
    "                continue\n",
    "\n",
    "            _, res_bytes = result_data\n",
    "            result = pickle.loads(res_bytes)\n",
    "            if result['task_id'] == task_id:\n",
    "                if result['status'] == 'success':\n",
    "                    return result['result']  # 返回CSV字符串\n",
    "                else:\n",
    "                    raise Exception(f\"远程执行失败：{result['error']}\")\n",
    "            self.redis.rpush(self.result_queue, res_bytes)\n",
    "        raise TimeoutError(\"任务超时\")\n",
    "\n",
    "    def save_to_csv(self, csv_str: Optional[str], filename: str) -> bool:\n",
    "        \"\"\"将CSV字符串保存为本地CSV文件（替代Parquet）\"\"\"\n",
    "        if not csv_str:\n",
    "            print(\"⚠️ 数据为空，不保存\")\n",
    "            return False\n",
    "        try:\n",
    "            # 从CSV字符串恢复DataFrame（兼容所有pandas版本）\n",
    "            df = pd.read_csv(StringIO(csv_str))\n",
    "            # 保存为CSV文件\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"✅ 保存成功：{filename}（{len(df)}条记录）\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 保存失败：{e}\")\n",
    "            return False\n",
    "\n",
    "def generate_date_range(start_date_str: str, end_date_str: str) -> list:\n",
    "    \"\"\"生成从开始日期到结束日期的所有日期字符串（YYYYMMDD格式）\"\"\"\n",
    "    dates = []\n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "        end_date = datetime.strptime(end_date_str, '%Y%m%d')\n",
    "        \n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"开始日期晚于结束日期\")\n",
    "            \n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            dates.append(current_date.strftime('%Y%m%d'))\n",
    "            current_date += timedelta(days=1)\n",
    "    except Exception as e:\n",
    "        print(f\"日期处理错误：{e}\")\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6225bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 发送端：Redis连接成功\n",
      "✅ 发送端pandas版本：2.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5447 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 共需获取 283 天的数据 ===\n",
      "📤 已调用远程函数：fetch_minute_stock_data（任务ID：task_f0733dec）\n",
      "⚠️ 数据为空，不保存\n",
      "📤 已调用远程函数：fetch_minute_stock_data（任务ID：task_6c496a80）\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 从配置文件读取Redis连接信息\n",
    "    with open('redis.conf', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('host='):\n",
    "                host = line.split('=')[1].strip()\n",
    "            elif line.startswith('port='):\n",
    "                port = int(line.split('=')[1].strip())\n",
    "            elif line.startswith('password='):\n",
    "                password = line.split('=')[1].strip()\n",
    "    # 初始化Redis发送端\n",
    "    sender = RemoteSender(host=host, port=port, password=password)\n",
    "    \n",
    "    # 定义日期范围：从20250516到20250923\n",
    "    start_date = '20251008'#(df['date'].max() + timedelta(days=1)).strftime('%Y%m%d')\n",
    "    # 获取当日日期-1，是end_date\n",
    "    end_date = (datetime.today() - timedelta(days=1)).strftime('%Y%m%d')\n",
    "    \n",
    "    \n",
    "    daily_df = pd.read_parquet('../data/stock_daily_price.parquet')\n",
    "    stock_code_list = daily_df['stock_code'].unique()\n",
    "\n",
    "    # 循环调用获取每日数据\n",
    "    for stock_code in tqdm(stock_code_list):\n",
    "        # 读取已存在的parquet文件\n",
    "        start_date2 = start_date\n",
    "\n",
    "        existing_file = f\"d:/workspace/xiaoyao/redis/minutely/{stock_code}.parquet\"\n",
    "        if os.path.exists(existing_file):\n",
    "            existing_df = pd.read_parquet(existing_file)\n",
    "            # 读取existing_df的date列的最大date\n",
    "            max_date = existing_df['date'].max()\n",
    "            start_date2 = (datetime.strptime(max_date, '%Y%m%d') + timedelta(days=1)).strftime('%Y%m%d')\n",
    "        \n",
    "        date_list = generate_date_range(start_date2, end_date)\n",
    "        print(f\"=== 共需获取 {len(date_list)} 天的数据 ===\")\n",
    "\n",
    "        for i, date in enumerate(date_list, 1):\n",
    "            try:\n",
    "                # 调用远程函数获取当日数据\n",
    "                csv_data = sender.call_remote_function('fetch_minute_stock_data', date,[stock_code])\n",
    "                # 保存为CSV文件，文件名包含日期\n",
    "                sender.save_to_csv(csv_data, f'./minutely/stock_minutely_price_{stock_code}_{date}.csv')\n",
    "                # 适当延迟，避免请求过于频繁\n",
    "                time.sleep(0.05)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {date} 处理失败：{e}\")\n",
    "                # 失败后也延迟一下，避免快速重试导致的问题\n",
    "                time.sleep(1)\n",
    "        # 下载完成后对csv进行合并\n",
    "        success = merge_stock_csv_to_parquet('./minutely/', stock_code, f'./minutely/{stock_code}_merged.parquet')\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"❌ {stock_code} 合并失败\")\n",
    "        \n",
    "        # 将生成的parquet进行合并\n",
    "        merge_parquet_files(stock_code)\n",
    "\n",
    "\n",
    "        # 删除指定的parquet文件\n",
    "        file_to_delete = f'./minutely/{stock_code}_merged.parquet'\n",
    "        if os.path.exists(file_to_delete):\n",
    "            os.remove(file_to_delete)\n",
    "        else:\n",
    "            print(f\"File not found: {file_to_delete}\")\n",
    "\n",
    "        file_to_delete = f'./minutely/{stock_code}.parquet'\n",
    "        if os.path.exists(file_to_delete):\n",
    "            os.remove(file_to_delete)\n",
    "        else:\n",
    "            print(f\"File not found: {file_to_delete}\")\n",
    "\n",
    "        # 删除无用的csv\n",
    "        delete_stock_csv_files(stock_code)\n",
    "\n",
    "    print(\"\\n=== 所有日期处理完成 ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712c785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
