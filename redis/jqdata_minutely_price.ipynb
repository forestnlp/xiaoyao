{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47950e5",
   "metadata": {},
   "source": [
    "## å°†csvåˆå¹¶ä¸ºä¸€ä¸ªparquetæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68654520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "å°† /d:/workspace/xiaoyao/redis/ ç›®å½•ä¸‹çš„æ‰€æœ‰ stock_***.csv æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ª parquet æ–‡ä»¶\n",
    "ç¡®ä¿ä¸ç°æœ‰ /d:/workspace/xiaoyao/data/stock_daily_price.parquet ä¿æŒå­—æ®µã€å‹ç¼©æ–¹å¼ä¸€è‡´\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "\n",
    "def merge_stock_csv_to_parquet(csv_dir,stock_code, output_parquet_file):\n",
    "    \"\"\"\n",
    "    åˆå¹¶æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ stock_***.csv æ–‡ä»¶åˆ°å•ä¸ª parquet æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        csv_dir: CSV æ–‡ä»¶æ‰€åœ¨ç›®å½•\n",
    "        output_parquet_file: è¾“å‡ºçš„ parquet æ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“ å¼€å§‹å¤„ç†ç›®å½•: {csv_dir}\")\n",
    "    \n",
    "    # è·å–æ‰€æœ‰ stock_***.csv æ–‡ä»¶\n",
    "    csv_pattern = os.path.join(csv_dir, f\"stock_minutely_price_{stock_code}*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"âŒ æœªæ‰¾åˆ° stock_minutely_price_{stock_code}*.csv æ–‡ä»¶\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"ğŸ“Š æ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶\")\n",
    "    \n",
    "    # æŒ‰æ–‡ä»¶åæ’åºï¼ˆç¡®ä¿æŒ‰æ—¥æœŸé¡ºåºå¤„ç†ï¼‰\n",
    "    csv_files.sort()\n",
    "    \n",
    "    # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰ CSV æ–‡ä»¶\n",
    "    all_dataframes = []\n",
    "    total_records = 0\n",
    "    \n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        filename = os.path.basename(csv_file)\n",
    "        print(f\"æ­£åœ¨å¤„ç† ({i}/{len(csv_files)}): {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # è¯»å– CSV æ–‡ä»¶\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # æ•°æ®éªŒè¯å’Œæ¸…æ´—\n",
    "            # ç¡®ä¿ date åˆ—æ˜¯ datetime ç±»å‹\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            # ç¡®ä¿æ•°å€¼åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®\n",
    "            numeric_columns = ['open', 'close', 'low', 'high', 'volume']\n",
    "            \n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # åˆ é™¤æ— æ•ˆæ•°æ®\n",
    "            df = df.dropna(subset=['date', 'stock_code','time'])\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            total_records += len(df)\n",
    "            print(f\"  âœ… æˆåŠŸè¯»å– {len(df)} æ¡è®°å½•\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ å¤„ç†å¤±è´¥: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ•°æ®\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\nğŸ“Š åˆå¹¶æ‰€æœ‰æ•°æ®...\")\n",
    "    # åˆå¹¶æ‰€æœ‰æ•°æ®æ¡†\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    # å»é‡ï¼ˆæŒ‰ date + stock_code + timeï¼‰\n",
    "    combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code','time'])\n",
    "    \n",
    "    # æŒ‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç æ’åº\n",
    "    combined_df = combined_df.sort_values(['date', 'stock_code','time']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ æ€»è®¡ {len(combined_df)} æ¡è®°å½•ï¼ˆå»é‡åï¼‰\")\n",
    "    \n",
    "    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "    output_dir = os.path.dirname(output_parquet_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}\")\n",
    "    \n",
    "    # è½¬æ¢ä¸º pyarrow Table\n",
    "    table = pa.Table.from_pandas(combined_df)\n",
    "    \n",
    "    # ä½¿ç”¨ä¸ç›®æ ‡æ–‡ä»¶ç›¸åŒçš„å‹ç¼©æ–¹å¼ (snappy) å’Œæ ¼å¼å†™å…¥ parquet\n",
    "    try:\n",
    "        pq.write_table(\n",
    "            table, \n",
    "            output_parquet_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',  # ä½¿ç”¨è¾ƒæ–°çš„ parquet ç‰ˆæœ¬\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024  # 64MB batch size for better performance\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… æˆåŠŸä¿å­˜åˆ°: {output_parquet_file}\")\n",
    "        print(f\"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(output_parquet_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜ parquet æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ff005",
   "metadata": {},
   "source": [
    "## å°†æ–°ç”Ÿæˆçš„ merged_stock_data.parquet ä¸ç°æœ‰çš„ stock_daily_price.parquet åˆå¹¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a0e4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "å°†æ–°ç”Ÿæˆçš„ merged_stock_data.parquet ä¸ç°æœ‰çš„ stock_daily_price.parquet åˆå¹¶\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def merge_parquet_files(stock_code):\n",
    "    \"\"\"\n",
    "    åˆå¹¶ä¸¤ä¸ª parquet æ–‡ä»¶\n",
    "    \n",
    "    Args:\n",
    "        existing_file: ç°æœ‰çš„ parquet æ–‡ä»¶è·¯å¾„\n",
    "        new_file: æ–°çš„ parquet æ–‡ä»¶è·¯å¾„  \n",
    "        output_file: è¾“å‡ºçš„åˆå¹¶æ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "\n",
    "    existing_file = f\"d:/workspace/xiaoyao/data/stock_minutely_price/{stock_code}.parquet\"\n",
    "    new_file = f\"d:/workspace/xiaoyao/redis/minutely/{stock_code}_merged.parquet\"\n",
    "    output_file = f\"d:/workspace/xiaoyao/redis/minutely/{stock_code}.parquet\"\n",
    "\n",
    "\n",
    "    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(existing_file):\n",
    "        print(f\"âŒ ç°æœ‰æ–‡ä»¶ä¸å­˜åœ¨: {existing_file}\")\n",
    "        #ç›´æ¥ç”¨å…ˆç”¨æ–‡ä»¶å»è¦†ç›–\n",
    "        shutil.move(new_file, existing_file)\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(new_file):\n",
    "        print(f\"âŒ æ–°æ–‡ä»¶ä¸å­˜åœ¨: {new_file}\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸ“Š å¼€å§‹åˆå¹¶ parquet æ–‡ä»¶...\")\n",
    "    \n",
    "    try:\n",
    "        # è¯»å–ç°æœ‰æ•°æ®\n",
    "        print(f\"ğŸ“– è¯»å–ç°æœ‰æ–‡ä»¶: {existing_file}\")\n",
    "        existing_df = pd.read_parquet(existing_file)\n",
    "        print(f\"   ç°æœ‰æ•°æ®è¡Œæ•°: {len(existing_df)}\")\n",
    "        \n",
    "        # è¯»å–æ–°æ•°æ®\n",
    "        print(f\"ğŸ“– è¯»å–æ–°æ–‡ä»¶: {new_file}\")\n",
    "        new_df = pd.read_parquet(new_file)\n",
    "        print(f\"   æ–°æ•°æ®è¡Œæ•°: {len(new_df)}\")\n",
    "        \n",
    "        # åˆå¹¶æ•°æ®\n",
    "        print(\"ğŸ”„ åˆå¹¶æ•°æ®ä¸­...\")\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        \n",
    "        # å»é‡ï¼ˆæŒ‰ date + stock_codeï¼‰\n",
    "        print(\"ğŸ§¹ å»é‡å¤„ç†...\")\n",
    "        combined_df = combined_df.drop_duplicates(subset=['date', 'stock_code', 'time'])\n",
    "        \n",
    "        # æ’åº\n",
    "        print(\"ğŸ“… æŒ‰æ—¥æœŸæ’åº...\")\n",
    "        combined_df = combined_df.sort_values(['date', 'stock_code', 'time']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"ğŸ“ˆ åˆå¹¶åæ€»è¡Œæ•°: {len(combined_df)}\")\n",
    "        \n",
    "        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # è½¬æ¢ä¸º pyarrow Table\n",
    "        table = pa.Table.from_pandas(combined_df)\n",
    "        \n",
    "        # å†™å…¥ parquetï¼ˆä½¿ç”¨ä¸æºæ–‡ä»¶ç›¸åŒçš„æ ¼å¼ï¼‰\n",
    "        print(f\"ğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœ: {output_file}\")\n",
    "        pq.write_table(\n",
    "            table,\n",
    "            output_file,\n",
    "            compression='snappy',\n",
    "            version='2.6',\n",
    "            use_dictionary=True,\n",
    "            write_batch_size=64 * 1024 * 1024\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… åˆå¹¶å®Œæˆï¼æ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # éªŒè¯ç»“æœ\n",
    "        print(\"\\nğŸ“‹ éªŒè¯ç»“æœ:\")\n",
    "        result_df = pd.read_parquet(output_file)\n",
    "        print(f\"   æœ€ç»ˆè¡Œæ•°: {len(result_df)}\")\n",
    "        print(f\"   æ—¥æœŸèŒƒå›´: {result_df['date'].min()} åˆ° {result_df['date'].max()}\")\n",
    "        print(f\"   è‚¡ç¥¨æ•°é‡: {result_df['stock_code'].nunique()}\")\n",
    "        \n",
    "\n",
    "        # ç§»åŠ¨æ–‡ä»¶ï¼Œæ¸…ç†\n",
    "        # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "        source_file_path = f'./minutely/{stock_code}.parquet'\n",
    "        target_file_path = f'D:/workspace/xiaoyao/redis/data/minutely/{stock_code}.parquet'\n",
    "        if os.path.exists(source_file_path):\n",
    "            # ç§»åŠ¨æ–‡ä»¶\n",
    "            shutil.move(source_file_path, target_file_path)\n",
    "            print(f\"Moved: {file_to_move} to {target_dir}\")\n",
    "        else:\n",
    "            print(f\"File not found: {source_file_path}\")\n",
    "\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åˆå¹¶å¤±è´¥: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8770ce2",
   "metadata": {},
   "source": [
    "## åˆ é™¤å·²ä½¿ç”¨çš„csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c223ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "åˆ é™¤æŒ‡å®šç›®å½•ä¸‹çš„ stock_***.csv æ–‡ä»¶\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def delete_stock_csv_files(stock_code,target_directory=r'D:\\workspace\\xiaoyao\\redis\\minutely'):\n",
    "    #åˆ é™¤æ»¡è¶³æ¨¡å¼çš„æ‰€æœ‰æ–‡ä»¶\n",
    "    pattern = f\"stock_minutely_price_{stock_code}*.csv\"\n",
    "    files = glob.glob(os.path.join(target_directory, pattern))\n",
    "    for file in files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"å·²åˆ é™¤ï¼š{file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"åˆ é™¤ {file} å¤±è´¥ï¼š{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d9ab5",
   "metadata": {},
   "source": [
    "## ä¸‹è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d277a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import pickle\n",
    "import time\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from typing import Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RemoteSender:\n",
    "    def __init__(self, host='*', port=6379, password='*'):\n",
    "        self.redis = redis.Redis(\n",
    "            host=host, port=port, password=password,\n",
    "            decode_responses=False\n",
    "        )\n",
    "        self.task_queue = 'function_calls'\n",
    "        self.result_queue = 'function_results'\n",
    "        self._test_connection()\n",
    "        print(f\"âœ… å‘é€ç«¯pandasç‰ˆæœ¬ï¼š{pd.__version__}\")\n",
    "\n",
    "    def _test_connection(self):\n",
    "        try:\n",
    "            self.redis.ping()\n",
    "            print(\"âœ… å‘é€ç«¯ï¼šRedisè¿æ¥æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å‘é€ç«¯ï¼šè¿æ¥å¤±è´¥ - {e}\")\n",
    "            raise\n",
    "\n",
    "    def call_remote_function(self, func_name: str, *args, **kwargs) -> Any:\n",
    "        task_id = f\"task_{uuid.uuid4().hex[:8]}\"\n",
    "        task = {\n",
    "            'func_name': func_name,\n",
    "            'args': args,\n",
    "            'kwargs': kwargs,\n",
    "            'task_id': task_id\n",
    "        }\n",
    "        self.redis.rpush(self.task_queue, pickle.dumps(task))\n",
    "        print(f\"ğŸ“¤ å·²è°ƒç”¨è¿œç¨‹å‡½æ•°ï¼š{func_name}ï¼ˆä»»åŠ¡IDï¼š{task_id}ï¼‰\")\n",
    "        return self._get_result(task_id)\n",
    "\n",
    "    def _get_result(self, task_id: str, timeout=300) -> Any:\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < timeout:\n",
    "            result_data = self.redis.blpop(self.result_queue, timeout=10)\n",
    "            if not result_data:\n",
    "                continue\n",
    "\n",
    "            _, res_bytes = result_data\n",
    "            result = pickle.loads(res_bytes)\n",
    "            if result['task_id'] == task_id:\n",
    "                if result['status'] == 'success':\n",
    "                    return result['result']  # è¿”å›CSVå­—ç¬¦ä¸²\n",
    "                else:\n",
    "                    raise Exception(f\"è¿œç¨‹æ‰§è¡Œå¤±è´¥ï¼š{result['error']}\")\n",
    "            self.redis.rpush(self.result_queue, res_bytes)\n",
    "        raise TimeoutError(\"ä»»åŠ¡è¶…æ—¶\")\n",
    "\n",
    "    def save_to_csv(self, csv_str: Optional[str], filename: str) -> bool:\n",
    "        \"\"\"å°†CSVå­—ç¬¦ä¸²ä¿å­˜ä¸ºæœ¬åœ°CSVæ–‡ä»¶ï¼ˆæ›¿ä»£Parquetï¼‰\"\"\"\n",
    "        if not csv_str:\n",
    "            print(\"âš ï¸ æ•°æ®ä¸ºç©ºï¼Œä¸ä¿å­˜\")\n",
    "            return False\n",
    "        try:\n",
    "            # ä»CSVå­—ç¬¦ä¸²æ¢å¤DataFrameï¼ˆå…¼å®¹æ‰€æœ‰pandasç‰ˆæœ¬ï¼‰\n",
    "            df = pd.read_csv(StringIO(csv_str))\n",
    "            # ä¿å­˜ä¸ºCSVæ–‡ä»¶\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"âœ… ä¿å­˜æˆåŠŸï¼š{filename}ï¼ˆ{len(df)}æ¡è®°å½•ï¼‰\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä¿å­˜å¤±è´¥ï¼š{e}\")\n",
    "            return False\n",
    "\n",
    "def generate_date_range(start_date_str: str, end_date_str: str) -> list:\n",
    "    \"\"\"ç”Ÿæˆä»å¼€å§‹æ—¥æœŸåˆ°ç»“æŸæ—¥æœŸçš„æ‰€æœ‰æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆYYYYMMDDæ ¼å¼ï¼‰\"\"\"\n",
    "    dates = []\n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "        end_date = datetime.strptime(end_date_str, '%Y%m%d')\n",
    "        \n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"å¼€å§‹æ—¥æœŸæ™šäºç»“æŸæ—¥æœŸ\")\n",
    "            \n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            dates.append(current_date.strftime('%Y%m%d'))\n",
    "            current_date += timedelta(days=1)\n",
    "    except Exception as e:\n",
    "        print(f\"æ—¥æœŸå¤„ç†é”™è¯¯ï¼š{e}\")\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6225bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å‘é€ç«¯ï¼šRedisè¿æ¥æˆåŠŸ\n",
      "âœ… å‘é€ç«¯pandasç‰ˆæœ¬ï¼š2.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5447 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å…±éœ€è·å– 283 å¤©çš„æ•°æ® ===\n",
      "ğŸ“¤ å·²è°ƒç”¨è¿œç¨‹å‡½æ•°ï¼šfetch_minute_stock_dataï¼ˆä»»åŠ¡IDï¼štask_f0733decï¼‰\n",
      "âš ï¸ æ•°æ®ä¸ºç©ºï¼Œä¸ä¿å­˜\n",
      "ğŸ“¤ å·²è°ƒç”¨è¿œç¨‹å‡½æ•°ï¼šfetch_minute_stock_dataï¼ˆä»»åŠ¡IDï¼štask_6c496a80ï¼‰\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ä»é…ç½®æ–‡ä»¶è¯»å–Redisè¿æ¥ä¿¡æ¯\n",
    "    with open('redis.conf', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('host='):\n",
    "                host = line.split('=')[1].strip()\n",
    "            elif line.startswith('port='):\n",
    "                port = int(line.split('=')[1].strip())\n",
    "            elif line.startswith('password='):\n",
    "                password = line.split('=')[1].strip()\n",
    "    # åˆå§‹åŒ–Rediså‘é€ç«¯\n",
    "    sender = RemoteSender(host=host, port=port, password=password)\n",
    "    \n",
    "    # å®šä¹‰æ—¥æœŸèŒƒå›´ï¼šä»20250516åˆ°20250923\n",
    "    start_date = '20251008'#(df['date'].max() + timedelta(days=1)).strftime('%Y%m%d')\n",
    "    # è·å–å½“æ—¥æ—¥æœŸ-1ï¼Œæ˜¯end_date\n",
    "    end_date = (datetime.today() - timedelta(days=1)).strftime('%Y%m%d')\n",
    "    \n",
    "    \n",
    "    daily_df = pd.read_parquet('../data/stock_daily_price.parquet')\n",
    "    stock_code_list = daily_df['stock_code'].unique()\n",
    "\n",
    "    # å¾ªç¯è°ƒç”¨è·å–æ¯æ—¥æ•°æ®\n",
    "    for stock_code in tqdm(stock_code_list):\n",
    "        # è¯»å–å·²å­˜åœ¨çš„parquetæ–‡ä»¶\n",
    "        start_date2 = start_date\n",
    "\n",
    "        existing_file = f\"d:/workspace/xiaoyao/redis/minutely/{stock_code}.parquet\"\n",
    "        if os.path.exists(existing_file):\n",
    "            existing_df = pd.read_parquet(existing_file)\n",
    "            # è¯»å–existing_dfçš„dateåˆ—çš„æœ€å¤§date\n",
    "            max_date = existing_df['date'].max()\n",
    "            start_date2 = (datetime.strptime(max_date, '%Y%m%d') + timedelta(days=1)).strftime('%Y%m%d')\n",
    "        \n",
    "        date_list = generate_date_range(start_date2, end_date)\n",
    "        print(f\"=== å…±éœ€è·å– {len(date_list)} å¤©çš„æ•°æ® ===\")\n",
    "\n",
    "        for i, date in enumerate(date_list, 1):\n",
    "            try:\n",
    "                # è°ƒç”¨è¿œç¨‹å‡½æ•°è·å–å½“æ—¥æ•°æ®\n",
    "                csv_data = sender.call_remote_function('fetch_minute_stock_data', date,[stock_code])\n",
    "                # ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼Œæ–‡ä»¶ååŒ…å«æ—¥æœŸ\n",
    "                sender.save_to_csv(csv_data, f'./minutely/stock_minutely_price_{stock_code}_{date}.csv')\n",
    "                # é€‚å½“å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹\n",
    "                time.sleep(0.05)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ {date} å¤„ç†å¤±è´¥ï¼š{e}\")\n",
    "                # å¤±è´¥åä¹Ÿå»¶è¿Ÿä¸€ä¸‹ï¼Œé¿å…å¿«é€Ÿé‡è¯•å¯¼è‡´çš„é—®é¢˜\n",
    "                time.sleep(1)\n",
    "        # ä¸‹è½½å®Œæˆåå¯¹csvè¿›è¡Œåˆå¹¶\n",
    "        success = merge_stock_csv_to_parquet('./minutely/', stock_code, f'./minutely/{stock_code}_merged.parquet')\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"âŒ {stock_code} åˆå¹¶å¤±è´¥\")\n",
    "        \n",
    "        # å°†ç”Ÿæˆçš„parquetè¿›è¡Œåˆå¹¶\n",
    "        merge_parquet_files(stock_code)\n",
    "\n",
    "\n",
    "        # åˆ é™¤æŒ‡å®šçš„parquetæ–‡ä»¶\n",
    "        file_to_delete = f'./minutely/{stock_code}_merged.parquet'\n",
    "        if os.path.exists(file_to_delete):\n",
    "            os.remove(file_to_delete)\n",
    "        else:\n",
    "            print(f\"File not found: {file_to_delete}\")\n",
    "\n",
    "        file_to_delete = f'./minutely/{stock_code}.parquet'\n",
    "        if os.path.exists(file_to_delete):\n",
    "            os.remove(file_to_delete)\n",
    "        else:\n",
    "            print(f\"File not found: {file_to_delete}\")\n",
    "\n",
    "        # åˆ é™¤æ— ç”¨çš„csv\n",
    "        delete_stock_csv_files(stock_code)\n",
    "\n",
    "    print(\"\\n=== æ‰€æœ‰æ—¥æœŸå¤„ç†å®Œæˆ ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712c785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
